{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reusable_annotator.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratmcu/wiki_ner/blob/master/reusable_annotator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQq2feNhp0bU",
        "colab_type": "code",
        "outputId": "23493f59-b049-493a-ea7d-02162c172d7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install pyahocorasick\n",
        "!pip install fuzzyset\n",
        "from ahocorasick import Automaton\n",
        "import fuzzyset\n",
        "import urllib\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: fuzzyset in /usr/local/lib/python3.6/dist-packages (0.0.19)\n",
            "Requirement already satisfied: python-levenshtein in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (0.12.0)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (1.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein->fuzzyset) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWXQXUvNqCCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PageContents():\n",
        "  def __init__(self, url):\n",
        "      self.quote_page = url\n",
        "      self.page = urllib.request.urlopen(self.quote_page)\n",
        "      self.soup = BeautifulSoup(self.page, 'html.parser')\n",
        "#       side_pane = self.soup.find('tbody')#.find_all('tr')\n",
        "      self.table_entry_list = self.soup.find('tbody').find_all('tr')\n",
        "   \n",
        "  def get_party(self):\n",
        "#       side_pane = self.soup.find('table', attrs={'class': 'infobox vcard'})\n",
        "#       table_entry_list = side_pane.find_all('tr')\n",
        "      party = ''\n",
        "      for table_entry in self.table_entry_list:\n",
        "          for child in table_entry.children:\n",
        "              if child.text == 'Political party':\n",
        "                  party = child.next_sibling.contents[0].get('title')\n",
        "#                   print(party)\n",
        "             \n",
        "  def get_name(self):\n",
        "#       side_pane = self.soup.find('table', attrs={'class': 'infobox vcard'})\n",
        "#       table_entry_list = side_pane.find_all('tr')\n",
        "      names = []\n",
        "      for table_entry in self.table_entry_list:\n",
        "          for child in table_entry.children:\n",
        "              if child.text == 'Born':\n",
        "                  div_list = child.next_sibling.find_all('div')\n",
        "                  for i, item in enumerate(div_list):\n",
        "                    try:\n",
        "                      t = item['class']\n",
        "#                       print(t)\n",
        "                      if t[0] == 'nickname':\n",
        "#                         print(item.text)\n",
        "                        names.append(item.text)\n",
        "                    except:\n",
        "                      pass\n",
        "                  break\n",
        "      if len(names) != 0:\n",
        "        return names\n",
        "      else:\n",
        "        div_list = self.soup.find('tbody').find_all('div')\n",
        "        for i, item in enumerate(div_list):\n",
        "          try:\n",
        "            t = item['class']\n",
        "#             print(t)\n",
        "            if t[0] == 'fn':\n",
        "#               print(item.contents[0])\n",
        "              names.append(item.contents[0])\n",
        "          except:\n",
        "            pass\n",
        "      return names\n",
        "  def get_birth_date(self):\n",
        "#       side_pane = self.soup.find('table', attrs={'class': 'infobox vcard'})\n",
        "#       table_entry_list = side_pane.find_all('tr')\n",
        "      date = []\n",
        "      for table_entry in self.table_entry_list:\n",
        "          for child in table_entry.children:\n",
        "              if child.text == 'Born':\n",
        "                  for i, item in enumerate(child.next_sibling.contents):\n",
        "                    try:\n",
        "                      t = item.span['class']\n",
        "#                       print(t)\n",
        "                      if t[0] == 'bday':\n",
        "#                         print(child.next_sibling.contents[i+1])\n",
        "                        date.append(child.next_sibling.contents[i+1])\n",
        "                    except:\n",
        "                      pass\n",
        "                  break\n",
        "      return date  \n",
        "    \n",
        "  def get_birth_place(self):\n",
        "#       side_pane = self.soup.find('table', attrs={'class': 'infobox vcard'})\n",
        "#       table_entry_list = side_pane.find_all('tr')\n",
        "      party = []\n",
        "      for table_entry in self.table_entry_list:\n",
        "          for child in table_entry.children:\n",
        "              if child.text == 'Born':\n",
        "#                   print(child.next_sibling.contents)\n",
        "#                 sibling = find all \n",
        "#                 for content in child.next_sibling.contents:\n",
        "#                   if \n",
        "                  party.append(child.next_sibling.contents[-1].text)\n",
        "                  break\n",
        "      return party\n",
        "    \n",
        "  def get_children(self):\n",
        "#       side_pane = self.soup.find('table', attrs={'class': 'infobox vcard'})\n",
        "#       table_entry_list = side_pane.find_all('tr')\n",
        "      party = []\n",
        "      for table_entry in self.table_entry_list:\n",
        "          for child in table_entry.children:\n",
        "#               print(child.text)\n",
        "              if child.text == 'Children':\n",
        "                  children = child.next_sibling.find_all('a')\n",
        "#                   print([child for child in children])\n",
        "#                   print(child.next_sibling.contents[0].text)\n",
        "#                   print(child.next_sibling.contents)\n",
        "#                   sen = re.sub('[[0-9]*]', '', sentence) #remove the reference brackets\n",
        "                  party.extend([re.sub('[[0-9]*]', '', child.text) for child in children])\n",
        "      if party != ['']:\n",
        "#         print(party)\n",
        "        return party\n",
        "      else:\n",
        "        return self.get_children_alternate()\n",
        "      \n",
        "  def get_children_alternate(self):\n",
        "#       side_pane = self.soup.find('table', attrs={'class': 'infobox vcard'})\n",
        "#       table_entry_list = side_pane.find_all('tr')\n",
        "      party = []\n",
        "      for table_entry in self.table_entry_list:\n",
        "          for child in table_entry.children:\n",
        "#               print(child.text)\n",
        "              if child.text == 'Children':\n",
        "#                   children = child.next_sibling.find_all('a')   \n",
        "#                   print('-----')\n",
        "#                   print(child.next_sibling.contents)\n",
        "                  for kid in child.next_sibling.contents:\n",
        "#                     print(type(kid))\n",
        "#                     print('---+++_++_+++')\n",
        "                    try:\n",
        "                      a = BeautifulSoup(kid, \"html.parser\").find_all('a')\n",
        "#                       print('printing a s')\n",
        "#                       print(a)\n",
        "                      if not bool(a):\n",
        "#                         print(kid)\n",
        "                        party.append(kid)\n",
        "#                       else:\n",
        "#                         print('html kid!')\n",
        "#                         party.append(re.sub('[[0-9]*]', '', kid.text))\n",
        "                    except:\n",
        "#                       print(kid.text)\n",
        "#                       party.append(re.sub('[[0-9]*]', '', kid.text))\n",
        "                        pass\n",
        "      return party\n",
        "    \n",
        "  def get_spouses(self):\n",
        "#       side_pane = self.soup.find('table', attrs={'class': 'infobox vcard'})\n",
        "#       table_entry_list = side_pane.find_self.all('tr')\n",
        "      party = []\n",
        "      for table_entry in self.table_entry_list:\n",
        "          for child in table_entry.children:\n",
        "              if child.text == 'Spouse(s)':\n",
        "                try:\n",
        "                  children = child.next_sibling.contents[0].find_all('a')\n",
        "#                   print([child.text for child in children])\n",
        "                  party.extend([child.text for child in children])\n",
        "                except:\n",
        "#                   print(child.next_sibling.contents)\n",
        "                  pass\n",
        "                break\n",
        "      return party\n",
        "  \n",
        "  def get_parents(self):\n",
        "#       side_pane = self.soup.find('table', attrs={'class': 'infobox vcard'})\n",
        "#       side_pane = self.soup.find('tbody')#.find_all('tr')\n",
        "#       table_entry_list = side_pane.find_all('tr')\n",
        "      party = []\n",
        "      for table_entry in self.table_entry_list:\n",
        "          for child in table_entry.children:\n",
        "              if child.text == 'Parents':\n",
        "                  children = child.next_sibling.find_all('a')\n",
        "#                   print([child.text for child in children])\n",
        "                  party.extend([child.text for child in children])\n",
        "      return party\n",
        "  \n",
        "  def get_any(self, field):\n",
        "#       side_pane = self.soup.find('table', attrs={'class': 'infobox vcard'})\n",
        "#       table_entry_list = side_pane.find_all('tr')\n",
        "      party = []\n",
        "      for table_entry in self.table_entry_list:\n",
        "          for child in table_entry.children:\n",
        "              if child.text == field:\n",
        "                  party.append(child.next_sibling.contents[0].text)\n",
        "      return party\n",
        "\n",
        "  def get_text(self):\n",
        "    nltk.download('punkt')\n",
        "    paraghraphs = self.soup.find_all('p')\n",
        "    paraghraphs_list = []\n",
        "    for par in paraghraphs:\n",
        "      paraghraphs_list.append(sent_tokenize(par.text))\n",
        "    #   len(sent_tokenize_list)\n",
        "    # paraghraphs_list\n",
        "\n",
        "    text = []\n",
        "    for paraghraph in paraghraphs_list: \n",
        "      for sentence in paraghraph:\n",
        "        sen = re.sub('[[0-9]*]', '', sentence) #remove the reference brackets\n",
        "        text.append(sen)\n",
        "    #     print(sen)  \n",
        "    return text\n",
        "  \n",
        "  def get_info_dict(self):\n",
        "    info_dict = {}\n",
        "    info_dict['NAME'] = self.get_name()\n",
        "    info_dict['BIRTH_DATE'] = self.get_birth_date()\n",
        "    info_dict['BIRTH_PLACE'] = self.get_birth_place()\n",
        "    info_dict['CHILDREN'] = self.get_children()\n",
        "    info_dict['SPOUSES'] = self.get_spouses()\n",
        "    info_dict['PARENTS'] = self.get_parents()\n",
        "    return info_dict\n",
        "  \n",
        "  def get_contents(self):\n",
        "    return self.get_info_dict(), self.get_text()\n",
        "  \n",
        "def aggregate_parts_and_annotate(annotated_list, txt, words_within = 2):\n",
        "  annot_list = []\n",
        "#   print(len(annotated_list))\n",
        "  if len(annotated_list) > 1:\n",
        "    matched = set([ annotated_list[i][3][1][1] for i in range(1,len(annotated_list)-1)])\n",
        "    sub_sentence = txt[annotated_list[0][0]][annotated_list[0][3][0]+1:annotated_list[-1][3][0]-len(annotated_list[-1][3][1][1])]\n",
        "    enclosed = set(sub_sentence.split()) # could there be an elegant way to do this?? \n",
        "    difference = enclosed.difference(matched)\n",
        "    if len(difference)>words_within:\n",
        "#       pass\n",
        "      for annot in annotated_list:\n",
        "        annot_list.append((annotated_list[0][0],(annot[3][0]-len(annot[3][1][1])+1, annot[3][0]+1), annot[4])) # [sentence number, (start char, end char), annotation type]\n",
        "    else:\n",
        "        annot_list.append((annotated_list[0][0],(annotated_list[0][3][0]-len(annotated_list[0][3][1][1])+1,annotated_list[-1][3][0]+1),annotated_list[0][4]))\n",
        "#     print(enclosed.difference(matched)) \n",
        "#     print(sub_sentence.split())\n",
        "  else:\n",
        "    annot_list.append((annotated_list[0][0],(annotated_list[0][3][0]-len(annotated_list[0][3][1][1])+1, annotated_list[0][3][0]+1), annotated_list[0][4])) # [sentnce number, (start char, end char), anootation type]\n",
        "#   print(annot_list)\n",
        "  return annot_list\n",
        "\n",
        "def annotate_text(info_dict, txt):\n",
        "  annotations = []\n",
        "  keys = list(info_dict.keys())\n",
        "  for key in keys:\n",
        "    annotate_list = []\n",
        "    for i, sentence in enumerate(txt):\n",
        "        fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "        for word in sentence.split():\n",
        "            fz.add(word)        \n",
        "    #     A=Automaton()\n",
        "        for j, detail in enumerate(info_dict[key]):\n",
        "          annotate_sub = []\n",
        "          matched_list = []\n",
        "          A=Automaton()\n",
        "          tokens_in_detail = len(detail.split())\n",
        "          for word in detail.split():\n",
        "    #         print(word)\n",
        "            result = fz.get(word)\n",
        "            if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2):\n",
        "              matched_list.append((word, result))\n",
        "  #             print(result[0][1], word)\n",
        "  #         if len(matched_list) == tokens_in_detail:\n",
        "          if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "            for matches in matched_list:\n",
        "                for match in matches[1]:\n",
        "                    A.add_word(match[1], (matches[0], match[1]))\n",
        "  #                   print(match[1], '--->', match)\n",
        "            A.make_automaton()\n",
        "    #         print(sentence)\n",
        "            for item in A.iter(sentence):\n",
        "    #             print(item, item[0]+1- len(item[1][1]), item[0] , sentence)\n",
        "    #             print(item, item[0]+1- len(item[1][1]), item[0])\n",
        "                annotate_sub.append([i, j, detail, item, key])\n",
        "            if len(annotate_sub) != 0:\n",
        "              annotate_list.append(annotate_sub)    \n",
        "#     annotation_minimized = []\n",
        "    for list_sub in annotate_list:\n",
        "      if list_sub[0][2].split()[0] != list_sub[0][3][1][0]:\n",
        "        list_sub.pop(0)\n",
        "#       annotation_minimized.extend(aggregate_parts_and_annotate(list_sub,txt))\n",
        "      annotations.extend(aggregate_parts_and_annotate(list_sub,txt))\n",
        "\n",
        "#   for list_sub in annotations: #uncomment to print the annotation list!\n",
        "#     print(list_sub, txt[list_sub[0]], '\\'', txt[list_sub[0]][list_sub[1][0]:list_sub[1][1]], '\\'')\n",
        "  return annotations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTe55CRXslyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# page_cnts = PageContents('https://en.wikipedia.org/wiki/Hafizullah_Amin')\n",
        "page_cnts = PageContents('https://en.wikipedia.org/wiki/Alfred_Moisiu')\n",
        "# annotate_text(page_cnts.get_contents())\n",
        "a,b = page_cnts.get_contents()\n",
        "c = annotate_text(a,b)\n",
        "a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZKNx6t9OnTu",
        "colab_type": "code",
        "outputId": "2d2959e9-2f23-4693-b66c-0341094d33ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "import pickle\n",
        "import time\n",
        "class hashabledict(dict):\n",
        "    def __hash__(self):\n",
        "        return hash(tuple(sorted(self.items())))\n",
        "      \n",
        "if not os.path.exists('dataset'):\n",
        "  os.makedirs('dataset')\n",
        "  \n",
        "category = 'politicians'\n",
        "\n",
        "if not os.path.exists('politicians'):\n",
        "  os.makedirs('politicians')\n",
        "  \n",
        "with open('president_list.pkl', 'rb') as f:\n",
        "    prsdnt_list = pickle.load(f)\n",
        "\n",
        "for cntry in prsdnt_list:\n",
        "  time.sleep(1)\n",
        "  if not os.path.exists('politicians' + '/' + cntry['country']):\n",
        "    os.makedirs('politicians' + '/' + cntry['country'])\n",
        "  for prsdnt in cntry['presidents']:\n",
        "    path = 'dataset' + '/' + 'politicians' + '/' + cntry['country'] + '/' +  prsdnt['name']\n",
        "    if not os.path.exists(path):\n",
        "      os.makedirs(path)\n",
        "    else:\n",
        "      continue\n",
        "    page_cnts = PageContents(prsdnt['url'])\n",
        "    print(prsdnt['url'])\n",
        "    a,b = page_cnts.get_contents()\n",
        "    print(a)\n",
        "    c = annotate_text(a,b)\n",
        "    annotation_idx = 0\n",
        "    null_annot = (None, (None, None), 'NONE')\n",
        "    annot_list = []\n",
        "    try:\n",
        "      for i, sentence in enumerate(b):\n",
        "        if c[annotation_idx][0] == i:\n",
        "          annot_list.append(c[annotation_idx])\n",
        "          annotation_idx = annotation_idx + 1\n",
        "        else:\n",
        "          annot_list.append(null_annot)\n",
        "    except:\n",
        "      print(self.soup)\n",
        "    d = {'sentence': b, 'annotation': annot_list}\n",
        "    df = pd.DataFrame(data=d)\n",
        "    df.to_csv (r'{0}/annotation_dataframe.csv'.format(path), index = None, header=True)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://en.wikipedia.org/wiki/Bujar_Nishani\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "{'NAME': ['Bujar Nishani'], 'BIRTH_DATE': ['29 September 1964'], 'BIRTH_PLACE': ['Albania'], 'CHILDREN': [], 'SPOUSES': ['Odeta Kosova'], 'PARENTS': []}\n",
            "https://en.wikipedia.org/wiki/Abdelkader_Bensalah\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "{'NAME': ['Abdelkader Bensalah'], 'BIRTH_DATE': ['24 November 1941'], 'BIRTH_PLACE': ['French Algeria'], 'CHILDREN': [], 'SPOUSES': [], 'PARENTS': []}\n",
            "https://en.wikipedia.org/wiki/Liamine_Z%C3%A9roual\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-2f91af0296fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mpage_cnts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPageContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprsdnt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprsdnt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpage_cnts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_contents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-7282631257cb>\u001b[0m in \u001b[0;36mget_contents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_contents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_info_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maggregate_parts_and_annotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotated_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_within\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-7282631257cb>\u001b[0m in \u001b[0;36mget_info_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0minfo_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NAME'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0minfo_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BIRTH_DATE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_birth_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0minfo_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BIRTH_PLACE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_birth_place\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0minfo_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CHILDREN'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0minfo_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SPOUSES'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_spouses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-7282631257cb>\u001b[0m in \u001b[0;36mget_birth_place\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m#                 for content in child.next_sibling.contents:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m#                   if\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                   \u001b[0mparty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_sibling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                   \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mparty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bs4/element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    795\u001b[0m             raise AttributeError(\n\u001b[1;32m    796\u001b[0m                 \"'%s' object has no attribute '%s'\" % (\n\u001b[0;32m--> 797\u001b[0;31m                     self.__class__.__name__, attr))\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moutput_ready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NavigableString' object has no attribute 'text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsQ8JRZYifJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "annotation_idx = 0\n",
        "null_annot = (None, (None, None), 'NONE')\n",
        "annot_list = []\n",
        "for i, sentence in enumerate(b):\n",
        "  if c[annotation_idx][0] == i:\n",
        "    annot_list.append(c[annotation_idx])\n",
        "    annotation_idx = annotation_idx + 1\n",
        "  else:\n",
        "    annot_list.append(null_annot)\n",
        "    \n",
        "d = {'sentence': b, 'annotation': annot_list}\n",
        "df = pd.DataFrame(data=d)\n",
        "# df\n",
        "\n",
        "df.to_csv (r'annotation_dataframe.csv', index = None, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7siRBT3jlO8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[:1]['annotation'][0][2]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcjHwpn1nbD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}