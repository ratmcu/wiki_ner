{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conll_tagged_ne.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratmcu/wiki_ner/blob/master/conll_tagged_ne.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcIYSgqMLy8V",
        "colab_type": "code",
        "outputId": "d5077541-2b82-4ede-ed3d-f0d744eab43e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import wget\n",
        "try:\n",
        "    import colabimport\n",
        "except:\n",
        "    colabimporturl = 'https://github.com/ratmcu/colaboratory_import/raw/master/colabimport.py'\n",
        "    filename = colabimporturl.split(\"/\")[-1].split(\"?\")[0]\n",
        "    if os.path.isfile(filename):\n",
        "        os.remove(filename)\n",
        "    wget.download(colabimporturl)\n",
        "    import colabimport\n",
        "colabimport.get_notebook('https://github.com/ratmcu/wiki_ner/blob/master/reusable_annotator.ipynb?raw=true')\n",
        "colabimport.get_notebook('https://github.com/ratmcu/wiki_ner/blob/master/info_box.ipynb?raw=true')\n",
        "# import io, os, sys, types\n",
        "from reusable_annotator import PageContents\n",
        "from info_box import InfoCard, PrivateEntities\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.DEBUG)\n",
        "# logging.debug(\"test\")\n",
        "class HashableTupleAnnotations(tuple):\n",
        "    def __hash__(self):\n",
        "        return hash(tuple(sorted([self[0:1],self[1:2]])))\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "import re\n",
        "!pip install pyahocorasick\n",
        "!pip install fuzzyset\n",
        "from ahocorasick import Automaton\n",
        "import fuzzyset\n",
        "from operator import itemgetter, attrgetter\n",
        "import pandas as pd\n",
        "# import json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "importing Jupyter notebook from reusable_annotator.ipynb\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 4.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick\n",
            "Successfully installed pyahocorasick-1.4.0\n",
            "Collecting fuzzyset\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/78/7509f3efbb6acbcf842d7bdbd9a919ca8c0ed248123bdd8c57f08497e0dd/fuzzyset-0.0.19.tar.gz (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 5.1MB/s \n",
            "\u001b[?25hCollecting python-levenshtein (from fuzzyset)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 18.1MB/s \n",
            "\u001b[?25hCollecting texttable (from fuzzyset)\n",
            "  Downloading https://files.pythonhosted.org/packages/82/a8/60df592e3a100a1f83928795aca210414d72cebdc6e4e0c95a6d8ac632fe/texttable-1.6.2.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein->fuzzyset) (41.0.1)\n",
            "Building wheels for collected packages: fuzzyset, python-levenshtein, texttable\n",
            "  Building wheel for fuzzyset (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/d8/36/9a/8f1cac047c7c3b03dce3d5434ed0088bfd8da8aeca615dfb4c\n",
            "  Building wheel for python-levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "  Building wheel for texttable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/d1/d6/dfbe4eb3c468832f7fbe4bd27f9875fa97277cabed8fb6715c\n",
            "Successfully built fuzzyset python-levenshtein texttable\n",
            "Installing collected packages: python-levenshtein, texttable, fuzzyset\n",
            "Successfully installed fuzzyset-0.0.19 python-levenshtein-0.12.0 texttable-1.6.2\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "importing Jupyter notebook from info_box.ipynb\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: fuzzyset in /usr/local/lib/python3.6/dist-packages (0.0.19)\n",
            "Requirement already satisfied: python-levenshtein in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (0.12.0)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (1.6.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein->fuzzyset) (41.0.1)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.0.7)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 4.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
            "Successfully built regex\n",
            "Installing collected packages: regex\n",
            "Successfully installed regex-2019.6.8\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.6)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.4)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: fuzzyset in /usr/local/lib/python3.6/dist-packages (0.0.19)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (1.6.2)\n",
            "Requirement already satisfied: python-levenshtein in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (0.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein->fuzzyset) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrMlO6iPL6mU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WikiConLLTagger():\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        self.page = PageContents(url)\n",
        "        self.text = self.page.get_text_chunk()\n",
        "        self.info_card = InfoCard(self.page)\n",
        "        self.private_entities = PrivateEntities(self.info_card).entity_dict   \n",
        "        self.doc = nlp(self.text)\n",
        "        self.spacy_noun_chunks =  [(chunk.text, chunk.start, chunk.end) for chunk in self.doc.noun_chunks]\n",
        "        self.tag_factory = TagFactory(self)\n",
        "#         print(sorted(self.tag_factory.get_annotations(), key=lambda annot: annot[1][0]))\n",
        "        \n",
        "    def _get_annotations(self):\n",
        "        '''stack all the annotations'''\n",
        "        return sorted(sorted(self.tag_factory.get_annotations(), key=lambda annot: annot[1][0]), reverse=True) # sort them by the start index of the annotaion token\n",
        "       \n",
        "    def get_metadata(self):\n",
        "        pe = self.private_entities\n",
        "        # pe = [list(map(lambda li: item[1], item)) for item in pe.items()]\n",
        "        dp = dict()\n",
        "        [dp.update({item[0]: item[1][1]}) for item in pe.items()]\n",
        "        dp.update({'URL' : self.url})\n",
        "        return dp\n",
        "        \n",
        "    def place_tags(self):\n",
        "        annotations = self._get_annotations()\n",
        "        words = []\n",
        "        tags = []\n",
        "        annotation_tags = ['O']*len(self.doc)\n",
        "#         annotation = annotations.pop()\n",
        "        for annotation in annotations:\n",
        "            annotation_tags[annotation[1][0]] = 'B-'+annotation[0] #\n",
        "            for i in range(annotation[1][1]-annotation[1][0]-1):\n",
        "                annotation_tags[annotation[1][0]+i+1] = 'I-'+annotation[0]\n",
        "#         for token in self.doc:\n",
        "#             words.append(token)\n",
        "#             if token\n",
        "        for sentence in self.doc.sents:\n",
        "            tags.extend(annotation_tags[sentence.start:sentence.end])\n",
        "            tags.append('\\n')\n",
        "            for token in sentence:\n",
        "                words.append(token)\n",
        "            words.append('\\n')\n",
        "#         print(words)\n",
        "#         print(tags)\n",
        "        return {'words': words, 'tags':tags}\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p315V1HpXryO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TagFactory():\n",
        "    '''will keep a set of all the tagged annotations in the form of\n",
        "       ('word phrase', 'ENTITY', start, end), each tagging function will update the list'''\n",
        "    def __init__(self, tagger):\n",
        "        self.doc =tagger.doc\n",
        "        self.entity_dict = tagger.private_entities\n",
        "        self.tag_set = set()\n",
        "        self.spacy_noun_chunks =  [(chunk.text, chunk.start, chunk.end) for chunk in self.doc.noun_chunks]\n",
        "        tagging_methods = [getattr(self, method) for method in dir(self) if callable(getattr(self, method)) and re.match('_tag_.*', method)]\n",
        "        for method in tagging_methods:\n",
        "            method()\n",
        "\n",
        "    def get_annotations(self):  \n",
        "        '''list with ('ENTITY', start, end) '''\n",
        "        return [annot[1:] for annot in list(self.tag_set)]\n",
        "    \n",
        "    def _tag_bd(self):\n",
        "        date_entities = []\n",
        "        if not 'BIRTH_DATE' in self.entity_dict:\n",
        "            return\n",
        "        birth_dates = self.entity_dict['BIRTH_DATE'][1]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'DATE':\n",
        "                date_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for date in date_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in date[0].split(): # add all the words into a fuzzy set from the entity\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(birth_dates): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (date[0], 'BD', date[2:]) # exact phrase, entity tag, token indices\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "    \n",
        "    def _tag_bp(self):\n",
        "        place_entities = []\n",
        "        if not 'BIRTH_PLACE' in self.entity_dict:\n",
        "            return\n",
        "        birth_places = self.entity_dict['BIRTH_PLACE'][1]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'GPE':\n",
        "                place_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "        for place in place_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in place[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(birth_places): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]==1):\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (place[0], 'BP', place[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(birth_places): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]==1):\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'BP', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'BP', noun[1:])))\n",
        "\n",
        "    def _tag_spouse(self):\n",
        "        spouse_entities = []\n",
        "        if not 'SPOUSES' in self.entity_dict:\n",
        "            return\n",
        "        spouse_names = self.entity_dict['SPOUSES'][1]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'PERSON':\n",
        "                spouse_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for spouse in spouse_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            # for word in spouse[0].split(): # add all the words into a fuzzy set from the entity\n",
        "            #     fz.add(word)  \n",
        "            fz.add(spouse[0])                  \n",
        "            for j, detail in enumerate(spouse_names): # get a detail in the list under an info line(things coming from the side info box)\n",
        "                # matched_list = []\n",
        "                # tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                # for word in detail.split(): # get a word from the detail\n",
        "                #     result = fz.get(word)    # get the matching \n",
        "                #     if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                #         matched_list.append((word, result))\n",
        "                result = fz.get(detail.split()[0])    # for a name, it should match the first name\n",
        "                if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(spouse[0])/2): #if the matching confidence is high and word length is high\n",
        "                    annot = (spouse[0], 'SP', spouse[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(spouse_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                # tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for wn, word in enumerate(detail.split()): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2: #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                    elif wn == 0: #if the first word doesn't match, break the matching\n",
        "                        break\n",
        "                if  len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'SP', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'SP', noun[1:])))\n",
        "                \n",
        "    def _tag_edu(self):\n",
        "        edu_entities = []\n",
        "        if not 'EDUCATION' in self.entity_dict:\n",
        "            return\n",
        "        edu_names = self.entity_dict['EDUCATION'][1]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'ORG':\n",
        "                edu_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for edu in edu_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in edu[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(edu_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.8 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (edu[0], 'ED', edu[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                    \n",
        "        logging.debug('------NOUN CHUNCK MATCHING------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(edu_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.9 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'ED', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'ED', noun[1:])))\n",
        "                # elif noun[0].split()[0] ==  detail.split()[0] and len(noun[0].split())>1: # lets add an exception for University name matching\n",
        "                    # logging.debug((noun[0], 'ED', noun[1:]))\n",
        "#                     self.tag_set.add(HashableTupleAnnotations((noun[0], 'ED', noun[1:])))\n",
        "            \n",
        "    def _tag_children(self):\n",
        "        children_entities = []\n",
        "        if not 'CHILDREN' in self.entity_dict:\n",
        "            return\n",
        "        children_names = self.entity_dict['CHILDREN'][1]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'PERSON':\n",
        "                children_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for child in children_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            # for word in child[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "            #     fz.add(word)        \n",
        "            # for j, detail in enumerate(children_names): # get a detail in the list under an info line\n",
        "            #     matched_list = []\n",
        "            #     tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "            #     for word in detail.split(): # get a word from the detail\n",
        "            #         result = fz.get(word)    # get the matching \n",
        "            #         if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "            #             matched_list.append((word, result))\n",
        "            #     if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "            #         annot = (child[0], 'CH', child[2:])\n",
        "            #         logging.debug(annot)\n",
        "            #         self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "            fz.add(child[0])\n",
        "            for j, detail in enumerate(children_names): # get a detail in the list under an info line(things coming from the side info box)            \n",
        "                result = fz.get(detail.split()[0])    # for a name, it should match the first name\n",
        "                if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(child[0])/2): #if the matching confidence is high and word length is high\n",
        "                    annot = (child[0], 'CH', child[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(children_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                # tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                # for word in detail.split(): # get a word from the detail\n",
        "                #     result = fz.get(word)    # get the matching \n",
        "                #     if result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2 \\\n",
        "                #               and result[0][1][0] == word[0]: #if the matching confidence is high and word length is high\n",
        "                #         matched_list.append((word, result))\n",
        "                # if  len(matched_list)!=0:\n",
        "                #     logging.debug((noun[0], 'CH', noun[1:]))\n",
        "                #     self.tag_set.add(HashableTupleAnnotations((noun[0], 'CH', noun[1:])))\n",
        "                for wn, word in enumerate(detail.split()): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2: #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                    elif wn == 0: #if the first word doesn't match, break the matching\n",
        "                        break\n",
        "                if  len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'CH', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'CH', noun[1:])))\n",
        "            \n",
        "    def _tag_parents(self):\n",
        "        parent_entities = []\n",
        "        if not 'PARENTS' in self.entity_dict:\n",
        "            return\n",
        "        parent_names = self.entity_dict['PARENTS'][1]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'PERSON':\n",
        "        #         print(entity.text, entity.label_, entity.start, entity.end)\n",
        "                parent_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "        logging.debug('------PERSON ENTITY MATCHING FOR PARENTS------')\n",
        "        for parent in parent_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in parent[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(parent_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (parent[0], 'PR', parent[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING FOR PARENTS------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            noun_words = noun[0].split()\n",
        "            for word in noun_words: # add all the words from the noun phrase into a fuzzy set\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(parent_names): # get a one candidate\n",
        "                matched_list = []\n",
        "                tokens_in_detail = detail.split()\n",
        "                if len(noun_words) < len(tokens_in_detail):\n",
        "                    continue\n",
        "                for i, word in enumerate(tokens_in_detail): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if result and result[0][0]>=0.5 and not len(result[0][1]) < len(word) \\\n",
        "                              and result[0][1][0] == word[0]: #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result, i))\n",
        "                # if  len(matched_list)!=0 and matched_list[0][2]==0: #and noun[0].split()[0][0] == detail.split()[0][0]:\n",
        "                #     logging.debug((noun[0], 'PR', noun[1:]))\n",
        "    #                 self.tag_set.add(HashableTupleAnnotations((noun[0], 'PR', noun[1:])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95y0R0RbNdnr",
        "colab_type": "code",
        "outputId": "180dc45d-18e6-4cc7-dbfc-4fa67fdfa036",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#experiment_code\n",
        "# tagger = WikiConLLTagger('https://en.wikipedia.org/wiki/Barack_Obama')\n",
        "tagger = WikiConLLTagger('https://en.wikipedia.org/wiki/Donald_Trump')\n",
        "# tagger = WikiConLLTagger('https://en.wikipedia.org/wiki/Mahinda_Rajapaksa')\n",
        "\n",
        "print(tagger.get_metadata())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "info card is scraped successfully\n",
            "[['Born', 'PERSON'], ['Donald John Trump']]\n",
            "[['Born', 'DATE'], ['1946-06-14', 'June 14, 1946']]\n",
            "[['Born', 'GPE'], ['New York City', 'Queens']]\n",
            "[['Children', 'PERSON'], ['Donald Jr.', 'Ivanka', 'Eric', 'Tiffany', 'Barron']]\n",
            "[['Spouse(s)', 'PERSON'], ['Ivana Zelníčková', 'Marla Maples', 'Melania Knauss']]\n",
            "[['Parents', 'PERSON'], ['Fred Trump', 'Mary Anne MacLeod']]\n",
            "[['Education', 'ORG'], ['The Wharton School']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "DEBUG:root:('New York City', 'BP', (41, 44))\n",
            "DEBUG:root:('Queens', 'BP', (81, 82))\n",
            "DEBUG:root:('Queens', 'BP', (485, 486))\n",
            "DEBUG:root:('New York City', 'BP', (487, 490))\n",
            "DEBUG:root:('Queens', 'BP', (1114, 1115))\n",
            "DEBUG:root:('Queens', 'BP', (1224, 1225))\n",
            "DEBUG:root:('New York City', 'BP', (3577, 3580))\n",
            "DEBUG:root:('New York City', 'BP', (5353, 5356))\n",
            "DEBUG:root:('New York City', 'BP', (6834, 6837))\n",
            "DEBUG:root:------NOUN CHUNCK MATCHING------\n",
            "DEBUG:root:('the New York City borough', 'BP', (40, 45))\n",
            "DEBUG:root:('Queens', 'BP', (46, 47))\n",
            "DEBUG:root:('Queens', 'BP', (81, 82))\n",
            "DEBUG:root:('Queens', 'BP', (485, 486))\n",
            "DEBUG:root:('New York City', 'BP', (487, 490))\n",
            "DEBUG:root:('Queens', 'BP', (538, 539))\n",
            "DEBUG:root:('Queens', 'BP', (1114, 1115))\n",
            "DEBUG:root:('Queens', 'BP', (1224, 1225))\n",
            "DEBUG:root:('Queens', 'BP', (1453, 1454))\n",
            "DEBUG:root:('New York City', 'BP', (3577, 3580))\n",
            "DEBUG:root:('New York City', 'BP', (5353, 5356))\n",
            "DEBUG:root:('New York City', 'BP', (6834, 6837))\n",
            "DEBUG:root:('Donald Trump', 'CH', (1157, 1159))\n",
            "DEBUG:root:('Donald Jr.', 'CH', (1279, 1281))\n",
            "DEBUG:root:('Ivanka', 'CH', (1286, 1287))\n",
            "DEBUG:root:('Eric', 'CH', (1293, 1294))\n",
            "DEBUG:root:('John Barron', 'CH', (2429, 2431))\n",
            "DEBUG:root:('Ivana', 'CH', (3986, 3987))\n",
            "DEBUG:root:('Donald Jr.', 'CH', (6237, 6239))\n",
            "DEBUG:root:('Eric', 'CH', (6240, 6241))\n",
            "DEBUG:root:('Ivanka', 'CH', (6243, 6244))\n",
            "DEBUG:root:('Donald Trump', 'CH', (7218, 7220))\n",
            "DEBUG:root:('Eric', 'CH', (13234, 13235))\n",
            "DEBUG:root:('Ivanka', 'CH', (13240, 13241))\n",
            "DEBUG:root:('Donald Trump', 'CH', (18823, 18825))\n",
            "DEBUG:root:('Barr', 'CH', (19067, 19068))\n",
            "DEBUG:root:('Barr', 'CH', (19120, 19121))\n",
            "DEBUG:root:('Barr', 'CH', (19150, 19151))\n",
            "DEBUG:root:('Barr', 'CH', (19430, 19431))\n",
            "DEBUG:root:('Barr', 'CH', (19465, 19466))\n",
            "DEBUG:root:('Barr', 'CH', (19497, 19498))\n",
            "DEBUG:root:------NOUN CHUNCK MATCHING------\n",
            "DEBUG:root:(' Donald John Trump', 'CH', (0, 4))\n",
            "DEBUG:root:('Donald John Trump', 'CH', (466, 469))\n",
            "DEBUG:root:('Donald Trump', 'CH', (1157, 1159))\n",
            "DEBUG:root:('Donald Jr.', 'CH', (1279, 1281))\n",
            "DEBUG:root:('Eric', 'CH', (1293, 1294))\n",
            "DEBUG:root:('Tiffany', 'CH', (1337, 1338))\n",
            "DEBUG:root:('Tiffany', 'CH', (1356, 1357))\n",
            "DEBUG:root:('Barron', 'CH', (1397, 1398))\n",
            "DEBUG:root:('Donald Trump', 'CH', (2113, 2115))\n",
            "DEBUG:root:('\"John Barron', 'CH', (2428, 2431))\n",
            "DEBUG:root:('Ronald Schnackenberg', 'CH', (5645, 5647))\n",
            "DEBUG:root:('The Donald J. Trump Foundation', 'CH', (5836, 5841))\n",
            "DEBUG:root:('Donald Jr.', 'CH', (6237, 6239))\n",
            "DEBUG:root:('Eric', 'CH', (6240, 6241))\n",
            "DEBUG:root:('Ivanka', 'CH', (6243, 6244))\n",
            "DEBUG:root:('Donald Trump', 'CH', (7218, 7220))\n",
            "DEBUG:root:('Ronald Reagan', 'CH', (7568, 7570))\n",
            "DEBUG:root:('Eric', 'CH', (13234, 13235))\n",
            "DEBUG:root:('Ivanka', 'CH', (13240, 13241))\n",
            "DEBUG:root:('President Donald Trump', 'CH', (18822, 18825))\n",
            "DEBUG:root:(\"Barr's description\", 'CH', (19150, 19153))\n",
            "DEBUG:root:(\"Barr's description\", 'CH', (19465, 19468))\n",
            "DEBUG:root:('the Wharton School', 'ED', (54, 57))\n",
            "DEBUG:root:('the Wharton School of the University of Pennsylvania', 'ED', (591, 599))\n",
            "DEBUG:root:------NOUN CHUNCK MATCHING------\n",
            "DEBUG:root:('the Wharton School', 'ED', (54, 57))\n",
            "DEBUG:root:('the Wharton School', 'ED', (591, 594))\n",
            "DEBUG:root:------PERSON ENTITY MATCHING FOR PARENTS------\n",
            "DEBUG:root:('Frederick Christ Trump', 'PR', (495, 498))\n",
            "DEBUG:root:('Mary Anne MacLeod', 'PR', (523, 526))\n",
            "DEBUG:root:('Frederick Trump', 'PR', (1004, 1006))\n",
            "DEBUG:root:('Mary Anne MacLeod', 'PR', (1173, 1176))\n",
            "DEBUG:root:(\"Fred Trump Jr.'s\", 'PR', (1671, 1675))\n",
            "DEBUG:root:('Fred Trump', 'PR', (3014, 3016))\n",
            "DEBUG:root:------NOUN CHUNCK MATCHING FOR PARENTS------\n",
            "DEBUG:root:('Ivanka', 'SP', (1286, 1287))\n",
            "DEBUG:root:('Marla Maples', 'SP', (1322, 1324))\n",
            "DEBUG:root:('Melania Knauss', 'SP', (1372, 1374))\n",
            "DEBUG:root:('Ivana', 'SP', (3986, 3987))\n",
            "DEBUG:root:('Ivanka', 'SP', (6243, 6244))\n",
            "DEBUG:root:('Ivanka', 'SP', (13240, 13241))\n",
            "DEBUG:root:------NOUN CHUNCK MATCHING------\n",
            "DEBUG:root:('Czech model Ivana Zelníčková', 'SP', (1268, 1272))\n",
            "DEBUG:root:('Ivana', 'SP', (1299, 1300))\n",
            "DEBUG:root:('actress Marla Maples', 'SP', (1321, 1324))\n",
            "DEBUG:root:('Marla', 'SP', (1360, 1361))\n",
            "DEBUG:root:('Slovenian model Melania Knauss', 'SP', (1370, 1374))\n",
            "DEBUG:root:('Ivana', 'SP', (3986, 3987))\n",
            "DEBUG:root:('Ivanka', 'SP', (6243, 6244))\n",
            "DEBUG:root:('Ivanka', 'SP', (13240, 13241))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'NAME': ['Donald John Trump'], 'BIRTH_DATE': ['1946-06-14', 'June 14, 1946'], 'BIRTH_PLACE': ['New York City', 'Queens'], 'CHILDREN': ['Donald Jr.', 'Ivanka', 'Eric', 'Tiffany', 'Barron'], 'SPOUSES': ['Ivana Zelníčková', 'Marla Maples', 'Melania Knauss'], 'PARENTS': ['Fred Trump', 'Mary Anne MacLeod'], 'EDUCATION': ['The Wharton School'], 'URL': 'https://en.wikipedia.org/wiki/Donald_Trump'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y23_nvZzFkFH",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "df = pd.DataFrame(data = tagger.place_tags())\n",
        "df.to_csv(r'conll_annot.csv', index = None, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4vG3X0nn3kL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "# for word in spouse[0].split(): # add all the words into a fuzzy set from the entity\n",
        "# fz.add('Shiranthi')\n",
        "fz.add('Wickremasinghe')\n",
        "fz.add('Shiranthi Wickremasinghe')\n",
        "        \n",
        "# for j, detail in enumerate(spouse_names): # get a detail in the list under an info line\n",
        "#     matched_list = []\n",
        "#     tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "#     for word in detail.split(): # get a word from the detail\n",
        "#         result = fz.get(word)    # get the matching \n",
        "#         if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "#             matched_list.append((word, result))\n",
        "\n",
        "result = fz.get('Wickremasinghe')\n",
        "result"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}