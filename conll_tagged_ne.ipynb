{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conll_tagged_ne.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratmcu/wiki_ner/blob/master/conll_tagged_ne.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcIYSgqMLy8V",
        "colab_type": "code",
        "outputId": "12c5ae6a-6e31-4794-d82c-b303af882087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        }
      },
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import wget\n",
        "try:\n",
        "    import colabimport\n",
        "except:\n",
        "    colabimporturl = 'https://github.com/ratmcu/colaboratory_import/raw/master/colabimport.py'\n",
        "    filename = colabimporturl.split(\"/\")[-1].split(\"?\")[0]\n",
        "    if os.path.isfile(filename):\n",
        "        os.remove(filename)\n",
        "    wget.download(colabimporturl)\n",
        "    import colabimport\n",
        "colabimport.get_notebook('https://github.com/ratmcu/wiki_ner/blob/master/reusable_annotator.ipynb?raw=true')\n",
        "colabimport.get_notebook('https://github.com/ratmcu/wiki_ner/blob/master/info_box.ipynb?raw=true')\n",
        "# import io, os, sys, types\n",
        "from reusable_annotator import PageContents\n",
        "from info_box import InfoCard, PrivateEntities\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.DEBUG)\n",
        "# logging.debug(\"test\")\n",
        "class HashableTupleAnnotations(tuple):\n",
        "    def __hash__(self):\n",
        "        return hash(tuple(sorted([self[0:1],self[1:2]])))\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "# from spacy import displacy\n",
        "from collections import Counter\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "import re\n",
        "!pip install pyahocorasick\n",
        "!pip install fuzzyset\n",
        "from ahocorasick import Automaton\n",
        "import fuzzyset\n",
        "from operator import itemgetter, attrgetter\n",
        "import pandas as pd\n",
        "# import json"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.1.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: fuzzyset in /usr/local/lib/python3.6/dist-packages (0.0.19)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (1.6.2)\n",
            "Requirement already satisfied: python-levenshtein in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (0.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein->fuzzyset) (41.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrMlO6iPL6mU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WikiConLLTagger():\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        self.page = PageContents(url)\n",
        "        self.info_card = InfoCard(self.page)\n",
        "#         print(sorted(self.tag_factory.get_annotations(), key=lambda annot: annot[1][0]))\n",
        "        \n",
        "    def _get_annotations(self):\n",
        "        '''stack all the annotations'''\n",
        "        return sorted(sorted(self.tag_factory.get_annotations(), key=lambda annot: annot[1][0]), reverse=True) # sort them by the start index of the annotaion token\n",
        "       \n",
        "#     def get_metadata(self):\n",
        "#         pe = self.private_entities\n",
        "#         # pe = [list(map(lambda li: item[1], item)) for item in pe.items()]\n",
        "#         dp = dict()\n",
        "#         [dp.update({item[0]: item[1][1]}) for item in pe.items()]\n",
        "#         dp.update({'URL' : self.url})\n",
        "#         return dp\n",
        "    \n",
        "    def place_tags(self):\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.text = self.page.get_text_chunk()\n",
        "        self.private_entities = PrivateEntities(self.info_card).entity_dict   \n",
        "        self.doc = nlp(self.text)\n",
        "#         self.spacy_noun_chunks =  [(chunk.text, chunk.start, chunk.end) for chunk in self.doc.noun_chunks]\n",
        "        self.tag_factory = TagFactory(self)\n",
        "        annotations = self._get_annotations()\n",
        "        words = []\n",
        "        tags = []\n",
        "        annotation_tags = ['O']*len(self.doc)\n",
        "#         annotation = annotations.pop()\n",
        "        for annotation in annotations:\n",
        "            annotation_tags[annotation[1][0]] = 'B-'+annotation[0] #\n",
        "            for i in range(annotation[1][1]-annotation[1][0]-1):\n",
        "                annotation_tags[annotation[1][0]+i+1] = 'I-'+annotation[0]\n",
        "#         for token in self.doc:\n",
        "#             words.append(token)\n",
        "#             if token\n",
        "        for sentence in self.doc.sents:\n",
        "            tags.extend(annotation_tags[sentence.start:sentence.end])\n",
        "            tags.append('\\n')\n",
        "            for token in sentence:\n",
        "                words.append(token.text)\n",
        "            words.append('\\n')\n",
        "#         print(words)\n",
        "#         print(tags)\n",
        "        return {'words': words, 'tags':tags}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p315V1HpXryO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TagFactory():\n",
        "    '''will keep a set of all the tagged annotations in the form of\n",
        "       ('word phrase', 'ENTITY', start, end), each tagging function will update the list'''\n",
        "    def __init__(self, tagger):\n",
        "        self.doc =tagger.doc\n",
        "        self.entity_dict = tagger.private_entities\n",
        "        self.tag_set = set()        \n",
        "        self.neg_tag_set = set()\n",
        "        self.spacy_noun_chunks =  [(chunk.text, chunk.start, chunk.end) for chunk in self.doc.noun_chunks]\n",
        "        tagging_methods = [getattr(self, method) for method in dir(self) if callable(getattr(self, method)) and re.match('_tag_.*', method)]\n",
        "        for method in tagging_methods:\n",
        "            method()\n",
        "            \n",
        "    def get_tag_summary(self):\n",
        "        BD_tags = len([y for y in self.tag_set if y[1]=='BD'])\n",
        "        NBD_tags = len([y for y in self.neg_tag_set if y[1]=='NBD']) \n",
        "#         [print(y) for y in self.neg_tag_set if y[1]=='NBD']\n",
        "        CH_tags = len([y for y in self.tag_set if y[1]=='CH'])\n",
        "        PR_tags = len([y for y in self.tag_set if y[1]=='PR'])\n",
        "        SP_tags = len([y for y in self.tag_set if y[1]=='SP'])\n",
        "        NPE_tags = len([y for y in self.neg_tag_set if y[1]=='NPE'])\n",
        "        \n",
        "        BP_tags = len([y for y in self.tag_set if y[1]=='BP'])\n",
        "        NGPE_tags = len([y for y in self.neg_tag_set if y[1]=='NGPE'])\n",
        "        \n",
        "        ED_tags = len([y for y in self.tag_set if y[1]=='ED'])\n",
        "        NORG_tags = len([y for y in self.neg_tag_set if y[1]=='NORG'])\n",
        "        return {'BD': BD_tags,\n",
        "                'NBD': NBD_tags,\n",
        "                'CH': CH_tags,\n",
        "                'PR': PR_tags,\n",
        "                'SP': SP_tags,\n",
        "                'NPE': NPE_tags,\n",
        "                'BP': BP_tags,\n",
        "                'NGPE': NGPE_tags,\n",
        "                'ED': ED_tags,\n",
        "                'NORG': NORG_tags,\n",
        "                'SENTS': len([sent for sent in self.doc.sents])}\n",
        "\n",
        "    def get_annotations(self):  \n",
        "        '''list with ('ENTITY', start, end) '''\n",
        "        return [annot[1:] for annot in list(self.tag_set)]\n",
        "    \n",
        "    def _tag_bd(self):\n",
        "        logging.debug('------BIRTHDAY MATCHING------')\n",
        "        date_entities = []\n",
        "        if not 'BIRTH_DATE' in self.entity_dict:\n",
        "            return\n",
        "        birth_dates =  [entity  for entity_list in self.entity_dict['BIRTH_DATE'] for entity in entity_list['entity_list']] #[{'dict_key':'Children', 'type':'PERSON', 'entity_list':[]}]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'DATE':\n",
        "                date_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for date in date_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in date[0].split(): # add all the words into a fuzzy set from the entity\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(birth_dates): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (date[0], 'BD', date[2:]) # exact phrase, entity tag, token indices\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                else:\n",
        "                    annot = (date[0], 'NBD', date[2:]) # exact phrase, entity tag, token indices\n",
        "                    self.neg_tag_set.add(HashableTupleAnnotations(annot))\n",
        "    \n",
        "    def _tag_bp(self):\n",
        "        logging.debug('------BIRTH PLACES------')\n",
        "        place_entities = []\n",
        "        if not 'BIRTH_PLACE' in self.entity_dict:\n",
        "            return\n",
        "        birth_places = [entity  for entity_list in self.entity_dict['BIRTH_PLACE'] for entity in entity_list['entity_list']]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'GPE':\n",
        "                place_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "        for place in place_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in place[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(birth_places): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]==0.6):\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (place[0], 'BP', place[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                else:\n",
        "                    annot = (place[0], 'NGPE', place[2:]) # exact phrase, entity tag, token indices\n",
        "                    self.neg_tag_set.add(HashableTupleAnnotations(annot))\n",
        "                    \n",
        "        logging.debug('------NOUN CHUNCK MATCHING FOR BP------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(birth_places): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]==0.6):\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'BP', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'BP', noun[1:])))\n",
        "#                 else:\n",
        "#                     self.neg_tag_set.add(HashableTupleAnnotations((noun[0], 'NBP', noun[1:])))\n",
        "    def _tag_spouse(self):\n",
        "        logging.debug('------SPOUSES NAMES------')\n",
        "        spouse_entities = []\n",
        "        if not 'SPOUSES' in self.entity_dict:\n",
        "            return\n",
        "        spouse_names = [entity  for entity_list in self.entity_dict['SPOUSES'] for entity in entity_list['entity_list']]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'PERSON':\n",
        "                spouse_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for spouse in spouse_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            # for word in spouse[0].split(): # add all the words into a fuzzy set from the entity\n",
        "            #     fz.add(word)  \n",
        "            fz.add(spouse[0])                  \n",
        "            for j, detail in enumerate(spouse_names): # get a detail in the list under an info line(things coming from the side info box)\n",
        "                # matched_list = []\n",
        "                # tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                # for word in detail.split(): # get a word from the detail\n",
        "                #     result = fz.get(word)    # get the matching \n",
        "                #     if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                #         matched_list.append((word, result))\n",
        "                result = fz.get(detail.split()[0])    # for a name, it should match the first name\n",
        "                if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(spouse[0])/2): #if the matching confidence is high and word length is high\n",
        "                    annot = (spouse[0], 'SP', spouse[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                else:\n",
        "                    annot = (spouse[0], 'NPE', spouse[2:]) # exact phrase, entity tag, token indices\n",
        "                    self.neg_tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING FOR SPOUSES------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(spouse_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                # tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for wn, word in enumerate(detail.split()): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2: #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                    elif wn == 0: #if the first word doesn't match, break the matching\n",
        "                        break\n",
        "                if  len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'SP', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'SP', noun[1:])))\n",
        "#                 else:\n",
        "#                     self.neg_tag_set.add(HashableTupleAnnotations((noun[0], 'NSP', noun[1:])))\n",
        "                \n",
        "    def _tag_edu(self):\n",
        "        logging.debug('------EDUCATION INSTITUTIONS------')\n",
        "        edu_entities = []\n",
        "        if not 'EDUCATION' in self.entity_dict:\n",
        "            return\n",
        "        edu_names = [entity  for entity_list in self.entity_dict['EDUCATION'] for entity in entity_list['entity_list']]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'ORG':\n",
        "                edu_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for edu in edu_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in edu[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(edu_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.8 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (edu[0], 'ED', edu[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                else:\n",
        "                    annot = (edu[0], 'NORG', edu[2:])\n",
        "#                     logging.debug(annot)\n",
        "                    self.neg_tag_set.add(HashableTupleAnnotations(annot))\n",
        "                    \n",
        "        logging.debug('------NOUN CHUNCK MATCHING FOR EDUCATION------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(edu_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.9 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'ED', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'ED', noun[1:])))\n",
        "                # elif noun[0].split()[0] ==  detail.split()[0] and len(noun[0].split())>1: # lets add an exception for University name matching\n",
        "                    # logging.debug((noun[0], 'ED', noun[1:]))\n",
        "#                     self.tag_set.add(HashableTupleAnnotations((noun[0], 'ED', noun[1:])))\n",
        "#                 else:\n",
        "#                     self.neg_tag_set.add(HashableTupleAnnotations((noun[0], 'NED', noun[1:])))\n",
        "            \n",
        "    def _tag_children(self):\n",
        "        logging.debug('------CHILDREN NAMES------')\n",
        "        children_entities = []\n",
        "        if not 'CHILDREN' in self.entity_dict:\n",
        "            return\n",
        "        children_names = [entity  for entity_list in self.entity_dict['CHILDREN'] for entity in entity_list['entity_list']]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'PERSON':\n",
        "                children_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for child in children_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            # for word in child[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "            #     fz.add(word)        \n",
        "            # for j, detail in enumerate(children_names): # get a detail in the list under an info line\n",
        "            #     matched_list = []\n",
        "            #     tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "            #     for word in detail.split(): # get a word from the detail\n",
        "            #         result = fz.get(word)    # get the matching \n",
        "            #         if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "            #             matched_list.append((word, result))\n",
        "            #     if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "            #         annot = (child[0], 'CH', child[2:])\n",
        "            #         logging.debug(annot)\n",
        "            #         self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "            fz.add(child[0])\n",
        "            for j, detail in enumerate(children_names): # get a detail in the list under an info line(things coming from the side info box)            \n",
        "                result = fz.get(detail.split()[0])    # for a name, it should match the first name\n",
        "                if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(child[0])/2): #if the matching confidence is high and word length is high\n",
        "                    annot = (child[0], 'CH', child[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                else:\n",
        "                    annot = (child[0], 'NPE', child[2:])\n",
        "#                     logging.debug(annot)\n",
        "                    self.neg_tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING CHILDREN------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(children_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                # tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                # for word in detail.split(): # get a word from the detail\n",
        "                #     result = fz.get(word)    # get the matching \n",
        "                #     if result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2 \\\n",
        "                #               and result[0][1][0] == word[0]: #if the matching confidence is high and word length is high\n",
        "                #         matched_list.append((word, result))\n",
        "                # if  len(matched_list)!=0:\n",
        "                #     logging.debug((noun[0], 'CH', noun[1:]))\n",
        "                #     self.tag_set.add(HashableTupleAnnotations((noun[0], 'CH', noun[1:])))\n",
        "                for wn, word in enumerate(detail.split()): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2: #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                    elif wn == 0: #if the first word doesn't match, break the matching\n",
        "                        break\n",
        "                if  len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'CH', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'CH', noun[1:])))\n",
        "#                 else:\n",
        "#                     self.neg_tag_set.add(HashableTupleAnnotations((noun[0], 'NCH', noun[1:])))\n",
        "\n",
        "    def _tag_parents(self):\n",
        "        logging.debug('------PARENTS NAMES------')\n",
        "        parent_entities = []\n",
        "        if not 'PARENTS' in self.entity_dict:\n",
        "            return\n",
        "        parent_names = [entity  for entity_list in self.entity_dict['PARENTS'] for entity in entity_list['entity_list']]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'PERSON':\n",
        "        #         print(entity.text, entity.label_, entity.start, entity.end)\n",
        "                parent_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "        logging.debug('------PERSON ENTITY MATCHING FOR PARENTS------')\n",
        "        for parent in parent_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in parent[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(parent_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (parent[0], 'PR', parent[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                else:\n",
        "                    annot = (parent[0], 'NPE', parent[2:])\n",
        "#                     logging.debug(annot)\n",
        "                    self.neg_tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING FOR PARENTS------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            noun_words = noun[0].split()\n",
        "            for word in noun_words: # add all the words from the noun phrase into a fuzzy set\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(parent_names): # get a one candidate\n",
        "                matched_list = []\n",
        "                tokens_in_detail = detail.split()\n",
        "                if len(noun_words) < len(tokens_in_detail):\n",
        "                    continue\n",
        "                for i, word in enumerate(tokens_in_detail): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if result and result[0][0]>=0.5 and not len(result[0][1]) < len(word) \\\n",
        "                              and result[0][1][0] == word[0]: #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result, i))\n",
        "                # if  len(matched_list)!=0 and matched_list[0][2]==0: #and noun[0].split()[0][0] == detail.split()[0][0]:\n",
        "                #     logging.debug((noun[0], 'PR', noun[1:]))\n",
        "    #                 self.tag_set.add(HashableTupleAnnotations((noun[0], 'PR', noun[1:])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WbWxzzwGXAN",
        "colab_type": "text"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95y0R0RbNdnr",
        "colab_type": "code",
        "outputId": "66e06aa1-a903-4e31-c59d-7bb9f23b22cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#experiment_code\n",
        "# tagger = WikiConLLTagger('https://en.wikipedia.org/wiki/Barack_Obama')\n",
        "# tagger = WikiConLLTagger('https://en.wikipedia.org/wiki/Donald_Trump')\n",
        "tagger = WikiConLLTagger('https://en.wikipedia.org/wiki/Mahinda_Rajapaksa')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEBUG:root:{'Prime Minister': ['Ranil Wickremesinghe'], 'Preceded by': ['D.P. Atapattu'], 'Succeeded by': ['Ranjit Atapattu', ''], 'President': ['D. B. Wijetunga', 'Chandrika Kumaratunga'], 'Born': ['Percy Mahendra Rajapaksa', '', '1945-11-18', ')', '18 November 1945', '', 'Weerakatiya,', 'Southern Province', ',', 'British Ceylon', '', 'Sri Lanka', ')'], 'Nationality': ['Sri Lankan'], 'Political party': ['Sri Lanka Podujana Peramuna', '– Present)', '', 'Sri Lanka Freedom Party', '2018)'], 'Spouse(s)': ['Shiranthi Rajapaksa', '', 'née', 'Wickremesinghe)'], 'Children': ['Namal', 'Yoshitha', 'Rohitha'], 'Residence': ['Medamulana Walawwa'], 'Alma mater': ['Sri Lanka Law College'], 'Profession': ['Attorney at law'], 'Website': ['Official website']}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "info card is scraped successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y23_nvZzFkFH",
        "outputId": "231964a6-ddee-4120-8440-1ab96f212b73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#experiment_code\n",
        "logger.setLevel(logging.INFO)\n",
        "df = pd.DataFrame(data = tagger.place_tags())\n",
        "df.to_csv(r'conll_annot.csv', index = None, header=True)\n",
        "print(tagger.tag_factory.entity_dict)\n",
        "print(tagger.tag_factory.get_tag_summary())\n",
        "print(df)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'NAME': [{'dict_key': 'Born', 'type': 'PERSON', 'entity_list': ['Percy Mahendra Rajapaksa', 'Weerakatiya']}], 'BIRTH_DATE': [{'dict_key': 'Born', 'type': 'DATE', 'entity_list': ['1945-11-18', '18 November 1945']}], 'BIRTH_PLACE': [{'dict_key': 'Born', 'type': 'GPE', 'entity_list': ['Weerakatiya,', 'British Ceylon', 'Sri Lanka']}], 'CHILDREN': [{'dict_key': 'Children', 'type': 'PERSON', 'entity_list': ['Namal', 'Yoshitha', 'Rohitha']}], 'SPOUSES': [{'dict_key': 'Spouse(s)', 'type': 'PERSON', 'entity_list': ['Shiranthi Rajapaksa', 'Wickremesinghe']}], 'PARENTS': [], 'EDUCATION': [{'dict_key': 'Alma mater', 'type': 'ORG', 'entity_list': ['Sri Lanka Law College']}]}\n",
            "{'BD': 0, 'NBD': 143, 'CH': 9, 'PR': 0, 'SP': 19, 'NPE': 122, 'BP': 0, 'NGPE': 83, 'ED': 2, 'NORG': 347, 'SENTS': 276}\n",
            "             words tags\n",
            "0                     O\n",
            "1            Percy    O\n",
            "2         Mahendra    O\n",
            "3        Rajapaksa    O\n",
            "4               \\n   \\n\n",
            "5                (    O\n",
            "6          Sinhala    O\n",
            "7                :    O\n",
            "8           මහින්ද    O\n",
            "9          රාජපක්ෂ    O\n",
            "10               ,    O\n",
            "11              \\n   \\n\n",
            "12           Tamil    O\n",
            "13               :    O\n",
            "14              \\n   \\n\n",
            "15          மஹிந்த    O\n",
            "16         ராஜபக்ஷ    O\n",
            "17               ;    O\n",
            "18            born    O\n",
            "19              18    O\n",
            "20        November    O\n",
            "21               )    O\n",
            "22              is    O\n",
            "23               a    O\n",
            "24             Sri    O\n",
            "25          Lankan    O\n",
            "26      politician    O\n",
            "27         serving    O\n",
            "28              as    O\n",
            "29          Leader    O\n",
            "...            ...  ...\n",
            "7576     Doctorate    O\n",
            "7577            by    O\n",
            "7578           the    O\n",
            "7579       Beijing    O\n",
            "7580    University    O\n",
            "7581            of    O\n",
            "7582       Foreign    O\n",
            "7583     Languages    O\n",
            "7584            in    O\n",
            "7585         China    O\n",
            "7586            in    O\n",
            "7587        August    O\n",
            "7588          2011    O\n",
            "7589             .    O\n",
            "7590                  O\n",
            "7591            \\n   \\n\n",
            "7592             *    O\n",
            "7593            \\n   \\n\n",
            "7594     Appointed    O\n",
            "7595           MPs    O\n",
            "7596          were    O\n",
            "7597     abolished    O\n",
            "7598            in    O\n",
            "7599          1972    O\n",
            "7600            by    O\n",
            "7601           the    O\n",
            "7602         First    O\n",
            "7603    Republican    O\n",
            "7604  Constitution    O\n",
            "7605            \\n   \\n\n",
            "\n",
            "[7606 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4vG3X0nn3kL",
        "colab_type": "code",
        "outputId": "fcb46f3c-291b-4df7-ffcd-c50701da88c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#experiment_code\n",
        "fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "# for word in spouse[0].split(): # add all the words into a fuzzy set from the entity\n",
        "# fz.add('Shiranthi')\n",
        "fz.add('Wickremasinghe')\n",
        "fz.add('Shiranthi Wickremasinghe')\n",
        "        \n",
        "# for j, detail in enumerate(spouse_names): # get a detail in the list under an info line\n",
        "#     matched_list = []\n",
        "#     tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "#     for word in detail.split(): # get a word from the detail\n",
        "#         result = fz.get(word)    # get the matching \n",
        "#         if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "#             matched_list.append((word, result))\n",
        "\n",
        "result = fz.get('Wickremasinghe')\n",
        "result"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 'Wickremasinghe')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    }
  ]
}