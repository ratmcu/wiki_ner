{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conll_tagged_ne.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratmcu/wiki_ner/blob/master/conll_tagged_ne.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcIYSgqMLy8V",
        "colab_type": "code",
        "outputId": "6427e15e-adda-4f0d-a9e6-82543fba7f5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import wget\n",
        "try:\n",
        "    import colabimport\n",
        "except:\n",
        "    colabimporturl = 'https://github.com/ratmcu/colaboratory_import/raw/master/colabimport.py'\n",
        "    filename = colabimporturl.split(\"/\")[-1].split(\"?\")[0]\n",
        "    if os.path.isfile(filename):\n",
        "        os.remove(filename)\n",
        "    wget.download(colabimporturl)\n",
        "    import colabimport\n",
        "colabimport.get_notebook('https://github.com/ratmcu/wiki_ner/blob/master/reusable_annotator.ipynb?raw=true')\n",
        "colabimport.get_notebook('https://github.com/ratmcu/wiki_ner/blob/master/info_box.ipynb?raw=true')\n",
        "# import io, os, sys, types\n",
        "from reusable_annotator import PageContents\n",
        "from info_box import InfoCard, PrivateEntities\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "# logging.debug(\"test\")\n",
        "class HashableTupleAnnotations(tuple):\n",
        "    def __hash__(self):\n",
        "        return hash(tuple(sorted([self[0:1],self[1:2]])))\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "import re\n",
        "!pip install pyahocorasick\n",
        "!pip install fuzzyset\n",
        "from ahocorasick import Automaton\n",
        "import fuzzyset\n",
        "from operator import itemgetter, attrgetter\n",
        "import pandas as pd\n",
        "# import json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.4)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.6.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.0.7)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy) (4.28.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: fuzzyset in /usr/local/lib/python3.6/dist-packages (0.0.19)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (1.6.2)\n",
            "Requirement already satisfied: python-levenshtein in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (0.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein->fuzzyset) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrMlO6iPL6mU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WikiConLLTagger():\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        self.page = PageContents(url)\n",
        "        self.text = self.page.get_text_chunk()\n",
        "        self.info_card = InfoCard(self.page)\n",
        "        self.private_entities = PrivateEntities(self.info_card).entity_dict   \n",
        "        self.doc = nlp(self.text)\n",
        "        self.spacy_noun_chunks =  [(chunk.text, chunk.start, chunk.end) for chunk in self.doc.noun_chunks]\n",
        "        self.tag_factory = TagFactory(self)\n",
        "#         print(sorted(self.tag_factory.get_annotations(), key=lambda annot: annot[1][0]))\n",
        "        \n",
        "    def _get_annotations(self):\n",
        "        '''stack all the annotations'''\n",
        "        return sorted(sorted(self.tag_factory.get_annotations(), key=lambda annot: annot[1][0]), reverse=True) # sort them by the start index of the annotaion token\n",
        "       \n",
        "    def get_metadata(self):\n",
        "        pe = self.private_entities\n",
        "        # pe = [list(map(lambda li: item[1], item)) for item in pe.items()]\n",
        "        dp = dict()\n",
        "        [dp.update({item[0]: item[1][1]}) for item in pe.items()]\n",
        "        dp.update({'URL' : self.url})\n",
        "        return dp\n",
        "        \n",
        "    def place_tags(self):\n",
        "        annotations = self._get_annotations()\n",
        "        words = []\n",
        "        tags = []\n",
        "        annotation_tags = ['O']*len(self.doc)\n",
        "#         annotation = annotations.pop()\n",
        "        for annotation in annotations:\n",
        "            annotation_tags[annotation[1][0]] = 'B-'+annotation[0]\n",
        "            for i in range(annotation[1][1]-annotation[1][0]-1):\n",
        "                annotation_tags[annotation[1][0]+i+1] = 'I-'+annotation[0]\n",
        "#         for token in self.doc:\n",
        "#             words.append(token)\n",
        "#             if token\n",
        "        for sentence in self.doc.sents:\n",
        "            tags.extend(annotation_tags[sentence.start:sentence.end])\n",
        "            tags.append('\\n')\n",
        "            for token in sentence:\n",
        "                words.append(token)\n",
        "            words.append('\\n')\n",
        "#         print(words)\n",
        "#         print(tags)\n",
        "        return {'words': words, 'tags':tags}\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p315V1HpXryO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TagFactory():\n",
        "    '''will keep a set of all the tagged annotations in the form of\n",
        "       ('word phrase', 'ENTITY', start, end), each tagging function will update the list'''\n",
        "    def __init__(self, tagger):\n",
        "        self.doc =tagger.doc\n",
        "        self.entity_dict = tagger.private_entities\n",
        "        self.tag_set = set()\n",
        "        self.spacy_noun_chunks =  [(chunk.text, chunk.start, chunk.end) for chunk in self.doc.noun_chunks]\n",
        "        tagging_methods = [getattr(self, method) for method in dir(self) if callable(getattr(self, method)) and re.match('_tag_.*', method)]\n",
        "        for method in tagging_methods:\n",
        "            method()\n",
        "            \n",
        "    def get_annotations(self):  \n",
        "        '''list with ('ENTITY', start, end) '''\n",
        "        return [annot[1:] for annot in list(self.tag_set)]\n",
        "    \n",
        "    def _tag_bd(self):\n",
        "        date_entities = []\n",
        "        if not 'BIRTH_DATE' in entity_dict:\n",
        "            return\n",
        "        birth_dates = self.entity_dict['BIRTH_DATE'][1]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'DATE':\n",
        "                date_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for date in date_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in date[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(birth_dates): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (date[0], 'BD', date[2:])\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "    \n",
        "    def _tag_bp(self):\n",
        "        place_entities = []\n",
        "        if not 'BIRTH_PLACE' in entity_dict:\n",
        "            return\n",
        "        birth_places = self.entity_dict['BIRTH_PLACE'][1]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'GPE':\n",
        "                place_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "        for place in place_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in place[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(birth_places): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]==1):\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (place[0], 'BP', place[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(birth_places): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]==1):\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'BP', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'BP', noun[1:])))\n",
        "\n",
        "    def _tag_spouse(self):\n",
        "        children_entities = []\n",
        "        if not 'SPOUSES' in entity_dict:\n",
        "            return\n",
        "        children_names = self.entity_dict['SPOUSES'][1]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'PERSON':\n",
        "                children_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for child in children_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in child[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(children_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (child[0], 'SP', child[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(children_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if  len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'SP', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'SP', noun[1:])))\n",
        "                \n",
        "    def _tag_edu(self):\n",
        "        children_entities = []\n",
        "        if not 'EDUCATION' in entity_dict:\n",
        "            return\n",
        "        children_names = self.entity_dict['EDUCATION'][1]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'ORG':\n",
        "                children_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for child in children_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in child[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(children_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.8 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (child[0], 'ED', child[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                    \n",
        "        logging.debug('------NOUN CHUNCK MATCHING------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(children_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.9 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'ED', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'ED', noun[1:])))\n",
        "                elif noun[0].split()[0] ==  detail.split()[0] and len(noun[0].split())>1: # lets add an exception for University name matching\n",
        "                    logging.debug((noun[0], 'ED', noun[1:]))\n",
        "#                     self.tag_set.add(HashableTupleAnnotations((noun[0], 'ED', noun[1:])))\n",
        "            \n",
        "    def _tag_children(self):\n",
        "        children_entities = []\n",
        "        if not 'CHILDREN' in entity_dict:\n",
        "            return\n",
        "        children_names = self.entity_dict['CHILDREN'][1]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'PERSON':\n",
        "                children_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for child in children_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in child[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(children_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (child[0], 'CH', child[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "\n",
        "                    logging.debug('------NOUN CHUNCK MATCHING------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(children_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2 \\\n",
        "                              and result[0][1][0] == word[0]: #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if  len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'CH', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'CH', noun[1:])))\n",
        "            \n",
        "            \n",
        "    def _tag_parents(self):\n",
        "        parent_entities = []\n",
        "        if not 'PARENTS' in entity_dict:\n",
        "            return\n",
        "        parent_names = self.entity_dict['PARENTS'][1]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'PERSON':\n",
        "        #         print(entity.text, entity.label_, entity.start, entity.end)\n",
        "                parent_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for parent in parent_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in parent[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(parent_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (parent[0], 'PR', parent[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            noun_words = noun[0].split()\n",
        "            for word in noun_words: # add all the words from the noun phrase into a fuzzy set\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(parent_names): # get a one candidate\n",
        "                matched_list = []\n",
        "                tokens_in_detail = detail.split()\n",
        "                if len(noun_words) < len(tokens_in_detail):\n",
        "                    continue\n",
        "                for i, word in enumerate(tokens_in_detail): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if result and result[0][0]>=0.5 and not len(result[0][1]) < len(word) \\\n",
        "                              and result[0][1][0] == word[0]: #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result, i))\n",
        "                if  len(matched_list)!=0 and matched_list[0][2]==0: #and noun[0].split()[0][0] == detail.split()[0][0]:\n",
        "                    logging.debug((noun[0], 'PR', noun[1:]))\n",
        "    #                 self.tag_set.add(HashableTupleAnnotations((noun[0], 'PR', noun[1:])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95y0R0RbNdnr",
        "colab_type": "code",
        "outputId": "a2c4a6ee-3549-4649-8802-e9785946131d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "#experiment_code\n",
        "# tagger = WikiConLLTagger('https://en.wikipedia.org/wiki/Barack_Obama')\n",
        "tagger = WikiConLLTagger('https://en.wikipedia.org/wiki/Donald_Trump')\n",
        "print(tagger.get_metadata())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "info card is scraped successfully\n",
            "[['Born', 'PERSON'], ['Donald John Trump']]\n",
            "[['Born', 'DATE'], ['1946-06-14', 'June 14, 1946']]\n",
            "[['Born', 'GPE'], ['New York City', 'Queens']]\n",
            "[['Children', 'PERSON'], ['Donald Jr.', 'Ivanka', 'Eric', 'Tiffany', 'Barron']]\n",
            "[['Spouse(s)', 'PERSON'], ['Ivana Zelníčková', 'Marla Maples', 'Melania Knauss']]\n",
            "[['Parents', 'PERSON'], ['Fred Trump', 'Mary Anne MacLeod']]\n",
            "[['Education', 'ORG'], ['The Wharton School']]\n",
            "b'{\"NAME\": [\"Donald John Trump\"], \"BIRTH_DATE\": [\"1946-06-14\", \"June 14, 1946\"], \"BIRTH_PLACE\": [\"New York City\", \"Queens\"], \"CHILDREN\": [\"Donald Jr.\", \"Ivanka\", \"Eric\", \"Tiffany\", \"Barron\"], \"SPOUSES\": [\"Ivana Zeln\\xc3\\xad\\xc4\\x8dkov\\xc3\\xa1\", \"Marla Maples\", \"Melania Knauss\"], \"PARENTS\": [\"Fred Trump\", \"Mary Anne MacLeod\"], \"EDUCATION\": [\"The Wharton School\"], \"URL\": \"https://en.wikipedia.org/wiki/Donald_Trump\"}'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y23_nvZzFkFH",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "df = pd.DataFrame(data = tagger.place_tags())\n",
        "df.to_csv(r'conll_annot.csv', index = None, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}