{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conll_tagged_ne.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratmcu/wiki_ner/blob/master/conll_tagged_ne.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcIYSgqMLy8V",
        "colab_type": "code",
        "outputId": "06979bda-61c7-4eaf-838a-bf0a4dad1b92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import wget\n",
        "try:\n",
        "    import colabimport\n",
        "except:\n",
        "    colabimporturl = 'https://github.com/ratmcu/colaboratory_import/raw/master/colabimport.py'\n",
        "    filename = colabimporturl.split(\"/\")[-1].split(\"?\")[0]\n",
        "    if os.path.isfile(filename):\n",
        "        os.remove(filename)\n",
        "    wget.download(colabimporturl)\n",
        "    import colabimport\n",
        "colabimport.get_notebook('https://github.com/ratmcu/wiki_ner/blob/master/reusable_annotator.ipynb?raw=true')\n",
        "colabimport.get_notebook('https://github.com/ratmcu/wiki_ner/blob/master/info_box.ipynb?raw=true')\n",
        "# import io, os, sys, types\n",
        "from reusable_annotator import PageContents\n",
        "from info_box import InfoCard, PrivateEntities\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.DEBUG)\n",
        "# logging.debug(\"test\")\n",
        "class HashableTupleAnnotations(tuple):\n",
        "    def __hash__(self):\n",
        "        return hash(tuple(sorted([self[0:1],self[1:2]])))\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "import re\n",
        "!pip install pyahocorasick\n",
        "!pip install fuzzyset\n",
        "from ahocorasick import Automaton\n",
        "import fuzzyset\n",
        "from operator import itemgetter, attrgetter\n",
        "import pandas as pd\n",
        "# import json"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=0191ceb0f45eabd5194616f82e9c18ba868aba262bf0828c75277c160e978c77\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "importing Jupyter notebook from reusable_annotator.ipynb\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 2.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81706 sha256=32400fa480f5aacae160e0ca1a75929dd20252afb7bee4ef00a4245f0337cd25\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick\n",
            "Successfully installed pyahocorasick-1.4.0\n",
            "Collecting fuzzyset\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/78/7509f3efbb6acbcf842d7bdbd9a919ca8c0ed248123bdd8c57f08497e0dd/fuzzyset-0.0.19.tar.gz (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 2.8MB/s \n",
            "\u001b[?25hCollecting python-levenshtein (from fuzzyset)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 20.3MB/s \n",
            "\u001b[?25hCollecting texttable (from fuzzyset)\n",
            "  Downloading https://files.pythonhosted.org/packages/82/a8/60df592e3a100a1f83928795aca210414d72cebdc6e4e0c95a6d8ac632fe/texttable-1.6.2.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein->fuzzyset) (41.2.0)\n",
            "Building wheels for collected packages: fuzzyset, python-levenshtein, texttable\n",
            "  Building wheel for fuzzyset (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fuzzyset: filename=fuzzyset-0.0.19-cp36-cp36m-linux_x86_64.whl size=167545 sha256=56bb29a30f29b4d8398abe78026e1e2f1383adb5e107e0a627b7614c00aa0637\n",
            "  Stored in directory: /root/.cache/pip/wheels/d8/36/9a/8f1cac047c7c3b03dce3d5434ed0088bfd8da8aeca615dfb4c\n",
            "  Building wheel for python-levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144663 sha256=0feabc466f35f3d3a4bec6bdc17625dae7bd6329ae740d085fa0cf3e56f76a83\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "  Building wheel for texttable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for texttable: filename=texttable-1.6.2-cp36-none-any.whl size=10654 sha256=2c0fdd312bd5c685333d4623623b81b58248daf8d718b74de91601f63d57c3a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/d1/d6/dfbe4eb3c468832f7fbe4bd27f9875fa97277cabed8fb6715c\n",
            "Successfully built fuzzyset python-levenshtein texttable\n",
            "Installing collected packages: python-levenshtein, texttable, fuzzyset\n",
            "Successfully installed fuzzyset-0.0.19 python-levenshtein-0.12.0 texttable-1.6.2\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "importing Jupyter notebook from info_box.ipynb\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: fuzzyset in /usr/local/lib/python3.6/dist-packages (0.0.19)\n",
            "Requirement already satisfied: python-levenshtein in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (0.12.0)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (1.6.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein->fuzzyset) (41.2.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.5)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.1.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.2)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 2.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.8.19-cp36-cp36m-linux_x86_64.whl size=609223 sha256=45fcf78f36e7dfa1b105b4cd3b62bfc6a5a80ee161920189241e1336ed19f4bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\n",
            "Successfully built regex\n",
            "Installing collected packages: regex\n",
            "Successfully installed regex-2019.8.19\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.8)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.1.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: fuzzyset in /usr/local/lib/python3.6/dist-packages (0.0.19)\n",
            "Requirement already satisfied: python-levenshtein in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (0.12.0)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (1.6.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein->fuzzyset) (41.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrMlO6iPL6mU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WikiConLLTagger():\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        self.page = PageContents(url)\n",
        "        self.info_card = InfoCard(self.page)\n",
        "#         print(sorted(self.tag_factory.get_annotations(), key=lambda annot: annot[1][0]))\n",
        "        \n",
        "    def _get_annotations(self):\n",
        "        '''stack all the annotations'''\n",
        "        return sorted(sorted(self.tag_factory.get_annotations(), key=lambda annot: annot[1][0]), reverse=True) # sort them by the start index of the annotaion token\n",
        "       \n",
        "#     def get_metadata(self):\n",
        "#         pe = self.private_entities\n",
        "#         # pe = [list(map(lambda li: item[1], item)) for item in pe.items()]\n",
        "#         dp = dict()\n",
        "#         [dp.update({item[0]: item[1][1]}) for item in pe.items()]\n",
        "#         dp.update({'URL' : self.url})\n",
        "#         return dp\n",
        "    \n",
        "    def place_tags(self):\n",
        "        self.text = self.page.get_text_chunk()\n",
        "        self.private_entities = PrivateEntities(self.info_card).entity_dict   \n",
        "        self.doc = nlp(self.text)\n",
        "#         self.spacy_noun_chunks =  [(chunk.text, chunk.start, chunk.end) for chunk in self.doc.noun_chunks]\n",
        "        self.tag_factory = TagFactory(self)\n",
        "        annotations = self._get_annotations()\n",
        "        words = []\n",
        "        tags = []\n",
        "        annotation_tags = ['O']*len(self.doc)\n",
        "#         annotation = annotations.pop()\n",
        "        for annotation in annotations:\n",
        "            annotation_tags[annotation[1][0]] = 'B-'+annotation[0] #\n",
        "            for i in range(annotation[1][1]-annotation[1][0]-1):\n",
        "                annotation_tags[annotation[1][0]+i+1] = 'I-'+annotation[0]\n",
        "#         for token in self.doc:\n",
        "#             words.append(token)\n",
        "#             if token\n",
        "        for sentence in self.doc.sents:\n",
        "            tags.extend(annotation_tags[sentence.start:sentence.end])\n",
        "            tags.append('\\n')\n",
        "            for token in sentence:\n",
        "                words.append(token)\n",
        "            words.append('\\n')\n",
        "#         print(words)\n",
        "#         print(tags)\n",
        "        return {'words': words, 'tags':tags}\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p315V1HpXryO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TagFactory():\n",
        "    '''will keep a set of all the tagged annotations in the form of\n",
        "       ('word phrase', 'ENTITY', start, end), each tagging function will update the list'''\n",
        "    def __init__(self, tagger):\n",
        "        self.doc =tagger.doc\n",
        "        self.entity_dict = tagger.private_entities\n",
        "        self.tag_set = set()\n",
        "        self.spacy_noun_chunks =  [(chunk.text, chunk.start, chunk.end) for chunk in self.doc.noun_chunks]\n",
        "        tagging_methods = [getattr(self, method) for method in dir(self) if callable(getattr(self, method)) and re.match('_tag_.*', method)]\n",
        "        for method in tagging_methods:\n",
        "            method()\n",
        "\n",
        "    def get_annotations(self):  \n",
        "        '''list with ('ENTITY', start, end) '''\n",
        "        return [annot[1:] for annot in list(self.tag_set)]\n",
        "    \n",
        "    def _tag_bd(self):\n",
        "        date_entities = []\n",
        "        if not 'BIRTH_DATE' in self.entity_dict:\n",
        "            return\n",
        "        birth_dates = self.entity_dict['BIRTH_DATE'][1]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'DATE':\n",
        "                date_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for date in date_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in date[0].split(): # add all the words into a fuzzy set from the entity\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(birth_dates): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (date[0], 'BD', date[2:]) # exact phrase, entity tag, token indices\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "    \n",
        "    def _tag_bp(self):\n",
        "        place_entities = []\n",
        "        if not 'BIRTH_PLACE' in self.entity_dict:\n",
        "            return\n",
        "        birth_places = self.entity_dict['BIRTH_PLACE'][1]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'GPE':\n",
        "                place_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "        for place in place_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in place[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(birth_places): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]==1):\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (place[0], 'BP', place[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(birth_places): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]==1):\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'BP', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'BP', noun[1:])))\n",
        "\n",
        "    def _tag_spouse(self):\n",
        "        spouse_entities = []\n",
        "        if not 'SPOUSES' in self.entity_dict:\n",
        "            return\n",
        "        spouse_names = self.entity_dict['SPOUSES'][1]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'PERSON':\n",
        "                spouse_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for spouse in spouse_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            # for word in spouse[0].split(): # add all the words into a fuzzy set from the entity\n",
        "            #     fz.add(word)  \n",
        "            fz.add(spouse[0])                  \n",
        "            for j, detail in enumerate(spouse_names): # get a detail in the list under an info line(things coming from the side info box)\n",
        "                # matched_list = []\n",
        "                # tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                # for word in detail.split(): # get a word from the detail\n",
        "                #     result = fz.get(word)    # get the matching \n",
        "                #     if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                #         matched_list.append((word, result))\n",
        "                result = fz.get(detail.split()[0])    # for a name, it should match the first name\n",
        "                if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(spouse[0])/2): #if the matching confidence is high and word length is high\n",
        "                    annot = (spouse[0], 'SP', spouse[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(spouse_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                # tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for wn, word in enumerate(detail.split()): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2: #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                    elif wn == 0: #if the first word doesn't match, break the matching\n",
        "                        break\n",
        "                if  len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'SP', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'SP', noun[1:])))\n",
        "                \n",
        "    def _tag_edu(self):\n",
        "        edu_entities = []\n",
        "        if not 'EDUCATION' in self.entity_dict:\n",
        "            return\n",
        "        edu_names = self.entity_dict['EDUCATION'][1]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'ORG':\n",
        "                edu_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for edu in edu_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in edu[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(edu_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.8 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (edu[0], 'ED', edu[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                    \n",
        "        logging.debug('------NOUN CHUNCK MATCHING------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(edu_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.9 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'ED', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'ED', noun[1:])))\n",
        "                # elif noun[0].split()[0] ==  detail.split()[0] and len(noun[0].split())>1: # lets add an exception for University name matching\n",
        "                    # logging.debug((noun[0], 'ED', noun[1:]))\n",
        "#                     self.tag_set.add(HashableTupleAnnotations((noun[0], 'ED', noun[1:])))\n",
        "            \n",
        "    def _tag_children(self):\n",
        "        children_entities = []\n",
        "        if not 'CHILDREN' in self.entity_dict:\n",
        "            return\n",
        "        children_names = self.entity_dict['CHILDREN'][1]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'PERSON':\n",
        "                children_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for child in children_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            # for word in child[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "            #     fz.add(word)        \n",
        "            # for j, detail in enumerate(children_names): # get a detail in the list under an info line\n",
        "            #     matched_list = []\n",
        "            #     tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "            #     for word in detail.split(): # get a word from the detail\n",
        "            #         result = fz.get(word)    # get the matching \n",
        "            #         if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "            #             matched_list.append((word, result))\n",
        "            #     if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "            #         annot = (child[0], 'CH', child[2:])\n",
        "            #         logging.debug(annot)\n",
        "            #         self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "            fz.add(child[0])\n",
        "            for j, detail in enumerate(children_names): # get a detail in the list under an info line(things coming from the side info box)            \n",
        "                result = fz.get(detail.split()[0])    # for a name, it should match the first name\n",
        "                if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(child[0])/2): #if the matching confidence is high and word length is high\n",
        "                    annot = (child[0], 'CH', child[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(children_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                # tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                # for word in detail.split(): # get a word from the detail\n",
        "                #     result = fz.get(word)    # get the matching \n",
        "                #     if result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2 \\\n",
        "                #               and result[0][1][0] == word[0]: #if the matching confidence is high and word length is high\n",
        "                #         matched_list.append((word, result))\n",
        "                # if  len(matched_list)!=0:\n",
        "                #     logging.debug((noun[0], 'CH', noun[1:]))\n",
        "                #     self.tag_set.add(HashableTupleAnnotations((noun[0], 'CH', noun[1:])))\n",
        "                for wn, word in enumerate(detail.split()): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2: #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                    elif wn == 0: #if the first word doesn't match, break the matching\n",
        "                        break\n",
        "                if  len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'CH', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'CH', noun[1:])))\n",
        "            \n",
        "    def _tag_parents(self):\n",
        "        parent_entities = []\n",
        "        if not 'PARENTS' in self.entity_dict:\n",
        "            return\n",
        "        parent_names = self.entity_dict['PARENTS'][1]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'PERSON':\n",
        "        #         print(entity.text, entity.label_, entity.start, entity.end)\n",
        "                parent_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "        logging.debug('------PERSON ENTITY MATCHING FOR PARENTS------')\n",
        "        for parent in parent_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in parent[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(parent_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (parent[0], 'PR', parent[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING FOR PARENTS------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            noun_words = noun[0].split()\n",
        "            for word in noun_words: # add all the words from the noun phrase into a fuzzy set\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(parent_names): # get a one candidate\n",
        "                matched_list = []\n",
        "                tokens_in_detail = detail.split()\n",
        "                if len(noun_words) < len(tokens_in_detail):\n",
        "                    continue\n",
        "                for i, word in enumerate(tokens_in_detail): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if result and result[0][0]>=0.5 and not len(result[0][1]) < len(word) \\\n",
        "                              and result[0][1][0] == word[0]: #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result, i))\n",
        "                # if  len(matched_list)!=0 and matched_list[0][2]==0: #and noun[0].split()[0][0] == detail.split()[0][0]:\n",
        "                #     logging.debug((noun[0], 'PR', noun[1:]))\n",
        "    #                 self.tag_set.add(HashableTupleAnnotations((noun[0], 'PR', noun[1:])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95y0R0RbNdnr",
        "colab_type": "code",
        "outputId": "3c1c8a60-5a5c-4197-ada5-c0ed629d027c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#experiment_code\n",
        "# tagger = WikiConLLTagger('https://en.wikipedia.org/wiki/Barack_Obama')\n",
        "# tagger = WikiConLLTagger('https://en.wikipedia.org/wiki/Donald_Trump')\n",
        "tagger = WikiConLLTagger('https://en.wikipedia.org/wiki/Mahinda_Rajapaksa')\n",
        "\n",
        "# print(tagger.get_metadata())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "info card is scraped successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y23_nvZzFkFH",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "df = pd.DataFrame(data = tagger.place_tags())\n",
        "df.to_csv(r'conll_annot.csv', index = None, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4vG3X0nn3kL",
        "colab_type": "code",
        "outputId": "19a82721-ac1f-4454-cebe-4e750376eef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#experiment_code\n",
        "fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "# for word in spouse[0].split(): # add all the words into a fuzzy set from the entity\n",
        "# fz.add('Shiranthi')\n",
        "fz.add('Wickremasinghe')\n",
        "fz.add('Shiranthi Wickremasinghe')\n",
        "        \n",
        "# for j, detail in enumerate(spouse_names): # get a detail in the list under an info line\n",
        "#     matched_list = []\n",
        "#     tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "#     for word in detail.split(): # get a word from the detail\n",
        "#         result = fz.get(word)    # get the matching \n",
        "#         if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "#             matched_list.append((word, result))\n",
        "\n",
        "result = fz.get('Wickremasinghe')\n",
        "result"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 'Wickremasinghe')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N02fl_r0aZX3",
        "colab_type": "code",
        "outputId": "b8d46cac-7f5e-47d8-b46e-9fe6bbbdcc38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}