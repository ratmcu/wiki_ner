{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conll_tagged_ne.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratmcu/wiki_ner/blob/master/conll_tagged_ne.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcIYSgqMLy8V",
        "colab_type": "code",
        "outputId": "e670a09c-cf27-41d3-ce04-184e1383b2e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import wget\n",
        "try:\n",
        "    import colabimport\n",
        "except:\n",
        "    colabimporturl = 'https://github.com/ratmcu/colaboratory_import/raw/master/colabimport.py'\n",
        "    filename = colabimporturl.split(\"/\")[-1].split(\"?\")[0]\n",
        "    if os.path.isfile(filename):\n",
        "        os.remove(filename)\n",
        "    wget.download(colabimporturl)\n",
        "    import colabimport\n",
        "colabimport.get_notebook('https://github.com/ratmcu/wiki_ner/blob/master/reusable_annotator.ipynb?raw=true')\n",
        "colabimport.get_notebook('https://github.com/ratmcu/wiki_ner/blob/master/info_box.ipynb?raw=true')\n",
        "# import io, os, sys, types\n",
        "from reusable_annotator import PageContents\n",
        "from info_box import InfoCard, PrivateEntities\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.DEBUG)\n",
        "# logging.debug(\"test\")\n",
        "class HashableTupleAnnotations(tuple):\n",
        "    def __hash__(self):\n",
        "        return hash(tuple(sorted([self[0:1],self[1:2]])))\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "import re\n",
        "!pip install pyahocorasick\n",
        "!pip install fuzzyset\n",
        "from ahocorasick import Automaton\n",
        "import fuzzyset\n",
        "from operator import itemgetter, attrgetter\n",
        "import pandas as pd\n",
        "# import json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=15b74434ff1a68bc5fe8a09291d4e4120759b1201f3036407a5a8b2fc839f68b\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "importing Jupyter notebook from reusable_annotator.ipynb\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 4.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81711 sha256=b7297e7bda9333f81cfd84199c7b333a174dd968bdef5f8f7bd023d3b9cea4df\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick\n",
            "Successfully installed pyahocorasick-1.4.0\n",
            "Collecting fuzzyset\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/78/7509f3efbb6acbcf842d7bdbd9a919ca8c0ed248123bdd8c57f08497e0dd/fuzzyset-0.0.19.tar.gz (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 5.1MB/s \n",
            "\u001b[?25hCollecting python-levenshtein (from fuzzyset)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 16.8MB/s \n",
            "\u001b[?25hCollecting texttable (from fuzzyset)\n",
            "  Downloading https://files.pythonhosted.org/packages/82/a8/60df592e3a100a1f83928795aca210414d72cebdc6e4e0c95a6d8ac632fe/texttable-1.6.2.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein->fuzzyset) (41.2.0)\n",
            "Building wheels for collected packages: fuzzyset, python-levenshtein, texttable\n",
            "  Building wheel for fuzzyset (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fuzzyset: filename=fuzzyset-0.0.19-cp36-cp36m-linux_x86_64.whl size=167535 sha256=4440f88b1c395bb001d7595b717ba70f3cf597ed87ed2b9d49d4ae41fd1ea4ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/d8/36/9a/8f1cac047c7c3b03dce3d5434ed0088bfd8da8aeca615dfb4c\n",
            "  Building wheel for python-levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144672 sha256=45ed6af5742d8c30847da47fac2a898a16facdfba424b0c4d1fceaeb86b40eea\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "  Building wheel for texttable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for texttable: filename=texttable-1.6.2-cp36-none-any.whl size=10654 sha256=02a05b50769bcf0209a5213d8c5de5b7f5343795beb95ca371afb1464a9ebe35\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/d1/d6/dfbe4eb3c468832f7fbe4bd27f9875fa97277cabed8fb6715c\n",
            "Successfully built fuzzyset python-levenshtein texttable\n",
            "Installing collected packages: python-levenshtein, texttable, fuzzyset\n",
            "Successfully installed fuzzyset-0.0.19 python-levenshtein-0.12.0 texttable-1.6.2\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "importing Jupyter notebook from info_box.ipynb\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: fuzzyset in /usr/local/lib/python3.6/dist-packages (0.0.19)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (1.6.2)\n",
            "Requirement already satisfied: python-levenshtein in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (0.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein->fuzzyset) (41.2.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.1.0)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.2)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 4.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.8.19-cp36-cp36m-linux_x86_64.whl size=609222 sha256=99b001cfa63a144590b7ec874731ce117167a9134003660748d2450015a65cb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\n",
            "Successfully built regex\n",
            "Installing collected packages: regex\n",
            "Successfully installed regex-2019.8.19\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.8)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.1.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: fuzzyset in /usr/local/lib/python3.6/dist-packages (0.0.19)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (1.6.2)\n",
            "Requirement already satisfied: python-levenshtein in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (0.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein->fuzzyset) (41.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrMlO6iPL6mU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WikiConLLTagger():\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        self.page = PageContents(url)\n",
        "        self.info_card = InfoCard(self.page)\n",
        "#         print(sorted(self.tag_factory.get_annotations(), key=lambda annot: annot[1][0]))\n",
        "        \n",
        "    def _get_annotations(self):\n",
        "        '''stack all the annotations'''\n",
        "        return sorted(sorted(self.tag_factory.get_annotations(), key=lambda annot: annot[1][0]), reverse=True) # sort them by the start index of the annotaion token\n",
        "       \n",
        "#     def get_metadata(self):\n",
        "#         pe = self.private_entities\n",
        "#         # pe = [list(map(lambda li: item[1], item)) for item in pe.items()]\n",
        "#         dp = dict()\n",
        "#         [dp.update({item[0]: item[1][1]}) for item in pe.items()]\n",
        "#         dp.update({'URL' : self.url})\n",
        "#         return dp\n",
        "    \n",
        "    def place_tags(self):\n",
        "        self.text = self.page.get_text_chunk()\n",
        "        self.private_entities = PrivateEntities(self.info_card).entity_dict   \n",
        "        self.doc = nlp(self.text)\n",
        "#         self.spacy_noun_chunks =  [(chunk.text, chunk.start, chunk.end) for chunk in self.doc.noun_chunks]\n",
        "        self.tag_factory = TagFactory(self)\n",
        "        annotations = self._get_annotations()\n",
        "        words = []\n",
        "        tags = []\n",
        "        annotation_tags = ['O']*len(self.doc)\n",
        "#         annotation = annotations.pop()\n",
        "        for annotation in annotations:\n",
        "            annotation_tags[annotation[1][0]] = 'B-'+annotation[0] #\n",
        "            for i in range(annotation[1][1]-annotation[1][0]-1):\n",
        "                annotation_tags[annotation[1][0]+i+1] = 'I-'+annotation[0]\n",
        "#         for token in self.doc:\n",
        "#             words.append(token)\n",
        "#             if token\n",
        "        for sentence in self.doc.sents:\n",
        "            tags.extend(annotation_tags[sentence.start:sentence.end])\n",
        "            tags.append('\\n')\n",
        "            for token in sentence:\n",
        "                words.append(token)\n",
        "            words.append('\\n')\n",
        "#         print(words)\n",
        "#         print(tags)\n",
        "        return {'words': words, 'tags':tags}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p315V1HpXryO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TagFactory():\n",
        "    '''will keep a set of all the tagged annotations in the form of\n",
        "       ('word phrase', 'ENTITY', start, end), each tagging function will update the list'''\n",
        "    def __init__(self, tagger):\n",
        "        self.doc =tagger.doc\n",
        "        self.entity_dict = tagger.private_entities\n",
        "        self.tag_set = set()        \n",
        "        self.neg_tag_set = set()\n",
        "        self.spacy_noun_chunks =  [(chunk.text, chunk.start, chunk.end) for chunk in self.doc.noun_chunks]\n",
        "        tagging_methods = [getattr(self, method) for method in dir(self) if callable(getattr(self, method)) and re.match('_tag_.*', method)]\n",
        "        for method in tagging_methods:\n",
        "            method()\n",
        "            \n",
        "    def get_tag_summary(self):\n",
        "        BD_tags = len([y for y in self.tag_set if y[1]=='BD'])\n",
        "        NBD_tags = len([y for y in self.neg_tag_set if y[1]=='NBD']) \n",
        "#         [print(y) for y in self.neg_tag_set if y[1]=='NBD']\n",
        "        CH_tags = len([y for y in self.tag_set if y[1]=='CH'])\n",
        "        PR_tags = len([y for y in self.tag_set if y[1]=='PR'])\n",
        "        SP_tags = len([y for y in self.tag_set if y[1]=='SP'])\n",
        "        NPE_tags = len([y for y in self.neg_tag_set if y[1]=='NPE'])\n",
        "        \n",
        "        BP_tags = len([y for y in self.tag_set if y[1]=='BP'])\n",
        "        NGPE_tags = len([y for y in self.neg_tag_set if y[1]=='NGPE'])\n",
        "        \n",
        "        ED_tags = len([y for y in self.tag_set if y[1]=='ED'])\n",
        "        NORG_tags = len([y for y in self.neg_tag_set if y[1]=='NORG'])\n",
        "        return {'BD': BD_tags,\n",
        "                'NBD': NBD_tags,\n",
        "                'CH': CH_tags,\n",
        "                'PR': PR_tags,\n",
        "                'SP': SP_tags,\n",
        "                'NPE': NPE_tags,\n",
        "                'BP': BP_tags,\n",
        "                'NGPE': NGPE_tags,\n",
        "                'ED': ED_tags,\n",
        "                'NORG': NORG_tags,\n",
        "                'SENTS': len([sent for sent in self.doc.sents])}\n",
        "\n",
        "    def get_annotations(self):  \n",
        "        '''list with ('ENTITY', start, end) '''\n",
        "        return [annot[1:] for annot in list(self.tag_set)]\n",
        "    \n",
        "    def _tag_bd(self):\n",
        "        logging.debug('------BIRTHDAY MATCHING------')\n",
        "        date_entities = []\n",
        "        if not 'BIRTH_DATE' in self.entity_dict:\n",
        "            return\n",
        "        birth_dates =  [entity  for entity_list in self.entity_dict['BIRTH_DATE'] for entity in entity_list['entity_list']] #[{'dict_key':'Children', 'type':'PERSON', 'entity_list':[]}]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'DATE':\n",
        "                date_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for date in date_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in date[0].split(): # add all the words into a fuzzy set from the entity\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(birth_dates): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (date[0], 'BD', date[2:]) # exact phrase, entity tag, token indices\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                else:\n",
        "                    annot = (date[0], 'NBD', date[2:]) # exact phrase, entity tag, token indices\n",
        "                    self.neg_tag_set.add(HashableTupleAnnotations(annot))\n",
        "    \n",
        "    def _tag_bp(self):\n",
        "        logging.debug('------BIRTH PLACES------')\n",
        "        place_entities = []\n",
        "        if not 'BIRTH_PLACE' in self.entity_dict:\n",
        "            return\n",
        "        birth_places = [entity  for entity_list in self.entity_dict['BIRTH_PLACE'] for entity in entity_list['entity_list']]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'GPE':\n",
        "                place_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "        for place in place_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in place[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(birth_places): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]==0.6):\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (place[0], 'BP', place[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                else:\n",
        "                    annot = (place[0], 'NGPE', place[2:]) # exact phrase, entity tag, token indices\n",
        "                    self.neg_tag_set.add(HashableTupleAnnotations(annot))\n",
        "                    \n",
        "        logging.debug('------NOUN CHUNCK MATCHING FOR BP------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(birth_places): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]==0.6):\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'BP', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'BP', noun[1:])))\n",
        "#                 else:\n",
        "#                     self.neg_tag_set.add(HashableTupleAnnotations((noun[0], 'NBP', noun[1:])))\n",
        "    def _tag_spouse(self):\n",
        "        logging.debug('------SPOUSES NAMES------')\n",
        "        spouse_entities = []\n",
        "        if not 'SPOUSES' in self.entity_dict:\n",
        "            return\n",
        "        spouse_names = [entity  for entity_list in self.entity_dict['SPOUSES'] for entity in entity_list['entity_list']]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'PERSON':\n",
        "                spouse_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for spouse in spouse_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            # for word in spouse[0].split(): # add all the words into a fuzzy set from the entity\n",
        "            #     fz.add(word)  \n",
        "            fz.add(spouse[0])                  \n",
        "            for j, detail in enumerate(spouse_names): # get a detail in the list under an info line(things coming from the side info box)\n",
        "                # matched_list = []\n",
        "                # tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                # for word in detail.split(): # get a word from the detail\n",
        "                #     result = fz.get(word)    # get the matching \n",
        "                #     if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                #         matched_list.append((word, result))\n",
        "                result = fz.get(detail.split()[0])    # for a name, it should match the first name\n",
        "                if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(spouse[0])/2): #if the matching confidence is high and word length is high\n",
        "                    annot = (spouse[0], 'SP', spouse[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                else:\n",
        "                    annot = (spouse[0], 'NPE', spouse[2:]) # exact phrase, entity tag, token indices\n",
        "                    self.neg_tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING FOR SPOUSES------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(spouse_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                # tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for wn, word in enumerate(detail.split()): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2: #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                    elif wn == 0: #if the first word doesn't match, break the matching\n",
        "                        break\n",
        "                if  len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'SP', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'SP', noun[1:])))\n",
        "#                 else:\n",
        "#                     self.neg_tag_set.add(HashableTupleAnnotations((noun[0], 'NSP', noun[1:])))\n",
        "                \n",
        "    def _tag_edu(self):\n",
        "        logging.debug('------EDUCATION INSTITUTIONS------')\n",
        "        edu_entities = []\n",
        "        if not 'EDUCATION' in self.entity_dict:\n",
        "            return\n",
        "        edu_names = [entity  for entity_list in self.entity_dict['EDUCATION'] for entity in entity_list['entity_list']]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'ORG':\n",
        "                edu_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for edu in edu_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in edu[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(edu_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.8 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (edu[0], 'ED', edu[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                else:\n",
        "                    annot = (edu[0], 'NORG', edu[2:])\n",
        "#                     logging.debug(annot)\n",
        "                    self.neg_tag_set.add(HashableTupleAnnotations(annot))\n",
        "                    \n",
        "        logging.debug('------NOUN CHUNCK MATCHING FOR EDUCATION------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(edu_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.9 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'ED', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'ED', noun[1:])))\n",
        "                # elif noun[0].split()[0] ==  detail.split()[0] and len(noun[0].split())>1: # lets add an exception for University name matching\n",
        "                    # logging.debug((noun[0], 'ED', noun[1:]))\n",
        "#                     self.tag_set.add(HashableTupleAnnotations((noun[0], 'ED', noun[1:])))\n",
        "#                 else:\n",
        "#                     self.neg_tag_set.add(HashableTupleAnnotations((noun[0], 'NED', noun[1:])))\n",
        "            \n",
        "    def _tag_children(self):\n",
        "        logging.debug('------CHILDREN NAMES------')\n",
        "        children_entities = []\n",
        "        if not 'CHILDREN' in self.entity_dict:\n",
        "            return\n",
        "        children_names = [entity  for entity_list in self.entity_dict['CHILDREN'] for entity in entity_list['entity_list']]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'PERSON':\n",
        "                children_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for child in children_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            # for word in child[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "            #     fz.add(word)        \n",
        "            # for j, detail in enumerate(children_names): # get a detail in the list under an info line\n",
        "            #     matched_list = []\n",
        "            #     tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "            #     for word in detail.split(): # get a word from the detail\n",
        "            #         result = fz.get(word)    # get the matching \n",
        "            #         if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "            #             matched_list.append((word, result))\n",
        "            #     if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "            #         annot = (child[0], 'CH', child[2:])\n",
        "            #         logging.debug(annot)\n",
        "            #         self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "            fz.add(child[0])\n",
        "            for j, detail in enumerate(children_names): # get a detail in the list under an info line(things coming from the side info box)            \n",
        "                result = fz.get(detail.split()[0])    # for a name, it should match the first name\n",
        "                if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(child[0])/2): #if the matching confidence is high and word length is high\n",
        "                    annot = (child[0], 'CH', child[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                else:\n",
        "                    annot = (child[0], 'NPE', child[2:])\n",
        "#                     logging.debug(annot)\n",
        "                    self.neg_tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING CHILDREN------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(children_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                # tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                # for word in detail.split(): # get a word from the detail\n",
        "                #     result = fz.get(word)    # get the matching \n",
        "                #     if result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2 \\\n",
        "                #               and result[0][1][0] == word[0]: #if the matching confidence is high and word length is high\n",
        "                #         matched_list.append((word, result))\n",
        "                # if  len(matched_list)!=0:\n",
        "                #     logging.debug((noun[0], 'CH', noun[1:]))\n",
        "                #     self.tag_set.add(HashableTupleAnnotations((noun[0], 'CH', noun[1:])))\n",
        "                for wn, word in enumerate(detail.split()): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2: #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                    elif wn == 0: #if the first word doesn't match, break the matching\n",
        "                        break\n",
        "                if  len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'CH', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'CH', noun[1:])))\n",
        "#                 else:\n",
        "#                     self.neg_tag_set.add(HashableTupleAnnotations((noun[0], 'NCH', noun[1:])))\n",
        "\n",
        "    def _tag_parents(self):\n",
        "        logging.debug('------PARENTS NAMES------')\n",
        "        parent_entities = []\n",
        "        if not 'PARENTS' in self.entity_dict:\n",
        "            return\n",
        "        parent_names = [entity  for entity_list in self.entity_dict['PARENTS'] for entity in entity_list['entity_list']]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'PERSON':\n",
        "        #         print(entity.text, entity.label_, entity.start, entity.end)\n",
        "                parent_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "        logging.debug('------PERSON ENTITY MATCHING FOR PARENTS------')\n",
        "        for parent in parent_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in parent[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(parent_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (parent[0], 'PR', parent[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                else:\n",
        "                    annot = (parent[0], 'NPE', parent[2:])\n",
        "#                     logging.debug(annot)\n",
        "                    self.neg_tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING FOR PARENTS------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            noun_words = noun[0].split()\n",
        "            for word in noun_words: # add all the words from the noun phrase into a fuzzy set\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(parent_names): # get a one candidate\n",
        "                matched_list = []\n",
        "                tokens_in_detail = detail.split()\n",
        "                if len(noun_words) < len(tokens_in_detail):\n",
        "                    continue\n",
        "                for i, word in enumerate(tokens_in_detail): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if result and result[0][0]>=0.5 and not len(result[0][1]) < len(word) \\\n",
        "                              and result[0][1][0] == word[0]: #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result, i))\n",
        "                # if  len(matched_list)!=0 and matched_list[0][2]==0: #and noun[0].split()[0][0] == detail.split()[0][0]:\n",
        "                #     logging.debug((noun[0], 'PR', noun[1:]))\n",
        "    #                 self.tag_set.add(HashableTupleAnnotations((noun[0], 'PR', noun[1:])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WbWxzzwGXAN",
        "colab_type": "text"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95y0R0RbNdnr",
        "colab_type": "code",
        "outputId": "92548b0d-3ea1-4acd-b6bc-10a7143a49c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#experiment_code\n",
        "# tagger = WikiConLLTagger('https://en.wikipedia.org/wiki/Barack_Obama')\n",
        "# tagger = WikiConLLTagger('https://en.wikipedia.org/wiki/Donald_Trump')\n",
        "tagger = WikiConLLTagger('https://en.wikipedia.org/wiki/Mahinda_Rajapaksa')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEBUG:root:{'Prime Minister': ['Ranil Wickremesinghe'], 'Preceded by': ['D.P. Atapattu'], 'Succeeded by': ['Ranjit Atapattu', ''], 'President': ['D. B. Wijetunga', 'Chandrika Kumaratunga'], 'Born': ['Percy Mahendra Rajapaksa', '', '1945-11-18', ')', '18 November 1945', '', 'Weerakatiya,', 'Southern Province', ',', 'British Ceylon', '', 'Sri Lanka', ')'], 'Nationality': ['Sri Lankan'], 'Political party': ['Sri Lanka Podujana Peramuna', '– Present)', '', 'Sri Lanka Freedom Party', '2018)'], 'Spouse(s)': ['Shiranthi Rajapaksa', '', 'née', 'Wickremesinghe)'], 'Children': ['Namal', 'Yoshitha', 'Rohitha'], 'Residence': ['Medamulana Walawwa'], 'Alma mater': ['Sri Lanka Law College'], 'Profession': ['Attorney at law'], 'Website': ['Official website']}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "info card is scraped successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y23_nvZzFkFH",
        "outputId": "2ac2b8a6-278a-4d51-f0cd-cd141ce68721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#experiment_code\n",
        "logger.setLevel(logging.INFO)\n",
        "df = pd.DataFrame(data = tagger.place_tags())\n",
        "df.to_csv(r'conll_annot.csv', index = None, header=True)\n",
        "print(tagger.tag_factory.entity_dict)\n",
        "print(tagger.tag_factory.get_tag_summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ")\n",
            ")\n",
            "{'NAME': [{'dict_key': 'Born', 'type': 'PERSON', 'entity_list': ['Percy Mahendra Rajapaksa', 'Weerakatiya']}], 'BIRTH_DATE': [{'dict_key': 'Born', 'type': 'DATE', 'entity_list': ['1945-11-18', '18 November 1945']}], 'BIRTH_PLACE': [{'dict_key': 'Born', 'type': 'GPE', 'entity_list': ['Weerakatiya,', 'British Ceylon', 'Sri Lanka']}], 'CHILDREN': [{'dict_key': 'Children', 'type': 'PERSON', 'entity_list': ['Namal', 'Yoshitha', 'Rohitha']}], 'SPOUSES': [{'dict_key': 'Spouse(s)', 'type': 'PERSON', 'entity_list': ['Shiranthi Rajapaksa', 'Wickremesinghe']}], 'PARENTS': [], 'EDUCATION': [{'dict_key': 'Born', 'type': 'ORG', 'entity_list': []}]}\n",
            "('November 2016', 'NBD', (3157, 3159))\n",
            "('May 2009', 'NBD', (2011, 2013))\n",
            "('May 2009', 'NBD', (2838, 2840))\n",
            "('18 November 2014', 'NBD', (3478, 3481))\n",
            "('27 January 2010', 'NBD', (155, 158))\n",
            "('November 1977', 'NBD', (583, 585))\n",
            "('November 1977', 'NBD', (1122, 1124))\n",
            "('May 2015', 'NBD', (5813, 5815))\n",
            "('the final weeks', 'NBD', (2159, 2162))\n",
            "('June 2018', 'NBD', (3726, 3728))\n",
            "('April 2011', 'NBD', (2129, 2131))\n",
            "('the base year', 'NBD', (2594, 2597))\n",
            "('10 days', 'NBD', (3913, 3915))\n",
            "('2005', 'NBD', (65, 66))\n",
            "('2005', 'NBD', (1676, 1677))\n",
            "('2005', 'NBD', (2683, 2684))\n",
            "('2005', 'NBD', (123, 124))\n",
            "('3 December', 'NBD', (5195, 5197))\n",
            "('2010', 'NBD', (2370, 2371))\n",
            "('2010', 'NBD', (2740, 2741))\n",
            "('2010', 'NBD', (6248, 6249))\n",
            "('2010', 'NBD', (6416, 6417))\n",
            "('3 December 2018', 'NBD', (319, 322))\n",
            "('158th', 'NBD', (6429, 6430))\n",
            "('February', 'NBD', (5688, 5689))\n",
            "('1972', 'NBD', (7324, 7325))\n",
            "('the final months', 'NBD', (3812, 3815))\n",
            "('October 2018', 'NBD', (5025, 5027))\n",
            "('23 January 2015', 'NBD', (5652, 5655))\n",
            "('1970', 'NBD', (106, 107))\n",
            "('1970', 'NBD', (1089, 1090))\n",
            "('1949 independence day', 'NBD', (6787, 6790))\n",
            "('1983', 'NBD', (614, 615))\n",
            "('2018', 'NBD', (31, 32))\n",
            "('2018', 'NBD', (4980, 4981))\n",
            "('2018', 'NBD', (5074, 5075))\n",
            "('21 July 2006', 'NBD', (1943, 1946))\n",
            "('2015', 'NBD', (201, 202))\n",
            "('2015', 'NBD', (46, 47))\n",
            "('2015', 'NBD', (706, 707))\n",
            "('2015', 'NBD', (2756, 2757))\n",
            "('2015', 'NBD', (2789, 2790))\n",
            "('2015', 'NBD', (2824, 2825))\n",
            "('2015', 'NBD', (67, 68))\n",
            "('2015', 'NBD', (3739, 3740))\n",
            "('2015', 'NBD', (4440, 4441))\n",
            "('2015', 'NBD', (4634, 4635))\n",
            "('25 October 1990', 'NBD', (1287, 1290))\n",
            "('2015', 'NBD', (6250, 6251))\n",
            "('2015', 'NBD', (6297, 6298))\n",
            "('2015', 'NBD', (171, 172))\n",
            "('annual', 'NBD', (6398, 6399))\n",
            "('20 months', 'NBD', (5256, 5258))\n",
            "('April 2010', 'NBD', (661, 663))\n",
            "('April 2010', 'NBD', (817, 819))\n",
            "('nomination day', 'NBD', (3407, 3409))\n",
            "('18 May 2009', 'NBD', (2005, 2008))\n",
            "('2014', 'NBD', (2612, 2613))\n",
            "('one year', 'NBD', (3580, 3582))\n",
            "('August 2011', 'NBD', (7314, 7316))\n",
            "('February 2010', 'NBD', (7263, 7265))\n",
            "('2001', 'NBD', (1420, 1421))\n",
            "('17 November 2005', 'NBD', (1537, 1540))\n",
            "('2012', 'NBD', (5409, 5410))\n",
            "('January 2015', 'NBD', (3686, 3688))\n",
            "('2004', 'NBD', (529, 530))\n",
            "('2004', 'NBD', (1450, 1451))\n",
            "('November 2014', 'NBD', (3077, 3079))\n",
            "('November 2014', 'NBD', (3104, 3106))\n",
            "('March 2002', 'NBD', (1441, 1443))\n",
            "('September 2014', 'NBD', (4648, 4650))\n",
            "('November', 'NBD', (17, 18))\n",
            "('July 2018', 'NBD', (4030, 4032))\n",
            "('12 November', 'NBD', (5147, 5149))\n",
            "('The next day 15 December', 'NBD', (5296, 5301))\n",
            "('1997', 'NBD', (1384, 1385))\n",
            "('less than 30 days', 'NBD', (1693, 1697))\n",
            "('1967', 'NBD', (1070, 1071))\n",
            "('15 November 2009', 'NBD', (2331, 2334))\n",
            "('March 2009', 'NBD', (726, 728))\n",
            "('2002', 'NBD', (1801, 1802))\n",
            "('2002', 'NBD', (2621, 2622))\n",
            "('year', 'NBD', (2624, 2625))\n",
            "('April 2015', 'NBD', (834, 836))\n",
            "('19 November 2005', 'NBD', (141, 144))\n",
            "('24 March 2015', 'NBD', (4372, 4375))\n",
            "('6 February 2010', 'NBD', (7228, 7231))\n",
            "('28 December 2014', 'NBD', (3521, 3524))\n",
            "('8 December 2014', 'NBD', (3410, 3413))\n",
            "('9 January 2015', 'NBD', (181, 184))\n",
            "('9 January 2015', 'NBD', (781, 784))\n",
            "('January 2013', 'NBD', (3052, 3054))\n",
            "('14 and 16 November 2018', 'NBD', (297, 302))\n",
            "('two years', 'NBD', (2385, 2387))\n",
            "('1989', 'NBD', (855, 856))\n",
            "('1989', 'NBD', (1146, 1147))\n",
            "('August 2018', 'NBD', (5998, 6000))\n",
            "('November 19', 'NBD', (3182, 3184))\n",
            "('21 November 2014', 'NBD', (3247, 3250))\n",
            "('the 1930s', 'NBD', (449, 451))\n",
            "('Two years ahead', 'NBD', (3097, 3100))\n",
            "('2009', 'NBD', (2421, 2422))\n",
            "('2009', 'NBD', (6295, 6296))\n",
            "('five years', 'NBD', (2604, 2606))\n",
            "('23 November 2005', 'NBD', (1664, 1667))\n",
            "('their final year', 'NBD', (4961, 4964))\n",
            "('9 November', 'NBD', (5103, 5105))\n",
            "('the 26 October 2018', 'NBD', (231, 235))\n",
            "('2013', 'NBD', (2610, 2611))\n",
            "('23 December 2014', 'NBD', (3559, 3562))\n",
            "('the next three and a half years', 'NBD', (1708, 1715))\n",
            "('16 January 2015', 'NBD', (5414, 5417))\n",
            "('2017', 'NBD', (4900, 4901))\n",
            "('May 2012', 'NBD', (2410, 2412))\n",
            "('January 2010', 'NBD', (2847, 2849))\n",
            "('1994', 'NBD', (550, 551))\n",
            "('1994', 'NBD', (1346, 1347))\n",
            "('Several months', 'NBD', (186, 188))\n",
            "('13th', 'NBD', (1475, 1476))\n",
            "('six-year', 'NBD', (134, 137))\n",
            "('four years', 'NBD', (3169, 3171))\n",
            "('18 December 2018', 'NBD', (5317, 5320))\n",
            "('The next year', 'NBD', (6405, 6408))\n",
            "('May 2008', 'NBD', (6101, 6103))\n",
            "('6 April 2004', 'NBD', (115, 118))\n",
            "('6 April 2004', 'NBD', (1479, 1482))\n",
            "('Sunday', 'NBD', (2268, 2269))\n",
            "('Sunday', 'NBD', (6530, 6531))\n",
            "('20 years', 'NBD', (762, 764))\n",
            "('the 9th September 2010', 'NBD', (2890, 2894))\n",
            "('2019', 'NBD', (80, 81))\n",
            "('15 December 2018', 'NBD', (353, 356))\n",
            "('1948', 'NBD', (6778, 6779))\n",
            "('13 December 2018', 'NBD', (5239, 5242))\n",
            "('2016', 'NBD', (4672, 4673))\n",
            "('6 September 2009', 'NBD', (7209, 7212))\n",
            "('November 11, 2018', 'NBD', (5112, 5116))\n",
            "('2008', 'NBD', (6036, 6037))\n",
            "('2008', 'NBD', (6385, 6386))\n",
            "('three days', 'NBD', (4333, 4335))\n",
            "('1977', 'NBD', (1138, 1139))\n",
            "('the year 2002', 'NBD', (2590, 2593))\n",
            "('the Independence day', 'NBD', (6870, 6873))\n",
            "{'BD': 0, 'NBD': 143, 'CH': 9, 'PR': 0, 'SP': 19, 'NPE': 122, 'BP': 0, 'NGPE': 83, 'ED': 0, 'NORG': 0, 'SENTS': 276}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4vG3X0nn3kL",
        "colab_type": "code",
        "outputId": "d31ef113-82cd-42d0-89cf-1a4bf4220064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#experiment_code\n",
        "fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "# for word in spouse[0].split(): # add all the words into a fuzzy set from the entity\n",
        "# fz.add('Shiranthi')\n",
        "fz.add('Wickremasinghe')\n",
        "fz.add('Shiranthi Wickremasinghe')\n",
        "        \n",
        "# for j, detail in enumerate(spouse_names): # get a detail in the list under an info line\n",
        "#     matched_list = []\n",
        "#     tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "#     for word in detail.split(): # get a word from the detail\n",
        "#         result = fz.get(word)    # get the matching \n",
        "#         if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "#             matched_list.append((word, result))\n",
        "\n",
        "result = fz.get('Wickremasinghe')\n",
        "result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 'Wickremasinghe')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwDUk03BdGsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}