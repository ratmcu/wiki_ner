{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conll_tagged_ne.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratmcu/wiki_ner/blob/master/conll_tagged_ne.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcIYSgqMLy8V",
        "colab_type": "code",
        "outputId": "01836c96-06a5-4dd5-986d-1e9db0c346d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import wget\n",
        "try:\n",
        "    import colabimport\n",
        "except:\n",
        "    colabimporturl = 'https://github.com/ratmcu/colaboratory_import/raw/master/colabimport.py'\n",
        "    filename = colabimporturl.split(\"/\")[-1].split(\"?\")[0]\n",
        "    if os.path.isfile(filename):\n",
        "        os.remove(filename)\n",
        "    wget.download(colabimporturl)\n",
        "    import colabimport\n",
        "colabimport.get_notebook('https://github.com/ratmcu/wiki_ner/blob/master/reusable_annotator.ipynb?raw=true')\n",
        "colabimport.get_notebook('https://github.com/ratmcu/wiki_ner/blob/master/info_box.ipynb?raw=true')\n",
        "# import io, os, sys, types\n",
        "from reusable_annotator import PageContents\n",
        "from info_box import InfoCard, PrivateEntities\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.DEBUG)\n",
        "# logging.debug(\"test\")\n",
        "class HashableTupleAnnotations(tuple):\n",
        "    def __hash__(self):\n",
        "        return hash(tuple(sorted([self[0:1],self[1:2]])))\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "# import spacy\n",
        "from collections import Counter\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "import re\n",
        "!pip install pyahocorasick\n",
        "!pip install fuzzyset\n",
        "from ahocorasick import Automaton\n",
        "import fuzzyset\n",
        "from operator import itemgetter, attrgetter\n",
        "import pandas as pd\n",
        "# import json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=0e85a1f7c12a78028b14664b4dba16d6d4c3fb5e3bd2603e7a6c811ced9fef9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrMlO6iPL6mU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WikiConLLTagger():\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        self.page = PageContents(url)\n",
        "        self.info_card = InfoCard(self.page)\n",
        "#         print(sorted(self.tag_factory.get_annotations(), key=lambda annot: annot[1][0]))\n",
        "        \n",
        "    def _get_annotations(self):\n",
        "        '''stack all the annotations'''\n",
        "        return sorted(sorted(self.tag_factory.get_annotations(), key=lambda annot: annot[1][0]), reverse=True) # sort them by the start index of the annotaion token\n",
        "       \n",
        "#     def get_metadata(self):\n",
        "#         pe = self.private_entities\n",
        "#         # pe = [list(map(lambda li: item[1], item)) for item in pe.items()]\n",
        "#         dp = dict()\n",
        "#         [dp.update({item[0]: item[1][1]}) for item in pe.items()]\n",
        "#         dp.update({'URL' : self.url})\n",
        "#         return dp\n",
        "    \n",
        "    def place_tags(self):\n",
        "        import spacy\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.text = self.page.get_text_chunk()\n",
        "        self.private_entities = PrivateEntities(self.info_card).entity_dict   \n",
        "        self.doc = nlp(self.text)\n",
        "#         self.spacy_noun_chunks =  [(chunk.text, chunk.start, chunk.end) for chunk in self.doc.noun_chunks]\n",
        "        self.tag_factory = TagFactory(self)\n",
        "        annotations = self._get_annotations()\n",
        "        words = []\n",
        "        tags = []\n",
        "        annotation_tags = ['O']*len(self.doc)\n",
        "#         annotation = annotations.pop()\n",
        "        for annotation in annotations:\n",
        "            annotation_tags[annotation[1][0]] = 'B-'+annotation[0] #\n",
        "            for i in range(annotation[1][1]-annotation[1][0]-1):\n",
        "                annotation_tags[annotation[1][0]+i+1] = 'I-'+annotation[0]\n",
        "#         for token in self.doc:\n",
        "#             words.append(token)\n",
        "#             if token\n",
        "        for sentence in self.doc.sents:\n",
        "            tags.extend(annotation_tags[sentence.start:sentence.end])\n",
        "            tags.append('\\n')\n",
        "            for token in sentence:\n",
        "                words.append(token)\n",
        "            words.append('\\n')\n",
        "#         print(words)\n",
        "#         print(tags)\n",
        "        return {'words': words, 'tags':tags}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p315V1HpXryO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TagFactory():\n",
        "    '''will keep a set of all the tagged annotations in the form of\n",
        "       ('word phrase', 'ENTITY', start, end), each tagging function will update the list'''\n",
        "    def __init__(self, tagger):\n",
        "        self.doc =tagger.doc\n",
        "        self.entity_dict = tagger.private_entities\n",
        "        self.tag_set = set()        \n",
        "        self.neg_tag_set = set()\n",
        "        self.spacy_noun_chunks =  [(chunk.text, chunk.start, chunk.end) for chunk in self.doc.noun_chunks]\n",
        "        tagging_methods = [getattr(self, method) for method in dir(self) if callable(getattr(self, method)) and re.match('_tag_.*', method)]\n",
        "        for method in tagging_methods:\n",
        "            method()\n",
        "            \n",
        "    def get_tag_summary(self):\n",
        "        BD_tags = len([y for y in self.tag_set if y[1]=='BD'])\n",
        "        NBD_tags = len([y for y in self.neg_tag_set if y[1]=='NBD']) \n",
        "#         [print(y) for y in self.neg_tag_set if y[1]=='NBD']\n",
        "        CH_tags = len([y for y in self.tag_set if y[1]=='CH'])\n",
        "        PR_tags = len([y for y in self.tag_set if y[1]=='PR'])\n",
        "        SP_tags = len([y for y in self.tag_set if y[1]=='SP'])\n",
        "        NPE_tags = len([y for y in self.neg_tag_set if y[1]=='NPE'])\n",
        "        \n",
        "        BP_tags = len([y for y in self.tag_set if y[1]=='BP'])\n",
        "        NGPE_tags = len([y for y in self.neg_tag_set if y[1]=='NGPE'])\n",
        "        \n",
        "        ED_tags = len([y for y in self.tag_set if y[1]=='ED'])\n",
        "        NORG_tags = len([y for y in self.neg_tag_set if y[1]=='NORG'])\n",
        "        return {'BD': BD_tags,\n",
        "                'NBD': NBD_tags,\n",
        "                'CH': CH_tags,\n",
        "                'PR': PR_tags,\n",
        "                'SP': SP_tags,\n",
        "                'NPE': NPE_tags,\n",
        "                'BP': BP_tags,\n",
        "                'NGPE': NGPE_tags,\n",
        "                'ED': ED_tags,\n",
        "                'NORG': NORG_tags,\n",
        "                'SENTS': len([sent for sent in self.doc.sents])}\n",
        "\n",
        "    def get_annotations(self):  \n",
        "        '''list with ('ENTITY', start, end) '''\n",
        "        return [annot[1:] for annot in list(self.tag_set)]\n",
        "    \n",
        "    def _tag_bd(self):\n",
        "        logging.debug('------BIRTHDAY MATCHING------')\n",
        "        date_entities = []\n",
        "        if not 'BIRTH_DATE' in self.entity_dict:\n",
        "            return\n",
        "        birth_dates =  [entity  for entity_list in self.entity_dict['BIRTH_DATE'] for entity in entity_list['entity_list']] #[{'dict_key':'Children', 'type':'PERSON', 'entity_list':[]}]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'DATE':\n",
        "                date_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for date in date_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in date[0].split(): # add all the words into a fuzzy set from the entity\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(birth_dates): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (date[0], 'BD', date[2:]) # exact phrase, entity tag, token indices\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                else:\n",
        "                    annot = (date[0], 'NBD', date[2:]) # exact phrase, entity tag, token indices\n",
        "                    self.neg_tag_set.add(HashableTupleAnnotations(annot))\n",
        "    \n",
        "    def _tag_bp(self):\n",
        "        logging.debug('------BIRTH PLACES------')\n",
        "        place_entities = []\n",
        "        if not 'BIRTH_PLACE' in self.entity_dict:\n",
        "            return\n",
        "        birth_places = [entity  for entity_list in self.entity_dict['BIRTH_PLACE'] for entity in entity_list['entity_list']]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'GPE':\n",
        "                place_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "        for place in place_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in place[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(birth_places): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]==0.6):\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (place[0], 'BP', place[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                else:\n",
        "                    annot = (place[0], 'NGPE', place[2:]) # exact phrase, entity tag, token indices\n",
        "                    self.neg_tag_set.add(HashableTupleAnnotations(annot))\n",
        "                    \n",
        "        logging.debug('------NOUN CHUNCK MATCHING FOR BP------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(birth_places): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]==0.6):\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'BP', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'BP', noun[1:])))\n",
        "#                 else:\n",
        "#                     self.neg_tag_set.add(HashableTupleAnnotations((noun[0], 'NBP', noun[1:])))\n",
        "    def _tag_spouse(self):\n",
        "        logging.debug('------SPOUSES NAMES------')\n",
        "        spouse_entities = []\n",
        "        if not 'SPOUSES' in self.entity_dict:\n",
        "            return\n",
        "        spouse_names = [entity  for entity_list in self.entity_dict['SPOUSES'] for entity in entity_list['entity_list']]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'PERSON':\n",
        "                spouse_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for spouse in spouse_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            # for word in spouse[0].split(): # add all the words into a fuzzy set from the entity\n",
        "            #     fz.add(word)  \n",
        "            fz.add(spouse[0])                  \n",
        "            for j, detail in enumerate(spouse_names): # get a detail in the list under an info line(things coming from the side info box)\n",
        "                # matched_list = []\n",
        "                # tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                # for word in detail.split(): # get a word from the detail\n",
        "                #     result = fz.get(word)    # get the matching \n",
        "                #     if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                #         matched_list.append((word, result))\n",
        "                result = fz.get(detail.split()[0])    # for a name, it should match the first name\n",
        "                if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(spouse[0])/2): #if the matching confidence is high and word length is high\n",
        "                    annot = (spouse[0], 'SP', spouse[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                else:\n",
        "                    annot = (spouse[0], 'NPE', spouse[2:]) # exact phrase, entity tag, token indices\n",
        "                    self.neg_tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING FOR SPOUSES------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(spouse_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                # tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for wn, word in enumerate(detail.split()): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2: #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                    elif wn == 0: #if the first word doesn't match, break the matching\n",
        "                        break\n",
        "                if  len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'SP', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'SP', noun[1:])))\n",
        "#                 else:\n",
        "#                     self.neg_tag_set.add(HashableTupleAnnotations((noun[0], 'NSP', noun[1:])))\n",
        "                \n",
        "    def _tag_edu(self):\n",
        "        logging.debug('------EDUCATION INSTITUTIONS------')\n",
        "        edu_entities = []\n",
        "        if not 'EDUCATION' in self.entity_dict:\n",
        "            return\n",
        "        edu_names = [entity  for entity_list in self.entity_dict['EDUCATION'] for entity in entity_list['entity_list']]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'ORG':\n",
        "                edu_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for edu in edu_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in edu[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(edu_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.8 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (edu[0], 'ED', edu[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                else:\n",
        "                    annot = (edu[0], 'NORG', edu[2:])\n",
        "#                     logging.debug(annot)\n",
        "                    self.neg_tag_set.add(HashableTupleAnnotations(annot))\n",
        "                    \n",
        "        logging.debug('------NOUN CHUNCK MATCHING FOR EDUCATION------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(edu_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.9 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'ED', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'ED', noun[1:])))\n",
        "                # elif noun[0].split()[0] ==  detail.split()[0] and len(noun[0].split())>1: # lets add an exception for University name matching\n",
        "                    # logging.debug((noun[0], 'ED', noun[1:]))\n",
        "#                     self.tag_set.add(HashableTupleAnnotations((noun[0], 'ED', noun[1:])))\n",
        "#                 else:\n",
        "#                     self.neg_tag_set.add(HashableTupleAnnotations((noun[0], 'NED', noun[1:])))\n",
        "            \n",
        "    def _tag_children(self):\n",
        "        logging.debug('------CHILDREN NAMES------')\n",
        "        children_entities = []\n",
        "        if not 'CHILDREN' in self.entity_dict:\n",
        "            return\n",
        "        children_names = [entity  for entity_list in self.entity_dict['CHILDREN'] for entity in entity_list['entity_list']]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'PERSON':\n",
        "                children_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "\n",
        "        for child in children_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            # for word in child[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "            #     fz.add(word)        \n",
        "            # for j, detail in enumerate(children_names): # get a detail in the list under an info line\n",
        "            #     matched_list = []\n",
        "            #     tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "            #     for word in detail.split(): # get a word from the detail\n",
        "            #         result = fz.get(word)    # get the matching \n",
        "            #         if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "            #             matched_list.append((word, result))\n",
        "            #     if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "            #         annot = (child[0], 'CH', child[2:])\n",
        "            #         logging.debug(annot)\n",
        "            #         self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "            fz.add(child[0])\n",
        "            for j, detail in enumerate(children_names): # get a detail in the list under an info line(things coming from the side info box)            \n",
        "                result = fz.get(detail.split()[0])    # for a name, it should match the first name\n",
        "                if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(child[0])/2): #if the matching confidence is high and word length is high\n",
        "                    annot = (child[0], 'CH', child[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                else:\n",
        "                    annot = (child[0], 'NPE', child[2:])\n",
        "#                     logging.debug(annot)\n",
        "                    self.neg_tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING CHILDREN------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in noun[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(children_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                # tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                # for word in detail.split(): # get a word from the detail\n",
        "                #     result = fz.get(word)    # get the matching \n",
        "                #     if result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2 \\\n",
        "                #               and result[0][1][0] == word[0]: #if the matching confidence is high and word length is high\n",
        "                #         matched_list.append((word, result))\n",
        "                # if  len(matched_list)!=0:\n",
        "                #     logging.debug((noun[0], 'CH', noun[1:]))\n",
        "                #     self.tag_set.add(HashableTupleAnnotations((noun[0], 'CH', noun[1:])))\n",
        "                for wn, word in enumerate(detail.split()): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2: #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                    elif wn == 0: #if the first word doesn't match, break the matching\n",
        "                        break\n",
        "                if  len(matched_list)!=0:\n",
        "                    logging.debug((noun[0], 'CH', noun[1:]))\n",
        "                    self.tag_set.add(HashableTupleAnnotations((noun[0], 'CH', noun[1:])))\n",
        "#                 else:\n",
        "#                     self.neg_tag_set.add(HashableTupleAnnotations((noun[0], 'NCH', noun[1:])))\n",
        "\n",
        "    def _tag_parents(self):\n",
        "        logging.debug('------PARENTS NAMES------')\n",
        "        parent_entities = []\n",
        "        if not 'PARENTS' in self.entity_dict:\n",
        "            return\n",
        "        parent_names = [entity  for entity_list in self.entity_dict['PARENTS'] for entity in entity_list['entity_list']]\n",
        "        for entity in self.doc.ents:\n",
        "            if entity.label_ == 'PERSON':\n",
        "        #         print(entity.text, entity.label_, entity.start, entity.end)\n",
        "                parent_entities.append((entity.text, entity.label_, entity.start, entity.end))\n",
        "        logging.debug('------PERSON ENTITY MATCHING FOR PARENTS------')\n",
        "        for parent in parent_entities:    \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            for word in parent[0].split(): # add all the words into a fuzzy set from that sentence\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(parent_names): # get a detail in the list under an info line\n",
        "                matched_list = []\n",
        "                tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "                for word in detail.split(): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result))\n",
        "                if abs(len(matched_list) - tokens_in_detail) < 1 and len(matched_list)!=0:\n",
        "                    annot = (parent[0], 'PR', parent[2:])\n",
        "                    logging.debug(annot)\n",
        "                    self.tag_set.add(HashableTupleAnnotations(annot))\n",
        "                else:\n",
        "                    annot = (parent[0], 'NPE', parent[2:])\n",
        "#                     logging.debug(annot)\n",
        "                    self.neg_tag_set.add(HashableTupleAnnotations(annot))\n",
        "        logging.debug('------NOUN CHUNCK MATCHING FOR PARENTS------')\n",
        "        for noun in self.spacy_noun_chunks:    #('Joachim Wilhelm Gauck', 0, 4) \n",
        "            fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "            noun_words = noun[0].split()\n",
        "            for word in noun_words: # add all the words from the noun phrase into a fuzzy set\n",
        "                fz.add(word)        \n",
        "            for j, detail in enumerate(parent_names): # get a one candidate\n",
        "                matched_list = []\n",
        "                tokens_in_detail = detail.split()\n",
        "                if len(noun_words) < len(tokens_in_detail):\n",
        "                    continue\n",
        "                for i, word in enumerate(tokens_in_detail): # get a word from the detail\n",
        "                    result = fz.get(word)    # get the matching \n",
        "                    if result and result[0][0]>=0.5 and not len(result[0][1]) < len(word) \\\n",
        "                              and result[0][1][0] == word[0]: #if the matching confidence is high and word length is high\n",
        "                        matched_list.append((word, result, i))\n",
        "                # if  len(matched_list)!=0 and matched_list[0][2]==0: #and noun[0].split()[0][0] == detail.split()[0][0]:\n",
        "                #     logging.debug((noun[0], 'PR', noun[1:]))\n",
        "    #                 self.tag_set.add(HashableTupleAnnotations((noun[0], 'PR', noun[1:])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WbWxzzwGXAN",
        "colab_type": "text"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95y0R0RbNdnr",
        "colab_type": "code",
        "outputId": "a508b987-3598-4b80-bab3-0d40cc6574bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#experiment_code\n",
        "# tagger = WikiConLLTagger('https://en.wikipedia.org/wiki/Barack_Obama')\n",
        "# tagger = WikiConLLTagger('https://en.wikipedia.org/wiki/Donald_Trump')\n",
        "tagger = WikiConLLTagger('https://en.wikipedia.org/wiki/Mahinda_Rajapaksa')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEBUG:root:{'Prime Minister': ['Ranil Wickremesinghe'], 'Preceded by': ['D.P. Atapattu'], 'Succeeded by': ['Ranjit Atapattu', ''], 'President': ['D. B. Wijetunga', 'Chandrika Kumaratunga'], 'Born': ['Percy Mahendra Rajapaksa', '', '1945-11-18', ')', '18 November 1945', '', 'Weerakatiya,', 'Southern Province', ',', 'British Ceylon', '', 'Sri Lanka', ')'], 'Nationality': ['Sri Lankan'], 'Political party': ['Sri Lanka Podujana Peramuna', '– Present)', '', 'Sri Lanka Freedom Party', '2018)'], 'Spouse(s)': ['Shiranthi Rajapaksa', '', 'née', 'Wickremesinghe)'], 'Children': ['Namal', 'Yoshitha', 'Rohitha'], 'Residence': ['Medamulana Walawwa'], 'Alma mater': ['Sri Lanka Law College'], 'Profession': ['Attorney at law'], 'Website': ['Official website']}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "info card is scraped successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y23_nvZzFkFH",
        "outputId": "89a4cd59-a5f7-4c76-99ac-60596ae39544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#experiment_code\n",
        "logger.setLevel(logging.INFO)\n",
        "df = pd.DataFrame(data = tagger.place_tags())\n",
        "df.to_csv(r'conll_annot.csv', index = None, header=True)\n",
        "print(tagger.tag_factory.entity_dict)\n",
        "print(tagger.tag_factory.get_tag_summary())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'NAME': [{'dict_key': 'Born', 'type': 'PERSON', 'entity_list': ['Percy Mahendra Rajapaksa', 'Weerakatiya']}], 'BIRTH_DATE': [{'dict_key': 'Born', 'type': 'DATE', 'entity_list': ['1945-11-18', '18 November 1945']}], 'BIRTH_PLACE': [{'dict_key': 'Born', 'type': 'GPE', 'entity_list': ['Weerakatiya,', 'British Ceylon', 'Sri Lanka']}], 'CHILDREN': [{'dict_key': 'Children', 'type': 'PERSON', 'entity_list': ['Namal', 'Yoshitha', 'Rohitha']}], 'SPOUSES': [{'dict_key': 'Spouse(s)', 'type': 'PERSON', 'entity_list': ['Shiranthi Rajapaksa', 'Wickremesinghe']}], 'PARENTS': [], 'EDUCATION': [{'dict_key': 'Alma mater', 'type': 'ORG', 'entity_list': ['Sri Lanka Law College']}]}\n",
            "{'BD': 0, 'NBD': 143, 'CH': 9, 'PR': 0, 'SP': 19, 'NPE': 122, 'BP': 0, 'NGPE': 83, 'ED': 2, 'NORG': 347, 'SENTS': 276}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4vG3X0nn3kL",
        "colab_type": "code",
        "outputId": "72915e40-54a2-47d7-e276-98ded346fc82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#experiment_code\n",
        "fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "# for word in spouse[0].split(): # add all the words into a fuzzy set from the entity\n",
        "# fz.add('Shiranthi')\n",
        "fz.add('Wickremasinghe')\n",
        "fz.add('Shiranthi Wickremasinghe')\n",
        "        \n",
        "# for j, detail in enumerate(spouse_names): # get a detail in the list under an info line\n",
        "#     matched_list = []\n",
        "#     tokens_in_detail = len(detail.split()) # split the detail into words\n",
        "#     for word in detail.split(): # get a word from the detail\n",
        "#         result = fz.get(word)    # get the matching \n",
        "#         if(result and result[0][0]>=0.5 and not len(result[0][1])/2 < len(word)/2): #if the matching confidence is high and word length is high\n",
        "#             matched_list.append((word, result))\n",
        "\n",
        "result = fz.get('Wickremasinghe')\n",
        "result"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 'Wickremasinghe')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}