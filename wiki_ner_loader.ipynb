{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wiki_ner_loader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratmcu/wiki_ner/blob/master/wiki_ner_loader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0Ev0aJhF5Py",
        "colab_type": "code",
        "outputId": "461b8c91-34dd-46bc-9689-a508695eb2e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "'''\n",
        "An entry or sent looks like ...\n",
        "SOCCER NN B-NP O\n",
        "- : O O\n",
        "JAPAN NNP B-NP B-LOC\n",
        "GET VB B-VP O\n",
        "LUCKY NNP B-NP O\n",
        "WIN NNP I-NP O\n",
        ", , O O\n",
        "CHINA NNP B-NP B-PER\n",
        "IN IN B-PP O\n",
        "SURPRISE DT B-NP O\n",
        "DEFEAT NN I-NP O\n",
        ". . O O\n",
        "Each mini-batch returns the followings:\n",
        "words: list of input sents. [\"The 26-year-old ...\", ...]\n",
        "x: encoded input sents. [N, T]. int64.\n",
        "is_heads: list of head markers. [[1, 1, 0, ...], [...]]\n",
        "tags: list of tags.['O O B-MISC ...', '...']\n",
        "y: encoded tags. [N, T]. int64\n",
        "seqlens: list of seqlens. [45, 49, 10, 50, ...]\n",
        "'''\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils import data\n",
        "!pip install pytorch-pretrained-bert\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "# import traceback\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
        "# VOCAB = ('<PAD>', 'O', 'I-LOC', 'B-PER', 'I-PER', 'I-ORG', 'I-MISC', 'B-MISC', 'B-LOC', 'B-ORG')\n",
        "\n",
        "tags = ['BD', 'BP', 'PR', 'SP', 'CH', 'ED']\n",
        "VOCAB_list = ['<PAD>', 'O',]\n",
        "for tag in tags:\n",
        "    VOCAB_list.append('I-'+tag)\n",
        "    VOCAB_list.append('B-'+tag)\n",
        "VOCAB = tuple(VOCAB_list)\n",
        "tag2idx = {tag: idx for idx, tag in enumerate(VOCAB)}\n",
        "idx2tag = {idx: tag for idx, tag in enumerate(VOCAB)}\n",
        "\n",
        "class NerDataset(data.Dataset):\n",
        "    def __init__(self, fpath):\n",
        "        \"\"\"\n",
        "        fpath: [train|valid|test].txt\n",
        "        \"\"\"\n",
        "        entries = open(fpath, 'r').read().strip().split(\"\\n\\n\")\n",
        "        sents, tags_li = [], [] # list of lists\n",
        "        for entry in entries:\n",
        "#             print(entry)\n",
        "            lines = entry.splitlines()\n",
        "            words = [line.split()[0] for line in entry.splitlines() if len(line.split()) > 1]\n",
        "#             try:\n",
        "#                 words = [line.split()[0] for line in entry.splitlines()]\n",
        "# #                 words = [line.split()[0] for line in entry.splitlines() if len(line.split())== 1 and line.split()[0] == 'O']\n",
        "#             except Exception as e:\n",
        "#                 print(traceback.format_exc())\n",
        "#                 print('splitting failed: ', [(ord(char),char) for char in entry])\n",
        "#                 continue\n",
        "            tags = ([line.split()[-1] for line in entry.splitlines() if len(line.split()) > 1])\n",
        "            sents.append([\"[CLS]\"] + words + [\"[SEP]\"])\n",
        "            tags_li.append([\"<PAD>\"] + tags + [\"<PAD>\"])\n",
        "        self.sents, self.tags_li = sents, tags_li\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n",
        "\n",
        "        # We give credits only to the first piece.\n",
        "        x, y = [], [] # list of ids\n",
        "        is_heads = [] # list. 1: the token is the first piece of a word\n",
        "        for w, t in zip(words, tags):\n",
        "            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n",
        "            xx = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "            is_head = [1] + [0]*(len(tokens) - 1)\n",
        "\n",
        "            t = [t] + [\"<PAD>\"] * (len(tokens) - 1)  # <PAD>: no decision\n",
        "            yy = [tag2idx[each] for each in t]  # (T,)\n",
        "\n",
        "            x.extend(xx)\n",
        "            is_heads.extend(is_head)\n",
        "            y.extend(yy)\n",
        "\n",
        "        assert len(x)==len(y)==len(is_heads), f\"len(x)={len(x)}, len(y)={len(y)}, len(is_heads)={len(is_heads)}\"\n",
        "\n",
        "        # seqlen\n",
        "        seqlen = len(y)\n",
        "\n",
        "        # to string\n",
        "        words = \" \".join(words)\n",
        "        tags = \" \".join(tags)\n",
        "        return words, x, is_heads, tags, y, seqlen\n",
        "    \n",
        "    def append(self, other):\n",
        "        self.sents.extend(other.sents)\n",
        "        self.tags_li.extend(other.tags_li)\n",
        "\n",
        "def pad(batch):\n",
        "    '''Pads to the longest sample'''\n",
        "    f = lambda x: [sample[x] for sample in batch]\n",
        "    words = f(0)\n",
        "    is_heads = f(2)\n",
        "    tags = f(3)\n",
        "    seqlens = f(-1)\n",
        "    maxlen = np.array(seqlens).max()\n",
        "\n",
        "    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n",
        "    x = f(1, maxlen)\n",
        "    y = f(-2, maxlen)\n",
        "\n",
        "\n",
        "    f = torch.LongTensor\n",
        "\n",
        "    return words, f(x), is_heads, tags, f(y), seqlens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 11.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.3.0+cu100)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.10.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.17.3)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/60/d9782c56ceefa76033a00e1f84cd8c586c75e6e7fea2cd45ee8b46a386c5/regex-2019.08.19-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.2 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.13.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.2->boto3->pytorch-pretrained-bert) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.2->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.2->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Installing collected packages: regex, pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2 regex-2019.8.19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 213450/213450 [00:00<00:00, 837744.59B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXa5C6TCYEPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from torch.utils import data as torch_data_utils\n",
        "# train_iter = torch_data_utils.DataLoader(dataset=train_dataset,\n",
        "#                              batch_size=1,\n",
        "#                              shuffle=True,\n",
        "#                              num_workers=4,\n",
        "#                              collate_fn=pad)\n",
        "# eval_iter = torch_data_utils.DataLoader(dataset=eval_dataset,\n",
        "#                             batch_size=1,\n",
        "#                             shuffle=False,\n",
        "#                             num_workers=4,\n",
        "#                             collate_fn=pad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1dibACaqHCg",
        "colab_type": "code",
        "outputId": "8b7dd829-d3eb-42e6-e4b0-972e3e90e432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "#experiment_code\n",
        "import os\n",
        "import time\n",
        "!pip install wget\n",
        "import wget\n",
        "import logging\n",
        "import pickle\n",
        "import ast\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib\n",
        "from bs4 import BeautifulSoup\n",
        "import tarfile\n",
        "if not os.path.exists('dataset.tar.gz'):\n",
        "    wget.download('https://github.com/ratmcu/wiki_ner/blob/master/dataset.tar.gz?raw=true')\n",
        "tar = tarfile.open('dataset.tar.gz', mode='r')\n",
        "tar.extractall('./')\n",
        "tar.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=7a32906f0d82dddcf5fdb818e4769c129b75252515ab36b45f2e8878dabbc31b\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8LI6Ec9qL-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def toConllTxt(path, save_file = None):\n",
        "    df = pd.read_csv(path)\n",
        "#     dir_path, _  = os.path.split(path)\n",
        "    if not save_file:\n",
        "        save_file = os.path.join(os.path.split(path)[0], '%s.txt' % path.split('/')[-1])\n",
        "    with open(save_file, 'w') as file:    \n",
        "        for i, row in enumerate(df.iterrows()):\n",
        "            if (row[1]['words'] == '\\n' and row[1]['tags'] == '\\n'):\n",
        "                file.write('\\n')\n",
        "            else:\n",
        "                try:\n",
        "                    file.write(row[1]['words']+' ')\n",
        "                except:\n",
        "                    file.write( str(row[1]['words']) + ' ')\n",
        "                file.write(row[1]['tags']+'\\n')\n",
        "    return save_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXY3rm9pngTr",
        "colab_type": "code",
        "outputId": "173ab946-3d9c-4aa8-c1e6-e9abd5f398c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        }
      },
      "source": [
        "#experiment_code\n",
        "paths = sorted([os.path.join(f[0], name) for f in os.walk('./dataset') if len(f[2])!=0 for name in f[2] if os.path.splitext(name)[-1] == '.csv'])\n",
        "import random\n",
        "rand_paths = random.choices(paths, k=10)\n",
        "dataset = NerDataset(toConllTxt(rand_paths[0]))\n",
        "for i, path in enumerate(rand_paths[1:]):\n",
        "    print(path.split('/')[-2])\n",
        "    txt_path = toConllTxt(path)\n",
        "    print(txt_path)\n",
        "    data_page = NerDataset(txt_path)\n",
        "    dataset.append(data_page)\n",
        "    print(len(data_page), ' ', i)\n",
        "print(len(dataset))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mihai Ghimpu\n",
            "./dataset/politicians/Moldova/Mihai Ghimpu/conll_tagged.csv.txt\n",
            "49   0\n",
            "Bujar Nishani\n",
            "./dataset/politicians/Albania/Bujar Nishani/conll_tagged.csv.txt\n",
            "28   1\n",
            "Natsagiin Bagabandi\n",
            "./dataset/politicians/Mongolia/Natsagiin Bagabandi/conll_tagged.csv.txt\n",
            "6   2\n",
            "Shavkat Mirziyoyev\n",
            "./dataset/politicians/Uzbekistan/Shavkat Mirziyoyev/conll_tagged.csv.txt\n",
            "101   3\n",
            "Pandeli Majko\n",
            "./dataset/politicians/Albania/Pandeli Majko/conll_tagged.csv.txt\n",
            "21   4\n",
            "Vincent Auriol\n",
            "./dataset/politicians/France/Vincent Auriol/conll_tagged.csv.txt\n",
            "46   5\n",
            "Erik Gustaf Boström\n",
            "./dataset/politicians/Poland/Erik Gustaf Boström/conll_tagged.csv.txt\n",
            "30   6\n",
            "Gaafar Nimeiry\n",
            "./dataset/politicians/South Sudan/Gaafar Nimeiry/conll_tagged.csv.txt\n",
            "80   7\n",
            "Sai Mauk Kham\n",
            "./dataset/politicians/Myanmar/Sai Mauk Kham/conll_tagged.csv.txt\n",
            "11   8\n",
            "413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5BQfS1-QTPO",
        "colab_type": "text"
      },
      "source": [
        "### **testing dataloader on all pages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YCgOwMgQQjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "paths = sorted([os.path.join(f[0], name) for f in os.walk('./dataset') if len(f[2])!=0 for name in f[2] if os.path.splitext(name)[-1] == '.csv'])\n",
        "dataset = NerDataset(toConllTxt(paths[0]))\n",
        "for i, path in enumerate(paths[1:]):\n",
        "    print(path.split('/')[-2])\n",
        "    txt_path = toConllTxt(path)\n",
        "    print(txt_path)\n",
        "    data_page = NerDataset(txt_path)\n",
        "    dataset.append(data_page)\n",
        "    print(len(data_page), ' ', i)\n",
        "print(len(dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "woqKeXVw7kaU",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "print(len(dataset))\n",
        "# dataset = NerDataset(toConllTxt(paths[0]))\n",
        "for sent in dataset:\n",
        "    print(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_zx39TVpZEo",
        "colab_type": "text"
      },
      "source": [
        "## working on Mass Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHoYxl7aAqVJ",
        "colab_type": "text"
      },
      "source": [
        "### creating the text files suitable for the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPcxIoFopcmw",
        "colab_type": "code",
        "outputId": "567e1d8a-f012-4541-d7e0-67eef97a4fca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "#experiment_code\n",
        "import tarfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "tar = tarfile.open('/content/drive/My Drive/Colab Notebooks/dataset.tar.gz', mode='r')\n",
        "tar.extractall('./dataset_2')\n",
        "tar.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47FWkwEUB7Dq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "import os\n",
        "paths_annot = sorted([os.path.join(f[0], name) for f in os.walk('./dataset_2') \n",
        "                if len(f[2])!=0 for name in f[2] if os.path.splitext(name)[-1] == '.csv' and name.split('_')[0]=='annot'],\n",
        "               key=lambda path: int(path.split('_')[-1].split('.')[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abn6umqVpSBy",
        "colab_type": "code",
        "outputId": "ca4de8a4-ab7a-4e36-f990-1dfabb777e85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        }
      },
      "source": [
        "#experiment_code\n",
        "import random\n",
        "rand_paths = random.choices(paths_annot, k=10)\n",
        "dataset = NerDataset(toConllTxt(rand_paths[0]))\n",
        "for i, path in enumerate(rand_paths[1:]):bl\n",
        "    print(path.split('/')[-2])\n",
        "    txt_path = toConllTxt(path, save_file = None)\n",
        "    print(txt_path)\n",
        "    data_page = NerDataset(txt_path)\n",
        "    dataset.append(data_page)\n",
        "    print(len(data_page), ' ', i)\n",
        "print(len(dataset))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "John_Fleming_(priest)\n",
            "./dataset_2/scrapes/John_Fleming_(priest)/annot_csv_519.csv.txt\n",
            "39   0\n",
            "Princess_Alexandra,_The_Honourable_Lady_Ogilvy\n",
            "./dataset_2/scrapes/Princess_Alexandra,_The_Honourable_Lady_Ogilvy/annot_csv_22710.csv.txt\n",
            "75   1\n",
            "Hugh_Cholmondeley,_5th_Baron_Delamere\n",
            "./dataset_2/scrapes/Hugh_Cholmondeley,_5th_Baron_Delamere/annot_csv_6436.csv.txt\n",
            "18   2\n",
            "M%C3%B3nica_Echeverr%C3%ADa\n",
            "./dataset_2/scrapes/M%C3%B3nica_Echeverr%C3%ADa/annot_csv_20534.csv.txt\n",
            "27   3\n",
            "Kate_O%27Regan\n",
            "./dataset_2/scrapes/Kate_O%27Regan/annot_csv_13227.csv.txt\n",
            "68   4\n",
            "Hwang_Shin-hye\n",
            "./dataset_2/scrapes/Hwang_Shin-hye/annot_csv_22632.csv.txt\n",
            "17   5\n",
            "Lakshmi_Manchu\n",
            "./dataset_2/scrapes/Lakshmi_Manchu/annot_csv_10045.csv.txt\n",
            "16   6\n",
            "David_Eisenhower\n",
            "./dataset_2/scrapes/David_Eisenhower/annot_csv_2482.csv.txt\n",
            "34   7\n",
            "Yoshitha_Rajapaksa\n",
            "./dataset_2/scrapes/Yoshitha_Rajapaksa/annot_csv_4140.csv.txt\n",
            "48   8\n",
            "361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhLOsbTzB_sY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_paths_annot = "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}