{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wiki_ner_loader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratmcu/wiki_ner/blob/master/wiki_ner_loader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww-93dpD-3bB",
        "colab_type": "text"
      },
      "source": [
        "##Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0Ev0aJhF5Py",
        "colab_type": "code",
        "outputId": "767c8a7b-c9c8-4ed8-c73f-fd6279673530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "'''\n",
        "An entry or sent looks like ...\n",
        "SOCCER NN B-NP O\n",
        "- : O O\n",
        "JAPAN NNP B-NP B-LOC\n",
        "GET VB B-VP O\n",
        "LUCKY NNP B-NP O\n",
        "WIN NNP I-NP O\n",
        ", , O O\n",
        "CHINA NNP B-NP B-PER\n",
        "IN IN B-PP O\n",
        "SURPRISE DT B-NP O\n",
        "DEFEAT NN I-NP O\n",
        ". . O O\n",
        "Each mini-batch returns the followings:\n",
        "words: list of input sents. [\"The 26-year-old ...\", ...]\n",
        "x: encoded input sents. [N, T]. int64.\n",
        "is_heads: list of head markers. [[1, 1, 0, ...], [...]]\n",
        "tags: list of tags.['O O B-MISC ...', '...']\n",
        "y: encoded tags. [N, T]. int64\n",
        "seqlens: list of seqlens. [45, 49, 10, 50, ...]\n",
        "'''\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils import data\n",
        "!pip install pytorch-pretrained-bert\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "# import traceback\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
        "# VOCAB = ('<PAD>', 'O', 'I-LOC', 'B-PER', 'I-PER', 'I-ORG', 'I-MISC', 'B-MISC', 'B-LOC', 'B-ORG')\n",
        "\n",
        "tags = ['BD', 'BP', 'PR', 'SP', 'CH', 'ED']\n",
        "VOCAB_list = ['<PAD>', 'O',]\n",
        "for tag in tags:\n",
        "    VOCAB_list.append('I-'+tag)\n",
        "    VOCAB_list.append('B-'+tag)\n",
        "VOCAB = tuple(VOCAB_list)\n",
        "tag2idx = {tag: idx for idx, tag in enumerate(VOCAB)}\n",
        "idx2tag = {idx: tag for idx, tag in enumerate(VOCAB)}\n",
        "\n",
        "class NerDataset(data.Dataset):\n",
        "    def __init__(self, fpath):\n",
        "        \"\"\"\n",
        "        fpath: [train|valid|test].txt\n",
        "        \"\"\"\n",
        "        entries = open(fpath, 'r').read().strip().split(\"\\n\\n\")\n",
        "        sents, tags_li = [], [] # list of lists\n",
        "        for entry in entries:\n",
        "#             print(entry)\n",
        "            lines = entry.splitlines()\n",
        "            words = [line.split()[0] for line in entry.splitlines() if len(line.split()) > 1]\n",
        "#             try:\n",
        "#                 words = [line.split()[0] for line in entry.splitlines()]\n",
        "# #                 words = [line.split()[0] for line in entry.splitlines() if len(line.split())== 1 and line.split()[0] == 'O']\n",
        "#             except Exception as e:\n",
        "#                 print(traceback.format_exc())\n",
        "#                 print('splitting failed: ', [(ord(char), char) for char in entry])\n",
        "#                 continue\n",
        "            tags = ([line.split()[-1] for line in entry.splitlines() if len(line.split()) > 1])\n",
        "            sents.append([\"[CLS]\"] + words + [\"[SEP]\"])\n",
        "            tags_li.append([\"<PAD>\"] + tags + [\"<PAD>\"])\n",
        "        self.sents, self.tags_li = sents, tags_li\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n",
        "\n",
        "        # We give credits only to the first piece.\n",
        "        x, y = [], [] # list of ids\n",
        "        is_heads = [] # list. 1: the token is the first piece of a word\n",
        "        for w, t in zip(words, tags):\n",
        "            if ord(w[0]) in [65533, 8206, 150, 61656, 128, 157] : #bad tokens that causes miss matches in the token and is_head legths\n",
        "                continue \n",
        "            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n",
        "            xx = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "            is_head = [1] + [0]*(len(tokens) - 1)\n",
        "\n",
        "            t = [t] + [\"<PAD>\"] * (len(tokens) - 1)  # <PAD>: no decision\n",
        "            yy = [tag2idx[each] for each in t]  # (T,)\n",
        "\n",
        "            x.extend(xx)\n",
        "            is_heads.extend(is_head)\n",
        "            y.extend(yy)\n",
        "        try:\n",
        "          assert len(x)==len(y)==len(is_heads), f\"len(x)={len(x)}, len(y)={len(y)}, len(is_heads)={len(is_heads)}\"\n",
        "        except AssertionError:\n",
        "          print(tags)\n",
        "          for tag in words:\n",
        "              for c in tag:\n",
        "                  print(c, ord(c))\n",
        "          print(words)\n",
        "          print(x)\n",
        "          print(y)\n",
        "          print(is_heads)\n",
        "          raise BaseException(f\"len(x)={len(x)}, len(y)={len(y)}, len(is_heads)={len(is_heads)}\")\n",
        "        # seqlen\n",
        "        seqlen = len(y)\n",
        "\n",
        "        # to string\n",
        "        words = \" \".join(words)\n",
        "        tags = \" \".join(tags)\n",
        "        return words, x, is_heads, tags, y, seqlen\n",
        "    \n",
        "    def append(self, other):\n",
        "        self.sents.extend(other.sents)\n",
        "        self.tags_li.extend(other.tags_li)\n",
        "\n",
        "def pad(batch):\n",
        "    '''Pads to 50'''\n",
        "    f = lambda x: [sample[x] for sample in batch]\n",
        "    g = lambda x, seqlen: [sample[x] + [\" #!#!\",[0],[0],\" <PAD>\"][x] * (seqlen - len(sample[x])) if len(sample[x]) < seqlen else sample[x][:seqlen] for sample in batch]  \n",
        "    seqlens = f(-1)\n",
        "    maxlen = min(50, np.array(seqlens).max())\n",
        "    # print(type(batch[0][3]))\n",
        "    # words = g(0, maxlen)\n",
        "    words = f(0)\n",
        "    # print(type(words))\n",
        "    is_heads = g(2, maxlen)\n",
        "    # print(type(is_heads))\n",
        "    tags = [sample[3] for sample in batch] #g(3, maxlen)\n",
        "    # print(type(tags))\n",
        "    # maxlen = np.array(seqlens).max()\n",
        "    # g = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n",
        "    g = lambda x, seqlen: [ sample[x] + [0] * (seqlen - len(sample[x])) if len(sample[x]) < seqlen else sample[x][:seqlen] for sample in batch]  # 0: <pad>\n",
        "\n",
        "    x = g(1, maxlen)\n",
        "    y = g(-2, maxlen)\n",
        "\n",
        "    f = torch.LongTensor\n",
        "    # print(maxlen)\n",
        "    # print(len(tags))\n",
        "    # print(tags)\n",
        "    return words, f(x), is_heads, tags, f(y), [maxlen for sample in batch]\n",
        "\n",
        "def pad_max(batch):\n",
        "    '''Pads to the longest sample'''\n",
        "    f = lambda x: [sample[x] for sample in batch]\n",
        "    words = f(0)\n",
        "    is_heads = f(2)\n",
        "    tags = f(3)\n",
        "    seqlens = f(-1)\n",
        "    maxlen = np.array(seqlens).max()\n",
        "\n",
        "    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n",
        "    x = f(1, maxlen)\n",
        "    y = f(-2, maxlen)\n",
        "    f = torch.LongTensor\n",
        "    return words, f(x), is_heads, tags, f(y), seqlens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 35.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.17.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.10.40)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.13.40)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->pytorch-pretrained-bert) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.40->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 213450/213450 [00:00<00:00, 1060742.61B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1dibACaqHCg",
        "colab_type": "code",
        "outputId": "8b7dd829-d3eb-42e6-e4b0-972e3e90e432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "#experiment_code\n",
        "import os\n",
        "import time\n",
        "!pip install wget\n",
        "import wget\n",
        "import logging\n",
        "import pickle\n",
        "import ast\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib\n",
        "from bs4 import BeautifulSoup\n",
        "import tarfile\n",
        "if not os.path.exists('dataset.tar.gz'):\n",
        "    wget.download('https://github.com/ratmcu/wiki_ner/blob/master/dataset.tar.gz?raw=true')\n",
        "tar = tarfile.open('dataset.tar.gz', mode='r')\n",
        "tar.extractall('./')\n",
        "tar.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=7a32906f0d82dddcf5fdb818e4769c129b75252515ab36b45f2e8878dabbc31b\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8LI6Ec9qL-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def toConllTxt(path, save_file = None):\n",
        "    df = pd.read_csv(path)\n",
        "#     dir_path, _  = os.path.split(path)\n",
        "    if not save_file:\n",
        "        save_file = os.path.join(os.path.split(path)[0], '%s.txt' % path.split('/')[-1])\n",
        "    with open(save_file, 'w') as file:    \n",
        "        for i, row in enumerate(df.iterrows()):\n",
        "            if (row[1]['words'] == '\\n' and row[1]['tags'] == '\\n'):\n",
        "                file.write('\\n')\n",
        "            else:\n",
        "                try:\n",
        "                    file.write(row[1]['words']+' ')\n",
        "                except:\n",
        "                    file.write( str(row[1]['words']) + ' ')\n",
        "                file.write(row[1]['tags']+'\\n')\n",
        "    return save_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXY3rm9pngTr",
        "colab_type": "code",
        "outputId": "173ab946-3d9c-4aa8-c1e6-e9abd5f398c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        }
      },
      "source": [
        "#experiment_code\n",
        "paths = sorted([os.path.join(f[0], name) for f in os.walk('./dataset') if len(f[2])!=0 for name in f[2] if os.path.splitext(name)[-1] == '.csv'])\n",
        "import random\n",
        "rand_paths = random.choices(paths, k=10)\n",
        "dataset = NerDataset(toConllTxt(rand_paths[0]))\n",
        "for i, path in enumerate(rand_paths[1:]):\n",
        "    print(path.split('/')[-2])\n",
        "    txt_path = toConllTxt(path)\n",
        "    print(txt_path)\n",
        "    data_page = NerDataset(txt_path)\n",
        "    dataset.append(data_page)\n",
        "    print(len(data_page), ' ', i)\n",
        "print(len(dataset))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mihai Ghimpu\n",
            "./dataset/politicians/Moldova/Mihai Ghimpu/conll_tagged.csv.txt\n",
            "49   0\n",
            "Bujar Nishani\n",
            "./dataset/politicians/Albania/Bujar Nishani/conll_tagged.csv.txt\n",
            "28   1\n",
            "Natsagiin Bagabandi\n",
            "./dataset/politicians/Mongolia/Natsagiin Bagabandi/conll_tagged.csv.txt\n",
            "6   2\n",
            "Shavkat Mirziyoyev\n",
            "./dataset/politicians/Uzbekistan/Shavkat Mirziyoyev/conll_tagged.csv.txt\n",
            "101   3\n",
            "Pandeli Majko\n",
            "./dataset/politicians/Albania/Pandeli Majko/conll_tagged.csv.txt\n",
            "21   4\n",
            "Vincent Auriol\n",
            "./dataset/politicians/France/Vincent Auriol/conll_tagged.csv.txt\n",
            "46   5\n",
            "Erik Gustaf Boström\n",
            "./dataset/politicians/Poland/Erik Gustaf Boström/conll_tagged.csv.txt\n",
            "30   6\n",
            "Gaafar Nimeiry\n",
            "./dataset/politicians/South Sudan/Gaafar Nimeiry/conll_tagged.csv.txt\n",
            "80   7\n",
            "Sai Mauk Kham\n",
            "./dataset/politicians/Myanmar/Sai Mauk Kham/conll_tagged.csv.txt\n",
            "11   8\n",
            "413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5BQfS1-QTPO",
        "colab_type": "text"
      },
      "source": [
        "### **testing dataloader on all pages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YCgOwMgQQjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "paths = sorted([os.path.join(f[0], name) for f in os.walk('./dataset') if len(f[2])!=0 for name in f[2] if os.path.splitext(name)[-1] == '.csv'])\n",
        "dataset = NerDataset(toConllTxt(paths[0]))\n",
        "for i, path in enumerate(paths[1:]):\n",
        "    print(path.split('/')[-2])\n",
        "    txt_path = toConllTxt(path)\n",
        "    print(txt_path)\n",
        "    data_page = NerDataset(txt_path)\n",
        "    dataset.append(data_page)\n",
        "    print(len(data_page), ' ', i)\n",
        "print(len(dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "woqKeXVw7kaU",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "print(len(dataset))\n",
        "# dataset = NerDataset(toConllTxt(paths[0]))\n",
        "for sent in dataset:\n",
        "    print(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_zx39TVpZEo",
        "colab_type": "text"
      },
      "source": [
        "## working on Mass Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHoYxl7aAqVJ",
        "colab_type": "text"
      },
      "source": [
        "### creating the text files suitable for the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPcxIoFopcmw",
        "colab_type": "code",
        "outputId": "af64595e-5a90-44fb-ee57-11b43897d8dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "#experiment_code\n",
        "import tarfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "tar = tarfile.open('/content/drive/My Drive/imrsv/Colab Notebooks/dataset.tar.gz', mode='r')\n",
        "tar.extractall('./dataset_2')\n",
        "tar.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47FWkwEUB7Dq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "import os\n",
        "paths_annot = sorted([os.path.join(f[0], name) for f in os.walk('./dataset_2') \n",
        "                if len(f[2])!=0 for name in f[2] if os.path.splitext(name)[-1] == '.csv' and name.split('_')[0]=='annot'],\n",
        "               key=lambda path: int(path.split('_')[-1].split('.')[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abn6umqVpSBy",
        "colab_type": "code",
        "outputId": "ca4de8a4-ab7a-4e36-f990-1dfabb777e85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        }
      },
      "source": [
        "#experiment_code\n",
        "import random\n",
        "rand_paths = random.choices(paths_annot, k=10)\n",
        "dataset = NerDataset(toConllTxt(rand_paths[0]))\n",
        "for i, path in enumerate(rand_paths[1:]):bl\n",
        "    print(path.split('/')[-2])\n",
        "    txt_path = toConllTxt(path, save_file = None)\n",
        "    print(txt_path)\n",
        "    data_page = NerDataset(txt_path)\n",
        "    dataset.append(data_page)\n",
        "    print(len(data_page), ' ', i)\n",
        "print(len(dataset))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "John_Fleming_(priest)\n",
            "./dataset_2/scrapes/John_Fleming_(priest)/annot_csv_519.csv.txt\n",
            "39   0\n",
            "Princess_Alexandra,_The_Honourable_Lady_Ogilvy\n",
            "./dataset_2/scrapes/Princess_Alexandra,_The_Honourable_Lady_Ogilvy/annot_csv_22710.csv.txt\n",
            "75   1\n",
            "Hugh_Cholmondeley,_5th_Baron_Delamere\n",
            "./dataset_2/scrapes/Hugh_Cholmondeley,_5th_Baron_Delamere/annot_csv_6436.csv.txt\n",
            "18   2\n",
            "M%C3%B3nica_Echeverr%C3%ADa\n",
            "./dataset_2/scrapes/M%C3%B3nica_Echeverr%C3%ADa/annot_csv_20534.csv.txt\n",
            "27   3\n",
            "Kate_O%27Regan\n",
            "./dataset_2/scrapes/Kate_O%27Regan/annot_csv_13227.csv.txt\n",
            "68   4\n",
            "Hwang_Shin-hye\n",
            "./dataset_2/scrapes/Hwang_Shin-hye/annot_csv_22632.csv.txt\n",
            "17   5\n",
            "Lakshmi_Manchu\n",
            "./dataset_2/scrapes/Lakshmi_Manchu/annot_csv_10045.csv.txt\n",
            "16   6\n",
            "David_Eisenhower\n",
            "./dataset_2/scrapes/David_Eisenhower/annot_csv_2482.csv.txt\n",
            "34   7\n",
            "Yoshitha_Rajapaksa\n",
            "./dataset_2/scrapes/Yoshitha_Rajapaksa/annot_csv_4140.csv.txt\n",
            "48   8\n",
            "361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lG1cWaZSM2o",
        "colab_type": "text"
      },
      "source": [
        "## model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnKZh68zF5SR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_pretrained_bert import BertModel\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, top_rnns=False, vocab_size=None, device='cpu', finetuning=False):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "        self.top_rnns=top_rnns\n",
        "        if top_rnns:\n",
        "            self.rnn = nn.LSTM(bidirectional=True, num_layers=2, input_size=768, hidden_size=768//2, batch_first=True)\n",
        "        self.fc = nn.Linear(768, vocab_size)\n",
        "\n",
        "        self.device = device\n",
        "        self.finetuning = finetuning\n",
        "\n",
        "    def forward(self, x, y, ):\n",
        "        '''\n",
        "        x: (N, T). int64\n",
        "        y: (N, T). int64\n",
        "        Returns\n",
        "        enc: (N, T, VOCAB)\n",
        "        '''\n",
        "        x = x.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        if self.training and self.finetuning:\n",
        "            # print(\"->bert.train()\")\n",
        "            self.bert.train()\n",
        "            encoded_layers, _ = self.bert(x)\n",
        "            enc = encoded_layers[-1]\n",
        "        else:\n",
        "            self.bert.eval()\n",
        "            with torch.no_grad():\n",
        "                encoded_layers, _ = self.bert(x)\n",
        "                enc = encoded_layers[-1]\n",
        "\n",
        "        if self.top_rnns:\n",
        "            enc, _ = self.rnn(enc)\n",
        "        logits = self.fc(enc)\n",
        "        y_hat = logits.argmax(-1)\n",
        "        return logits, y, y_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc6U5qZZIWMO",
        "colab_type": "text"
      },
      "source": [
        "##Loading the final dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3zHe5dMh3yh",
        "colab_type": "code",
        "outputId": "cf911f2f-73cb-4f2d-fca3-5b7a78bc1126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#experiment_code\n",
        "import tarfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "tar = tarfile.open('/content/drive/My Drive/imrsv/dataset_txt.tar.gz', mode='r')\n",
        "tar.extractall('./dataset_txt')\n",
        "tar.close()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acL2OzyNpArT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "import os\n",
        "paths_annot = sorted([os.path.join(f[0], name) for f in os.walk('./dataset_txt') \n",
        "                if len(f[2])!=0 for name in f[2] if os.path.splitext(name)[-1] == '.txt' and name.split('_')[0]=='annot'],\n",
        "               key=lambda path: int(path.split('_')[-1].split('.')[0]))\n",
        "\n",
        "training_files = [path for path in paths_annot if 'training' in path.split('/')]\n",
        "eval_files = [path for path in paths_annot if 'eval' in path.split('/')]\n",
        "test_files = [path for path in paths_annot if 'test' in path.split('/')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5Trfgji2zi0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "train_dataset = NerDataset(training_files[10])\n",
        "_ = [train_dataset.append(NerDataset(train_file)) for train_file in training_files[11:]]\n",
        "\n",
        "eval_dataset = NerDataset(eval_files[0])\n",
        "_ = [eval_dataset.append(NerDataset(eval_file)) for eval_file in eval_files[1:10]]\n",
        "\n",
        "test_dataset = NerDataset(test_files[0])\n",
        "_ = [test_dataset.append(NerDataset(test_file)) for test_file in test_files[1:]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmHXWLVJPl1G",
        "colab_type": "code",
        "outputId": "0f1d5528-17eb-4160-944a-3ead37596514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#experiment_code\n",
        "len(train_dataset), len(eval_dataset), len(test_dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(868503, 753, 20318)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewlytX15t5E1",
        "colab_type": "text"
      },
      "source": [
        "Iterator tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0P_WPUmS8SH",
        "colab_type": "code",
        "outputId": "9811744f-ff53-4bca-fadf-b2a3c1d526b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "#experiment_code\n",
        "from torch.utils import data\n",
        "train_iter = data.DataLoader(dataset=train_dataset,\n",
        "                              batch_size=32,\n",
        "                              shuffle=True,\n",
        "                              num_workers=16,\n",
        "                              collate_fn=pad)\n",
        "eval_iter = data.DataLoader(dataset=eval_dataset,\n",
        "                              batch_size=1,\n",
        "                              shuffle=True,\n",
        "                              num_workers=16,\n",
        "                              collate_fn=pad)\n",
        "test_iter = data.DataLoader(dataset=test_dataset,\n",
        "                              batch_size=32,\n",
        "                              shuffle=True,\n",
        "                              num_workers=16,\n",
        "                              collate_fn=pad)\n",
        "print(len(train_iter))\n",
        "print(len(eval_iter))\n",
        "print(len(test_iter))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27141\n",
            "753\n",
            "635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK1Z5x8pMpWk",
        "colab_type": "code",
        "outputId": "738361b0-9c9b-4a2a-ee2f-d82323168b2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#experiment_code\n",
        "for i, batch in enumerate(train_iter):\n",
        "    words, x, is_heads, tags, y, seqlens = batch \n",
        "    # print(words, x, is_heads, tags, y, seqlens)\n",
        "for i, batch in enumerate(eval_iter):\n",
        "    words, x, is_heads, tags, y, seqlens = batch \n",
        "    # print(words, x, is_heads, tags, y, seqlens)\n",
        "for i, batch in enumerate(test_iter):\n",
        "    words, x, is_heads, tags, y, seqlens = batch \n",
        "    # print(words, x, is_heads, tags, y, seqlens) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "503\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjrs3PLytSWk",
        "colab_type": "text"
      },
      "source": [
        "## Dataset introspection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvSrqDzItPl2",
        "colab_type": "code",
        "outputId": "28d37e9e-a1ca-44fd-9afc-9f64a5316cf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#experiment_code\n",
        "invstgt_iter = data.DataLoader(dataset=train_dataset,\n",
        "                              batch_size=1,\n",
        "                              shuffle=True,\n",
        "                              num_workers=1,\n",
        "                              collate_fn=pad)\n",
        "tag_count_dict = {} \n",
        "[tag_count_dict.update({tag:0}) for tag in tags]\n",
        "for i, batch in enumerate(invstgt_iter):\n",
        "    words, x, is_heads, tags_, y, seqlens = batch \n",
        "    # [ print(tag) for tag in tags_[0].split() if len(tag) > 2 and tag[2:] in tag_count_dict.keys() and tag[0] == 'B' ]\n",
        "    [ tag_count_dict.update({tag[2:]:tag_count_dict[tag[2:]]+1}) for tag in tags_[0].split() if len(tag) > 2 and tag[2:] in tag_count_dict.keys() and tag[0] == 'B' ]\n",
        "    # if i == 10: break\n",
        "tag_count_dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BD': 16880, 'BP': 0, 'CH': 10824, 'ED': 24365, 'PR': 6325, 'SP': 25163}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngo_3wsOxEXT",
        "colab_type": "code",
        "outputId": "80fb9749-3feb-44f4-ca0c-ecb7fd5e09d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#experiment_code\n",
        "#{'BD': 16880, 'BP': 0, 'CH': 10824, 'ED': 24365, 'PR': 6325, 'SP': 25163}\n",
        "tag_count_dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BD': 16880, 'BP': 0, 'CH': 10824, 'ED': 24365, 'PR': 6325, 'SP': 25163}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZeMO5NMz1ds",
        "colab_type": "code",
        "outputId": "81b6baac-32bd-495c-d3f6-cd9f102c3470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#experiment_code\n",
        "import multiprocessing\n",
        "multiprocessing.cpu_count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjF9fet1G7N9",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUgSDE_AJqRb",
        "colab_type": "code",
        "outputId": "fcd49f00-1cea-48ab-8f85-c4e5f20a1c8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "# from model import Net\n",
        "# from data_load import NerDataset, pad, VOCAB, tokenizer, tag2idx, idx2tag\n",
        "import os\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    model.train()\n",
        "    for i, batch in enumerate(iterator):\n",
        "        words, x, is_heads, tags, y, seqlens = batch\n",
        "        _y = y # for monitoring\n",
        "        optimizer.zero_grad()\n",
        "        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n",
        "\n",
        "        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n",
        "        y = y.view(-1)  # (N*T,)\n",
        "\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i==0:\n",
        "            print(\"=====sanity check======\")\n",
        "            print(\"words:\", words[0])\n",
        "            print(\"x:\", x.cpu().numpy()[0][:seqlens[0]])\n",
        "            print(\"tokens:\", tokenizer.convert_ids_to_tokens(x.cpu().numpy()[0])[:seqlens[0]])\n",
        "            print(\"is_heads:\", is_heads[0])\n",
        "            print(\"y:\", _y.cpu().numpy()[0][:seqlens[0]])\n",
        "            print(\"tags:\", tags[0])\n",
        "            print(\"seqlen:\", seqlens[0])\n",
        "            print(\"=======================\")\n",
        "\n",
        "        if i%10==0: # monitoring\n",
        "            print(f\"step: {i}, loss: {loss.item()}\")\n",
        "            # return\n",
        "\n",
        "def eval(model, iterator, f):\n",
        "    model.eval()\n",
        "\n",
        "    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            words, x, is_heads, tags, y, seqlens = batch\n",
        "\n",
        "            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n",
        "\n",
        "            Words.extend(words)\n",
        "            Is_heads.extend(is_heads)\n",
        "            Tags.extend(tags)\n",
        "            Y.extend(y.numpy().tolist())\n",
        "            Y_hat.extend(y_hat.cpu().numpy().tolist())\n",
        "\n",
        "    ## gets results and save\n",
        "    with open(\"temp\", 'w') as fout:\n",
        "        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n",
        "            # print(f\"len y_hat: {len(y_hat)} is_head {len(is_heads)} \\n\")\n",
        "            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n",
        "            preds = [idx2tag[hat] for hat in y_hat]\n",
        "            # assert len(preds)==len(words.split())==len(tags.split())\n",
        "            # assert len(words.split())==len(tags.split())\n",
        "            # print(preds, words, tags.split()[:len(preds)])\n",
        "            words = [word for word in words.split()]\n",
        "            # tokens = [tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w] for w in words.split()] \n",
        "            # print(preds, words, tags.split()[:len(preds)])\n",
        "            tags = tags.split()[:len(preds)]\n",
        "            # print(len(preds), len(words), len(tags))\n",
        "            # assert len(preds)==len(words),  f\"len(peds)={len(preds)}, len(words)={len(words)}, len(is_heads)={len(is_heads)}\"\n",
        "            for w, t, p in zip(words[1:-1], tags[1:-1], preds[1:-1]):\n",
        "                fout.write(f\"{w} {t} {p}\\n\")\n",
        "            fout.write(\"\\n\")\n",
        "\n",
        "    ## calc metric\n",
        "    y_true =  np.array([tag2idx[line.split()[1]] for line in open(\"temp\", 'r').read().splitlines() if len(line) > 0]) #original tags\n",
        "    y_pred =  np.array([tag2idx[line.split()[2]] for line in open(\"temp\", 'r').read().splitlines() if len(line) > 0]) #predicted tags\n",
        "\n",
        "    num_proposed = len(y_pred[y_pred>1])\n",
        "    num_correct = (np.logical_and(y_true==y_pred, y_true>1)).astype(np.int).sum()\n",
        "    num_gold = len(y_true[y_true>1])\n",
        "\n",
        "    print(f\"num_proposed:{num_proposed}\")\n",
        "    print(f\"num_correct:{num_correct}\")\n",
        "    print(f\"num_gold:{num_gold}\")\n",
        "    \n",
        "    try:\n",
        "        precision = num_correct / num_proposed\n",
        "    except ZeroDivisionError:\n",
        "        precision = 1.0\n",
        "\n",
        "    try:\n",
        "        recall = num_correct / num_gold\n",
        "    except ZeroDivisionError:\n",
        "        recall = 1.0\n",
        "\n",
        "    try:\n",
        "        f1 = 2*precision*recall / (precision + recall)\n",
        "    except ZeroDivisionError:\n",
        "        if precision*recall==0:\n",
        "            f1=1.0\n",
        "        else:\n",
        "            f1=0\n",
        "\n",
        "    final = f + \".P%.2f_R%.2f_F%.2f\" %(precision, recall, f1)\n",
        "    with open(final, 'w') as fout:\n",
        "        result = open(\"temp\", \"r\").read()\n",
        "        fout.write(f\"{result}\\n\")\n",
        "        fout.write(f\"precision={precision}\\n\")\n",
        "        fout.write(f\"recall={recall}\\n\")\n",
        "        fout.write(f\"f1={f1}\\n\")\n",
        "\n",
        "    os.remove(\"temp\")\n",
        "    print(\"precision=%.2f\"%precision)\n",
        "    print(\"recall=%.2f\"%recall)\n",
        "    print(\"f1=%.2f\"%f1)\n",
        "    return precision, recall, f1\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1735235584\n",
            "1735235584\n",
            "1735235584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxrHwSyPlqIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "# if __name__==\"__main__\":\n",
        "if True:\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument(\"--batch_size\", type=int, default=128)\n",
        "#     parser.add_argument(\"--lr\", type=float, default=0.0001)\n",
        "#     parser.add_argument(\"--n_epochs\", type=int, default=30)\n",
        "#     parser.add_argument(\"--finetuning\", dest=\"finetuning\", action=\"store_true\")\n",
        "#     parser.add_argument(\"--top_rnns\", dest=\"top_rnns\", action=\"store_true\")\n",
        "#     parser.add_argument(\"--logdir\", type=str, default=\"checkpoints/01\")\n",
        "#     parser.add_argument(\"--trainset\", type=str, default=\"conll2003/train.txt\")\n",
        "#     parser.add_argument(\"--validset\", type=str, default=\"conll2003/valid.txt\")\n",
        "#     hp = parser.parse_args()\n",
        "    print(torch.cuda.max_memory_allocated(device=None))\n",
        "    batch_size = 32\n",
        "    lr = 0.0001\n",
        "    n_epochs = 30\n",
        "    finetuning = True\n",
        "    top_rnns = False\n",
        "    logdir = \"checkpoints/01\"\n",
        "    trainset = \"conll2003/train.txt\"\n",
        "    validset = \"conll2003/valid.txt\"\n",
        "    \n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#     device = 'cpu'\n",
        "\n",
        "#     model = Net(hp.top_rnns, len(VOCAB), device, hp.finetuning).cuda()\n",
        "    model = Net(top_rnns=top_rnns, vocab_size=len(VOCAB), device=device, finetuning=finetuning).cuda()\n",
        "#     model = Net(top_rnns=top_rnns, vocab_size=len(VOCAB), device=device, finetuning=finetuning)\n",
        "    print(torch.cuda.max_memory_allocated(device=None))\n",
        "    model = nn.DataParallel(model)\n",
        "    print(torch.cuda.max_memory_allocated(device=None))\n",
        "\n",
        "    # train_dataset = NerDataset(trainset)\n",
        "    # eval_dataset = NerDataset(validset)\n",
        "    # pad_fn = pad\n",
        "    # train_iter = data.DataLoader(dataset=train_dataset,\n",
        "    #                              batch_size=batch_size,\n",
        "    #                              shuffle=True,\n",
        "    #                              num_workers=4,\n",
        "    #                              collate_fn=pad_fn)\n",
        "    # eval_iter = data.DataLoader(dataset=eval_dataset,\n",
        "    #                              batch_size=batch_size,\n",
        "    #                              shuffle=False,\n",
        "    #                              num_workers=4,\n",
        "    #                              collate_fn=pad_fn)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    # for epoch in range(1, n_epochs+1):\n",
        "    #     train(model, train_iter, optimizer, criterion)\n",
        "\n",
        "    #     print(f\"=========eval at epoch={epoch}=========\")\n",
        "    #     if not os.path.exists(logdir): os.makedirs(logdir)\n",
        "    #     fname = os.path.join(logdir, str(epoch))\n",
        "    #     precision, recall, f1 = eval(model, eval_iter, fname)\n",
        "\n",
        "    #     torch.save(model.state_dict(), f\"{fname}.pt\")\n",
        "    #     print(f\"weights were saved to {fname}.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFHLQO_tocHJ",
        "colab_type": "code",
        "outputId": "0a06c194-bca7-4181-b0c8-f09e6588ffe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#experiment_code\n",
        "for epoch in range(1, 1+1):\n",
        "    train(model, train_iter, optimizer, criterion)\n",
        "    # print(f\"=========eval at epoch={epoch}=========\")\n",
        "    # if not os.path.exists(logdir): os.makedirs(logdir)\n",
        "    # fname = os.path.join(logdir, str(epoch))\n",
        "    \n",
        "    # precision, recall, f1 = eval(model, eval_iter, fname)\n",
        "    # torch.save(model.state_dict(), f\"{fname}.pt\")\n",
        "    # print(f\"weights were saved to {fname}.pt\")\n",
        "print(torch.cuda.max_memory_allocated(device=None))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=====sanity check======\n",
            "words: [CLS] When the Boyaner Rebbe of New York died of a stroke on 2 March 1971 , the Boyaner Hasidim were left leaderless . [SEP]\n",
            "x: [  101  1332  1103  4596  6354  1197 11336 20584  1104  1203  1365  1452\n",
            "  1104   170  6625  1113   123  1345  2507   117  1103  4596  6354  1197\n",
            " 10736  2386  4060  1127  1286  2301  2008   119   102     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0]\n",
            "tokens: ['[CLS]', 'When', 'the', 'Boy', '##ane', '##r', 'Re', '##bbe', 'of', 'New', 'York', 'died', 'of', 'a', 'stroke', 'on', '2', 'March', '1971', ',', 'the', 'Boy', '##ane', '##r', 'Has', '##id', '##im', 'were', 'left', 'leader', '##less', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "is_heads: [1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "y: [0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "tags: <PAD> O O O O O O O O O O O O O O O O O O O O O O O <PAD>\n",
            "seqlen: 50\n",
            "=======================\n",
            "step: 0, loss: 0.09921260178089142\n",
            "step: 10, loss: 0.011440354399383068\n",
            "step: 20, loss: 0.02303982898592949\n",
            "step: 30, loss: 0.0892380028963089\n",
            "step: 40, loss: 0.07155375927686691\n",
            "step: 50, loss: 0.11157815903425217\n",
            "step: 60, loss: 0.13801045715808868\n",
            "step: 70, loss: 0.12890782952308655\n",
            "step: 80, loss: 0.10463543236255646\n",
            "step: 90, loss: 0.05927193537354469\n",
            "step: 100, loss: 0.01468541007488966\n",
            "step: 110, loss: 0.011586740612983704\n",
            "step: 120, loss: 0.05317993834614754\n",
            "step: 130, loss: 0.06509164720773697\n",
            "step: 140, loss: 0.24418967962265015\n",
            "step: 150, loss: 0.09700499475002289\n",
            "step: 160, loss: 0.12684927880764008\n",
            "step: 170, loss: 0.041645463556051254\n",
            "step: 180, loss: 0.17544783651828766\n",
            "step: 190, loss: 0.12965290248394012\n",
            "step: 200, loss: 0.14547674357891083\n",
            "step: 210, loss: 0.012778696604073048\n",
            "step: 220, loss: 0.08708412200212479\n",
            "step: 230, loss: 0.0824332907795906\n",
            "step: 240, loss: 0.04325448349118233\n",
            "step: 250, loss: 0.14100408554077148\n",
            "step: 260, loss: 0.10670474916696548\n",
            "step: 270, loss: 0.054318785667419434\n",
            "step: 280, loss: 0.011983939446508884\n",
            "step: 290, loss: 0.11299774795770645\n",
            "step: 300, loss: 0.15940171480178833\n",
            "step: 310, loss: 0.06694918125867844\n",
            "step: 320, loss: 0.013855342753231525\n",
            "step: 330, loss: 0.11605644226074219\n",
            "step: 340, loss: 0.18745824694633484\n",
            "step: 350, loss: 0.08356843143701553\n",
            "step: 360, loss: 0.0841778889298439\n",
            "step: 370, loss: 0.19810105860233307\n",
            "step: 380, loss: 0.19502657651901245\n",
            "step: 390, loss: 0.16042622923851013\n",
            "step: 400, loss: 0.050968557596206665\n",
            "step: 410, loss: 0.06681493669748306\n",
            "step: 420, loss: 0.11241377145051956\n",
            "step: 430, loss: 0.08933842927217484\n",
            "step: 440, loss: 0.011858650483191013\n",
            "step: 450, loss: 0.1333741396665573\n",
            "step: 460, loss: 0.032722581177949905\n",
            "step: 470, loss: 0.011973217129707336\n",
            "step: 480, loss: 0.12183527648448944\n",
            "step: 490, loss: 0.037839945405721664\n",
            "step: 500, loss: 0.15698644518852234\n",
            "step: 510, loss: 0.13046421110630035\n",
            "step: 520, loss: 0.11752510815858841\n",
            "step: 530, loss: 0.0967942476272583\n",
            "step: 540, loss: 0.16306893527507782\n",
            "step: 550, loss: 0.046900682151317596\n",
            "step: 560, loss: 0.06160985678434372\n",
            "step: 570, loss: 0.09776856005191803\n",
            "step: 580, loss: 0.13907600939273834\n",
            "step: 590, loss: 0.09388987720012665\n",
            "step: 600, loss: 0.015089969150722027\n",
            "step: 610, loss: 0.14324438571929932\n",
            "step: 620, loss: 0.11136049032211304\n",
            "step: 630, loss: 0.16800808906555176\n",
            "step: 640, loss: 0.08224450796842575\n",
            "step: 650, loss: 0.023430736735463142\n",
            "step: 660, loss: 0.15851491689682007\n",
            "step: 670, loss: 0.20357045531272888\n",
            "step: 680, loss: 0.06592650711536407\n",
            "step: 690, loss: 0.025847340002655983\n",
            "step: 700, loss: 0.011997845955193043\n",
            "step: 710, loss: 0.0357893630862236\n",
            "step: 720, loss: 0.06578850001096725\n",
            "step: 730, loss: 0.10454294085502625\n",
            "step: 740, loss: 0.12887364625930786\n",
            "step: 750, loss: 0.12459658086299896\n",
            "step: 760, loss: 0.08083748817443848\n",
            "step: 770, loss: 0.13064654171466827\n",
            "step: 780, loss: 0.1482093185186386\n",
            "step: 790, loss: 0.013385508209466934\n",
            "step: 800, loss: 0.07996433228254318\n",
            "step: 810, loss: 0.0477161668241024\n",
            "step: 820, loss: 0.051696788519620895\n",
            "step: 830, loss: 0.11150480806827545\n",
            "step: 840, loss: 0.11615503579378128\n",
            "step: 850, loss: 0.13129021227359772\n",
            "step: 860, loss: 0.1697191596031189\n",
            "step: 870, loss: 0.06365063786506653\n",
            "step: 880, loss: 0.24358773231506348\n",
            "step: 890, loss: 0.02669498324394226\n",
            "step: 900, loss: 0.14220474660396576\n",
            "step: 910, loss: 0.09595389664173126\n",
            "step: 920, loss: 0.1400029957294464\n",
            "step: 930, loss: 0.06114538013935089\n",
            "step: 940, loss: 0.09823979437351227\n",
            "step: 950, loss: 0.019827451556921005\n",
            "step: 960, loss: 0.06821321696043015\n",
            "step: 970, loss: 0.06467205286026001\n",
            "step: 980, loss: 0.08225894719362259\n",
            "step: 990, loss: 0.06216411665081978\n",
            "step: 1000, loss: 0.09664689004421234\n",
            "step: 1010, loss: 0.014007681980729103\n",
            "step: 1020, loss: 0.163412943482399\n",
            "step: 1030, loss: 0.13626548647880554\n",
            "step: 1040, loss: 0.10560968518257141\n",
            "step: 1050, loss: 0.02193368598818779\n",
            "step: 1060, loss: 0.10090915113687515\n",
            "step: 1070, loss: 0.062142059206962585\n",
            "step: 1080, loss: 0.18504148721694946\n",
            "step: 1090, loss: 0.18070486187934875\n",
            "step: 1100, loss: 0.10428822040557861\n",
            "step: 1110, loss: 0.037204284220933914\n",
            "step: 1120, loss: 0.13312047719955444\n",
            "step: 1130, loss: 0.051279596984386444\n",
            "step: 1140, loss: 0.18815010786056519\n",
            "step: 1150, loss: 0.025618620216846466\n",
            "step: 1160, loss: 0.17863501608371735\n",
            "step: 1170, loss: 0.041932787746191025\n",
            "step: 1180, loss: 0.06509094685316086\n",
            "step: 1190, loss: 0.05645546689629555\n",
            "step: 1200, loss: 0.17632794380187988\n",
            "step: 1210, loss: 0.07596241682767868\n",
            "step: 1220, loss: 0.12493900209665298\n",
            "step: 1230, loss: 0.0592193603515625\n",
            "step: 1240, loss: 0.043745726346969604\n",
            "step: 1250, loss: 0.04583737999200821\n",
            "step: 1260, loss: 0.18590879440307617\n",
            "step: 1270, loss: 0.16139239072799683\n",
            "step: 1280, loss: 0.1256345957517624\n",
            "step: 1290, loss: 0.03452198952436447\n",
            "step: 1300, loss: 0.07581766694784164\n",
            "step: 1310, loss: 0.17322973906993866\n",
            "step: 1320, loss: 0.04084837809205055\n",
            "step: 1330, loss: 0.012576218694448471\n",
            "step: 1340, loss: 0.05948605760931969\n",
            "step: 1350, loss: 0.0777013897895813\n",
            "step: 1360, loss: 0.0395321287214756\n",
            "step: 1370, loss: 0.181298166513443\n",
            "step: 1380, loss: 0.10271815955638885\n",
            "step: 1390, loss: 0.1420389860868454\n",
            "step: 1400, loss: 0.17599570751190186\n",
            "step: 1410, loss: 0.018933851271867752\n",
            "step: 1420, loss: 0.15585173666477203\n",
            "step: 1430, loss: 0.1048283725976944\n",
            "step: 1440, loss: 0.046707361936569214\n",
            "step: 1450, loss: 0.26250430941581726\n",
            "step: 1460, loss: 0.0798615887761116\n",
            "step: 1470, loss: 0.17696982622146606\n",
            "step: 1480, loss: 0.047881968319416046\n",
            "step: 1490, loss: 0.03469156473875046\n",
            "step: 1500, loss: 0.1927303671836853\n",
            "step: 1510, loss: 0.2192223072052002\n",
            "step: 1520, loss: 0.014561530202627182\n",
            "step: 1530, loss: 0.054324109107255936\n",
            "step: 1540, loss: 0.18220143020153046\n",
            "step: 1550, loss: 0.1065463274717331\n",
            "step: 1560, loss: 0.067404605448246\n",
            "step: 1570, loss: 0.12286887317895889\n",
            "step: 1580, loss: 0.04240229353308678\n",
            "step: 1590, loss: 0.21836473047733307\n",
            "step: 1600, loss: 0.24901798367500305\n",
            "step: 1610, loss: 0.20461145043373108\n",
            "step: 1620, loss: 0.10966544598340988\n",
            "step: 1630, loss: 0.195724219083786\n",
            "step: 1640, loss: 0.030078373849391937\n",
            "step: 1650, loss: 0.03518638387322426\n",
            "step: 1660, loss: 0.08666600286960602\n",
            "step: 1670, loss: 0.011465617455542088\n",
            "step: 1680, loss: 0.10431620478630066\n",
            "step: 1690, loss: 0.04209381341934204\n",
            "step: 1700, loss: 0.08139443397521973\n",
            "step: 1710, loss: 0.17329175770282745\n",
            "step: 1720, loss: 0.13291411101818085\n",
            "step: 1730, loss: 0.10837932676076889\n",
            "step: 1740, loss: 0.02166888676583767\n",
            "step: 1750, loss: 0.03883196786046028\n",
            "step: 1760, loss: 0.11656149476766586\n",
            "step: 1770, loss: 0.07701785117387772\n",
            "step: 1780, loss: 0.08807801455259323\n",
            "step: 1790, loss: 0.03615358844399452\n",
            "step: 1800, loss: 0.16330251097679138\n",
            "step: 1810, loss: 0.07722657173871994\n",
            "step: 1820, loss: 0.011751420795917511\n",
            "step: 1830, loss: 0.02825908176600933\n",
            "step: 1840, loss: 0.07825452089309692\n",
            "step: 1850, loss: 0.24205440282821655\n",
            "step: 1860, loss: 0.08889121562242508\n",
            "step: 1870, loss: 0.06455600261688232\n",
            "step: 1880, loss: 0.052781842648983\n",
            "step: 1890, loss: 0.012979002669453621\n",
            "step: 1900, loss: 0.13408522307872772\n",
            "step: 1910, loss: 0.014582224190235138\n",
            "step: 1920, loss: 0.04663848876953125\n",
            "step: 1930, loss: 0.15872123837471008\n",
            "step: 1940, loss: 0.05814937502145767\n",
            "step: 1950, loss: 0.07736848294734955\n",
            "step: 1960, loss: 0.10979209840297699\n",
            "step: 1970, loss: 0.07918667793273926\n",
            "step: 1980, loss: 0.01093302108347416\n",
            "step: 1990, loss: 0.17177671194076538\n",
            "step: 2000, loss: 0.10241018980741501\n",
            "step: 2010, loss: 0.18093299865722656\n",
            "step: 2020, loss: 0.0816173106431961\n",
            "step: 2030, loss: 0.03960783779621124\n",
            "step: 2040, loss: 0.05490078032016754\n",
            "step: 2050, loss: 0.083918996155262\n",
            "step: 2060, loss: 0.05765580013394356\n",
            "step: 2070, loss: 0.023664133623242378\n",
            "step: 2080, loss: 0.15875619649887085\n",
            "step: 2090, loss: 0.16957925260066986\n",
            "step: 2100, loss: 0.05528933182358742\n",
            "step: 2110, loss: 0.08797132968902588\n",
            "step: 2120, loss: 0.04170870780944824\n",
            "step: 2130, loss: 0.05703520402312279\n",
            "step: 2140, loss: 0.10544410347938538\n",
            "step: 2150, loss: 0.05814402177929878\n",
            "step: 2160, loss: 0.12599137425422668\n",
            "step: 2170, loss: 0.12669101357460022\n",
            "step: 2180, loss: 0.09749522060155869\n",
            "step: 2190, loss: 0.04291559010744095\n",
            "step: 2200, loss: 0.061253003776073456\n",
            "step: 2210, loss: 0.12474068254232407\n",
            "step: 2220, loss: 0.06124887242913246\n",
            "step: 2230, loss: 0.13562677800655365\n",
            "step: 2240, loss: 0.07313619554042816\n",
            "step: 2250, loss: 0.1925472617149353\n",
            "step: 2260, loss: 0.08934783190488815\n",
            "step: 2270, loss: 0.15435802936553955\n",
            "step: 2280, loss: 0.10315463691949844\n",
            "step: 2290, loss: 0.1291273683309555\n",
            "step: 2300, loss: 0.10670459270477295\n",
            "step: 2310, loss: 0.1239483430981636\n",
            "step: 2320, loss: 0.04410584643483162\n",
            "step: 2330, loss: 0.11541715264320374\n",
            "step: 2340, loss: 0.02140074037015438\n",
            "step: 2350, loss: 0.13059012591838837\n",
            "step: 2360, loss: 0.09213823825120926\n",
            "step: 2370, loss: 0.16135062277317047\n",
            "step: 2380, loss: 0.22375178337097168\n",
            "step: 2390, loss: 0.08967197686433792\n",
            "step: 2400, loss: 0.03694252297282219\n",
            "step: 2410, loss: 0.04712608456611633\n",
            "step: 2420, loss: 0.05070774629712105\n",
            "step: 2430, loss: 0.1875220537185669\n",
            "step: 2440, loss: 0.1329158991575241\n",
            "step: 2450, loss: 0.08980114758014679\n",
            "step: 2460, loss: 0.10824060440063477\n",
            "step: 2470, loss: 0.09641316533088684\n",
            "step: 2480, loss: 0.011231977492570877\n",
            "step: 2490, loss: 0.044116850942373276\n",
            "step: 2500, loss: 0.08987385034561157\n",
            "step: 2510, loss: 0.06627378612756729\n",
            "step: 2520, loss: 0.0976942926645279\n",
            "step: 2530, loss: 0.12842534482479095\n",
            "step: 2540, loss: 0.019801003858447075\n",
            "step: 2550, loss: 0.132119283080101\n",
            "step: 2560, loss: 0.11769707500934601\n",
            "step: 2570, loss: 0.10089902579784393\n",
            "step: 2580, loss: 0.022663818672299385\n",
            "step: 2590, loss: 0.040434472262859344\n",
            "step: 2600, loss: 0.031258855015039444\n",
            "step: 2610, loss: 0.029627110809087753\n",
            "step: 2620, loss: 0.08601538836956024\n",
            "step: 2630, loss: 0.09931296110153198\n",
            "step: 2640, loss: 0.1386832892894745\n",
            "step: 2650, loss: 0.19331097602844238\n",
            "step: 2660, loss: 0.1522802710533142\n",
            "step: 2670, loss: 0.10553479939699173\n",
            "step: 2680, loss: 0.15926013886928558\n",
            "step: 2690, loss: 0.09111841768026352\n",
            "step: 2700, loss: 0.033953096717596054\n",
            "step: 2710, loss: 0.10907396674156189\n",
            "step: 2720, loss: 0.03764466196298599\n",
            "step: 2730, loss: 0.10602307319641113\n",
            "step: 2740, loss: 0.09538490325212479\n",
            "step: 2750, loss: 0.10242382436990738\n",
            "step: 2760, loss: 0.07310114800930023\n",
            "step: 2770, loss: 0.2365618646144867\n",
            "step: 2780, loss: 0.09245354682207108\n",
            "step: 2790, loss: 0.07793615013360977\n",
            "step: 2800, loss: 0.07244464755058289\n",
            "step: 2810, loss: 0.13252905011177063\n",
            "step: 2820, loss: 0.07606305181980133\n",
            "step: 2830, loss: 0.1414932906627655\n",
            "step: 2840, loss: 0.10166029632091522\n",
            "step: 2850, loss: 0.10542353987693787\n",
            "step: 2860, loss: 0.09876246005296707\n",
            "step: 2870, loss: 0.06322701275348663\n",
            "step: 2880, loss: 0.17287437617778778\n",
            "step: 2890, loss: 0.2253551483154297\n",
            "step: 2900, loss: 0.056970637291669846\n",
            "step: 2910, loss: 0.14814698696136475\n",
            "step: 2920, loss: 0.13708151876926422\n",
            "step: 2930, loss: 0.1583511233329773\n",
            "step: 2940, loss: 0.11388584971427917\n",
            "step: 2950, loss: 0.07999112457036972\n",
            "step: 2960, loss: 0.051313914358615875\n",
            "step: 2970, loss: 0.06849640607833862\n",
            "step: 2980, loss: 0.0673149898648262\n",
            "step: 2990, loss: 0.08835915476083755\n",
            "step: 3000, loss: 0.06232535466551781\n",
            "step: 3010, loss: 0.10960082709789276\n",
            "step: 3020, loss: 0.1573891043663025\n",
            "step: 3030, loss: 0.06807076185941696\n",
            "step: 3040, loss: 0.06480606645345688\n",
            "step: 3050, loss: 0.06574197113513947\n",
            "step: 3060, loss: 0.11836004257202148\n",
            "step: 3070, loss: 0.14384327828884125\n",
            "step: 3080, loss: 0.12618814408779144\n",
            "step: 3090, loss: 0.1365194171667099\n",
            "step: 3100, loss: 0.012750388123095036\n",
            "step: 3110, loss: 0.1443604677915573\n",
            "step: 3120, loss: 0.06342313438653946\n",
            "step: 3130, loss: 0.03502139449119568\n",
            "step: 3140, loss: 0.14426401257514954\n",
            "step: 3150, loss: 0.07367106527090073\n",
            "step: 3160, loss: 0.1018853485584259\n",
            "step: 3170, loss: 0.08699454367160797\n",
            "step: 3180, loss: 0.02160094864666462\n",
            "step: 3190, loss: 0.10293225944042206\n",
            "step: 3200, loss: 0.15493522584438324\n",
            "step: 3210, loss: 0.08065057545900345\n",
            "step: 3220, loss: 0.1734367311000824\n",
            "step: 3230, loss: 0.033601947128772736\n",
            "step: 3240, loss: 0.02987329475581646\n",
            "step: 3250, loss: 0.18326261639595032\n",
            "step: 3260, loss: 0.08525049686431885\n",
            "step: 3270, loss: 0.22346067428588867\n",
            "step: 3280, loss: 0.06161428615450859\n",
            "step: 3290, loss: 0.07546612620353699\n",
            "step: 3300, loss: 0.08351535350084305\n",
            "step: 3310, loss: 0.17587417364120483\n",
            "step: 3320, loss: 0.162441223859787\n",
            "step: 3330, loss: 0.03953779488801956\n",
            "step: 3340, loss: 0.19768422842025757\n",
            "step: 3350, loss: 0.14649176597595215\n",
            "step: 3360, loss: 0.06398443132638931\n",
            "step: 3370, loss: 0.06566374003887177\n",
            "step: 3380, loss: 0.20399412512779236\n",
            "step: 3390, loss: 0.1407674252986908\n",
            "step: 3400, loss: 0.05311856418848038\n",
            "step: 3410, loss: 0.0661681517958641\n",
            "step: 3420, loss: 0.031320586800575256\n",
            "step: 3430, loss: 0.043004110455513\n",
            "step: 3440, loss: 0.12532010674476624\n",
            "step: 3450, loss: 0.16126649081707\n",
            "step: 3460, loss: 0.08382157236337662\n",
            "step: 3470, loss: 0.06232833117246628\n",
            "step: 3480, loss: 0.09044460207223892\n",
            "step: 3490, loss: 0.0724993348121643\n",
            "step: 3500, loss: 0.12903764843940735\n",
            "step: 3510, loss: 0.1318059116601944\n",
            "step: 3520, loss: 0.07554571330547333\n",
            "step: 3530, loss: 0.06236381456255913\n",
            "step: 3540, loss: 0.11282891035079956\n",
            "step: 3550, loss: 0.08659487217664719\n",
            "step: 3560, loss: 0.12212971597909927\n",
            "step: 3570, loss: 0.11819397658109665\n",
            "step: 3580, loss: 0.19861248135566711\n",
            "step: 3590, loss: 0.08606956154108047\n",
            "step: 3600, loss: 0.056552283465862274\n",
            "step: 3610, loss: 0.09493587166070938\n",
            "step: 3620, loss: 0.1180708110332489\n",
            "step: 3630, loss: 0.06047786399722099\n",
            "step: 3640, loss: 0.16499567031860352\n",
            "step: 3650, loss: 0.1797633171081543\n",
            "step: 3660, loss: 0.019000835716724396\n",
            "step: 3670, loss: 0.06535303592681885\n",
            "step: 3680, loss: 0.1339988261461258\n",
            "step: 3690, loss: 0.18782825767993927\n",
            "step: 3700, loss: 0.09542666375637054\n",
            "step: 3710, loss: 0.10002979636192322\n",
            "step: 3720, loss: 0.13392232358455658\n",
            "step: 3730, loss: 0.0545223131775856\n",
            "step: 3740, loss: 0.19711250066757202\n",
            "step: 3750, loss: 0.16429120302200317\n",
            "step: 3760, loss: 0.011403245851397514\n",
            "step: 3770, loss: 0.05946727097034454\n",
            "step: 3780, loss: 0.15688399970531464\n",
            "step: 3790, loss: 0.049107737839221954\n",
            "step: 3800, loss: 0.09202220290899277\n",
            "step: 3810, loss: 0.16251257061958313\n",
            "step: 3820, loss: 0.0470617450773716\n",
            "step: 3830, loss: 0.11202122271060944\n",
            "step: 3840, loss: 0.056971631944179535\n",
            "step: 3850, loss: 0.0549689382314682\n",
            "step: 3860, loss: 0.15584512054920197\n",
            "step: 3870, loss: 0.07605617493391037\n",
            "step: 3880, loss: 0.06680984795093536\n",
            "step: 3890, loss: 0.1968400627374649\n",
            "step: 3900, loss: 0.012098551727831364\n",
            "step: 3910, loss: 0.23587201535701752\n",
            "step: 3920, loss: 0.18675847351551056\n",
            "step: 3930, loss: 0.0580562949180603\n",
            "step: 3940, loss: 0.046870965510606766\n",
            "step: 3950, loss: 0.06542255729436874\n",
            "step: 3960, loss: 0.32584086060523987\n",
            "step: 3970, loss: 0.01700076088309288\n",
            "step: 3980, loss: 0.10345157235860825\n",
            "step: 3990, loss: 0.13027015328407288\n",
            "step: 4000, loss: 0.11076794564723969\n",
            "step: 4010, loss: 0.10009564459323883\n",
            "step: 4020, loss: 0.06651920825242996\n",
            "step: 4030, loss: 0.04125434532761574\n",
            "step: 4040, loss: 0.043385401368141174\n",
            "step: 4050, loss: 0.04274063929915428\n",
            "step: 4060, loss: 0.08630405366420746\n",
            "step: 4070, loss: 0.020460376515984535\n",
            "step: 4080, loss: 0.1508440375328064\n",
            "step: 4090, loss: 0.19242657721042633\n",
            "step: 4100, loss: 0.08826665580272675\n",
            "step: 4110, loss: 0.05011498183012009\n",
            "step: 4120, loss: 0.143480122089386\n",
            "step: 4130, loss: 0.07449295371770859\n",
            "step: 4140, loss: 0.056257639080286026\n",
            "step: 4150, loss: 0.12688827514648438\n",
            "step: 4160, loss: 0.07098919153213501\n",
            "step: 4170, loss: 0.10631398111581802\n",
            "step: 4180, loss: 0.04679202288389206\n",
            "step: 4190, loss: 0.08739282935857773\n",
            "step: 4200, loss: 0.08527586609125137\n",
            "step: 4210, loss: 0.08520577847957611\n",
            "step: 4220, loss: 0.04456043988466263\n",
            "step: 4230, loss: 0.10477535426616669\n",
            "step: 4240, loss: 0.07938548177480698\n",
            "step: 4250, loss: 0.040134247392416\n",
            "step: 4260, loss: 0.1385851800441742\n",
            "step: 4270, loss: 0.03087720088660717\n",
            "step: 4280, loss: 0.041159696877002716\n",
            "step: 4290, loss: 0.09733434021472931\n",
            "step: 4300, loss: 0.052959490567445755\n",
            "step: 4310, loss: 0.17573592066764832\n",
            "step: 4320, loss: 0.1617310643196106\n",
            "step: 4330, loss: 0.1313304454088211\n",
            "step: 4340, loss: 0.1766708791255951\n",
            "step: 4350, loss: 0.03612205386161804\n",
            "step: 4360, loss: 0.15163815021514893\n",
            "step: 4370, loss: 0.09029117971658707\n",
            "step: 4380, loss: 0.1564810574054718\n",
            "step: 4390, loss: 0.11992643773555756\n",
            "step: 4400, loss: 0.26418018341064453\n",
            "step: 4410, loss: 0.012528667226433754\n",
            "step: 4420, loss: 0.03712649643421173\n",
            "step: 4430, loss: 0.05920867621898651\n",
            "step: 4440, loss: 0.09710231423377991\n",
            "step: 4450, loss: 0.10615070909261703\n",
            "step: 4460, loss: 0.031688131392002106\n",
            "step: 4470, loss: 0.03912961483001709\n",
            "step: 4480, loss: 0.1739550530910492\n",
            "step: 4490, loss: 0.09641369432210922\n",
            "step: 4500, loss: 0.049073170870542526\n",
            "step: 4510, loss: 0.08065509796142578\n",
            "step: 4520, loss: 0.1763540506362915\n",
            "step: 4530, loss: 0.32392022013664246\n",
            "step: 4540, loss: 0.2565136253833771\n",
            "step: 4550, loss: 0.2274727076292038\n",
            "step: 4560, loss: 0.19378036260604858\n",
            "step: 4570, loss: 0.04390743374824524\n",
            "step: 4580, loss: 0.2283371537923813\n",
            "step: 4590, loss: 0.12946714460849762\n",
            "step: 4600, loss: 0.04269065707921982\n",
            "step: 4610, loss: 0.01445787027478218\n",
            "step: 4620, loss: 0.03215761110186577\n",
            "step: 4630, loss: 0.07060562074184418\n",
            "step: 4640, loss: 0.0975361317396164\n",
            "step: 4650, loss: 0.05872600898146629\n",
            "step: 4660, loss: 0.060102421790361404\n",
            "step: 4670, loss: 0.014046747237443924\n",
            "step: 4680, loss: 0.4126994013786316\n",
            "step: 4690, loss: 0.0326470285654068\n",
            "step: 4700, loss: 0.0541696771979332\n",
            "step: 4710, loss: 0.08659546077251434\n",
            "step: 4720, loss: 0.06512412428855896\n",
            "step: 4730, loss: 0.07468923181295395\n",
            "step: 4740, loss: 0.13224969804286957\n",
            "step: 4750, loss: 0.07050106674432755\n",
            "step: 4760, loss: 0.03932610899209976\n",
            "step: 4770, loss: 0.14588847756385803\n",
            "step: 4780, loss: 0.1507822424173355\n",
            "step: 4790, loss: 0.13135898113250732\n",
            "step: 4800, loss: 0.065025694668293\n",
            "step: 4810, loss: 0.07100376486778259\n",
            "step: 4820, loss: 0.13816416263580322\n",
            "step: 4830, loss: 0.08855649083852768\n",
            "step: 4840, loss: 0.025065192952752113\n",
            "step: 4850, loss: 0.023137109354138374\n",
            "step: 4860, loss: 0.06408523768186569\n",
            "step: 4870, loss: 0.27328142523765564\n",
            "step: 4880, loss: 0.13139230012893677\n",
            "step: 4890, loss: 0.14838632941246033\n",
            "step: 4900, loss: 0.14449775218963623\n",
            "step: 4910, loss: 0.13570240139961243\n",
            "step: 4920, loss: 0.08590857684612274\n",
            "step: 4930, loss: 0.15975816547870636\n",
            "step: 4940, loss: 0.11353110522031784\n",
            "step: 4950, loss: 0.11510805040597916\n",
            "step: 4960, loss: 0.0681929737329483\n",
            "step: 4970, loss: 0.010511821135878563\n",
            "step: 4980, loss: 0.10083692520856857\n",
            "step: 4990, loss: 0.07134188711643219\n",
            "step: 5000, loss: 0.08983396738767624\n",
            "step: 5010, loss: 0.03123689629137516\n",
            "step: 5020, loss: 0.0998479351401329\n",
            "step: 5030, loss: 0.1842694878578186\n",
            "step: 5040, loss: 0.22665120661258698\n",
            "step: 5050, loss: 0.08907033503055573\n",
            "step: 5060, loss: 0.01409105584025383\n",
            "step: 5070, loss: 0.0105360746383667\n",
            "step: 5080, loss: 0.05886214226484299\n",
            "step: 5090, loss: 0.066569484770298\n",
            "step: 5100, loss: 0.08396238833665848\n",
            "step: 5110, loss: 0.07013550400733948\n",
            "step: 5120, loss: 0.09033048152923584\n",
            "step: 5130, loss: 0.17809747159481049\n",
            "step: 5140, loss: 0.06271475553512573\n",
            "step: 5150, loss: 0.08163083344697952\n",
            "step: 5160, loss: 0.05085732415318489\n",
            "step: 5170, loss: 0.06602747738361359\n",
            "step: 5180, loss: 0.05450742319226265\n",
            "step: 5190, loss: 0.07266024500131607\n",
            "step: 5200, loss: 0.029790842905640602\n",
            "step: 5210, loss: 0.11777248233556747\n",
            "step: 5220, loss: 0.16469091176986694\n",
            "step: 5230, loss: 0.10830098390579224\n",
            "step: 5240, loss: 0.04371678829193115\n",
            "step: 5250, loss: 0.09617064893245697\n",
            "step: 5260, loss: 0.1356675773859024\n",
            "step: 5270, loss: 0.0993005782365799\n",
            "step: 5280, loss: 0.05164200812578201\n",
            "step: 5290, loss: 0.062429338693618774\n",
            "step: 5300, loss: 0.11443351954221725\n",
            "step: 5310, loss: 0.042690470814704895\n",
            "step: 5320, loss: 0.06444384157657623\n",
            "step: 5330, loss: 0.07095994055271149\n",
            "step: 5340, loss: 0.07640844583511353\n",
            "step: 5350, loss: 0.1576436460018158\n",
            "step: 5360, loss: 0.02744433842599392\n",
            "step: 5370, loss: 0.09814610332250595\n",
            "step: 5380, loss: 0.05879644677042961\n",
            "step: 5390, loss: 0.02078045904636383\n",
            "step: 5400, loss: 0.12885302305221558\n",
            "step: 5410, loss: 0.053582146763801575\n",
            "step: 5420, loss: 0.08459928631782532\n",
            "step: 5430, loss: 0.10200682282447815\n",
            "step: 5440, loss: 0.05981672555208206\n",
            "step: 5450, loss: 0.1840032935142517\n",
            "step: 5460, loss: 0.10940737277269363\n",
            "step: 5470, loss: 0.024296600371599197\n",
            "step: 5480, loss: 0.2135031670331955\n",
            "step: 5490, loss: 0.12761874496936798\n",
            "step: 5500, loss: 0.02168155461549759\n",
            "step: 5510, loss: 0.1921321302652359\n",
            "step: 5520, loss: 0.2958412170410156\n",
            "step: 5530, loss: 0.028446663171052933\n",
            "step: 5540, loss: 0.050462186336517334\n",
            "step: 5550, loss: 0.13262510299682617\n",
            "step: 5560, loss: 0.176629900932312\n",
            "step: 5570, loss: 0.03759497031569481\n",
            "step: 5580, loss: 0.07479863613843918\n",
            "step: 5590, loss: 0.07357829064130783\n",
            "step: 5600, loss: 0.10919266939163208\n",
            "step: 5610, loss: 0.17758478224277496\n",
            "step: 5620, loss: 0.040938496589660645\n",
            "step: 5630, loss: 0.038319025188684464\n",
            "step: 5640, loss: 0.1701740175485611\n",
            "step: 5650, loss: 0.15519475936889648\n",
            "step: 5660, loss: 0.12257350981235504\n",
            "step: 5670, loss: 0.178768128156662\n",
            "step: 5680, loss: 0.1809043437242508\n",
            "step: 5690, loss: 0.09739931672811508\n",
            "step: 5700, loss: 0.048665352165699005\n",
            "step: 5710, loss: 0.04019232466816902\n",
            "step: 5720, loss: 0.07337773591279984\n",
            "step: 5730, loss: 0.04592432454228401\n",
            "step: 5740, loss: 0.03457387909293175\n",
            "step: 5750, loss: 0.12651726603507996\n",
            "step: 5760, loss: 0.12162232398986816\n",
            "step: 5770, loss: 0.094947449862957\n",
            "step: 5780, loss: 0.08088504523038864\n",
            "step: 5790, loss: 0.012056677602231503\n",
            "step: 5800, loss: 0.1302844136953354\n",
            "step: 5810, loss: 0.1139366626739502\n",
            "step: 5820, loss: 0.14662697911262512\n",
            "step: 5830, loss: 0.09023836255073547\n",
            "step: 5840, loss: 0.07391877472400665\n",
            "step: 5850, loss: 0.015098778530955315\n",
            "step: 5860, loss: 0.17002297937870026\n",
            "step: 5870, loss: 0.14418207108974457\n",
            "step: 5880, loss: 0.033000510185956955\n",
            "step: 5890, loss: 0.09815823286771774\n",
            "step: 5900, loss: 0.04504143446683884\n",
            "step: 5910, loss: 0.09306710213422775\n",
            "step: 5920, loss: 0.08511384576559067\n",
            "step: 5930, loss: 0.03412610664963722\n",
            "step: 5940, loss: 0.06943400204181671\n",
            "step: 5950, loss: 0.012832364067435265\n",
            "step: 5960, loss: 0.16030505299568176\n",
            "step: 5970, loss: 0.03156578168272972\n",
            "step: 5980, loss: 0.0551113486289978\n",
            "step: 5990, loss: 0.21217112243175507\n",
            "step: 6000, loss: 0.04348306357860565\n",
            "step: 6010, loss: 0.06778691709041595\n",
            "step: 6020, loss: 0.1755085587501526\n",
            "step: 6030, loss: 0.033357057720422745\n",
            "step: 6040, loss: 0.22184062004089355\n",
            "step: 6050, loss: 0.02987820655107498\n",
            "step: 6060, loss: 0.011654631234705448\n",
            "step: 6070, loss: 0.22114306688308716\n",
            "step: 6080, loss: 0.22436261177062988\n",
            "step: 6090, loss: 0.16726262867450714\n",
            "step: 6100, loss: 0.013956627808511257\n",
            "step: 6110, loss: 0.08911018818616867\n",
            "step: 6120, loss: 0.10583426803350449\n",
            "step: 6130, loss: 0.09781083464622498\n",
            "step: 6140, loss: 0.022231636568903923\n",
            "step: 6150, loss: 0.1499704122543335\n",
            "step: 6160, loss: 0.15971018373966217\n",
            "step: 6170, loss: 0.19069930911064148\n",
            "step: 6180, loss: 0.09265463054180145\n",
            "step: 6190, loss: 0.10877645760774612\n",
            "step: 6200, loss: 0.040419142693281174\n",
            "step: 6210, loss: 0.01119239442050457\n",
            "step: 6220, loss: 0.09345404803752899\n",
            "step: 6230, loss: 0.10102295875549316\n",
            "step: 6240, loss: 0.01836301013827324\n",
            "step: 6250, loss: 0.09219501912593842\n",
            "step: 6260, loss: 0.13664697110652924\n",
            "step: 6270, loss: 0.10645885765552521\n",
            "step: 6280, loss: 0.058938201516866684\n",
            "step: 6290, loss: 0.17967963218688965\n",
            "step: 6300, loss: 0.06400034576654434\n",
            "step: 6310, loss: 0.12006501853466034\n",
            "step: 6320, loss: 0.04605647176504135\n",
            "step: 6330, loss: 0.18568675220012665\n",
            "step: 6340, loss: 0.029483899474143982\n",
            "step: 6350, loss: 0.09197068959474564\n",
            "step: 6360, loss: 0.060977328568696976\n",
            "step: 6370, loss: 0.10905206948518753\n",
            "step: 6380, loss: 0.08376055955886841\n",
            "step: 6390, loss: 0.11430961638689041\n",
            "step: 6400, loss: 0.08308330178260803\n",
            "step: 6410, loss: 0.08286698907613754\n",
            "step: 6420, loss: 0.04562295973300934\n",
            "step: 6430, loss: 0.08673243969678879\n",
            "step: 6440, loss: 0.013044281862676144\n",
            "step: 6450, loss: 0.13530050218105316\n",
            "step: 6460, loss: 0.11367537826299667\n",
            "step: 6470, loss: 0.1638166755437851\n",
            "step: 6480, loss: 0.19023734331130981\n",
            "step: 6490, loss: 0.09173765033483505\n",
            "step: 6500, loss: 0.014603698626160622\n",
            "step: 6510, loss: 0.10534942895174026\n",
            "step: 6520, loss: 0.10313370823860168\n",
            "step: 6530, loss: 0.11785712093114853\n",
            "step: 6540, loss: 0.06316285580396652\n",
            "step: 6550, loss: 0.07806646823883057\n",
            "step: 6560, loss: 0.07254716753959656\n",
            "step: 6570, loss: 0.10065721720457077\n",
            "step: 6580, loss: 0.039678093045949936\n",
            "step: 6590, loss: 0.13681213557720184\n",
            "step: 6600, loss: 0.06153762340545654\n",
            "step: 6610, loss: 0.07148429751396179\n",
            "step: 6620, loss: 0.14028124511241913\n",
            "step: 6630, loss: 0.055745989084243774\n",
            "step: 6640, loss: 0.177455872297287\n",
            "step: 6650, loss: 0.04949403554201126\n",
            "step: 6660, loss: 0.1517600268125534\n",
            "step: 6670, loss: 0.043205346912145615\n",
            "step: 6680, loss: 0.1439894735813141\n",
            "step: 6690, loss: 0.012484182603657246\n",
            "step: 6700, loss: 0.13568061590194702\n",
            "step: 6710, loss: 0.2119733840227127\n",
            "step: 6720, loss: 0.1100512370467186\n",
            "step: 6730, loss: 0.05494464188814163\n",
            "step: 6740, loss: 0.09619073569774628\n",
            "step: 6750, loss: 0.206819549202919\n",
            "step: 6760, loss: 0.1615970879793167\n",
            "step: 6770, loss: 0.2564597427845001\n",
            "step: 6780, loss: 0.1148107573390007\n",
            "step: 6790, loss: 0.07282938063144684\n",
            "step: 6800, loss: 0.06486882269382477\n",
            "step: 6810, loss: 0.03945838287472725\n",
            "step: 6820, loss: 0.21519318222999573\n",
            "step: 6830, loss: 0.0937734916806221\n",
            "step: 6840, loss: 0.07354478538036346\n",
            "step: 6850, loss: 0.046358171850442886\n",
            "step: 6860, loss: 0.0959128886461258\n",
            "step: 6870, loss: 0.1639053374528885\n",
            "step: 6880, loss: 0.08479004353284836\n",
            "step: 6890, loss: 0.049969181418418884\n",
            "step: 6900, loss: 0.06639009714126587\n",
            "step: 6910, loss: 0.02242487668991089\n",
            "step: 6920, loss: 0.05390643700957298\n",
            "step: 6930, loss: 0.11524306982755661\n",
            "step: 6940, loss: 0.1343420296907425\n",
            "step: 6950, loss: 0.0830070972442627\n",
            "step: 6960, loss: 0.0980944111943245\n",
            "step: 6970, loss: 0.04200819134712219\n",
            "step: 6980, loss: 0.030146965757012367\n",
            "step: 6990, loss: 0.0822134017944336\n",
            "step: 7000, loss: 0.20433244109153748\n",
            "step: 7010, loss: 0.09821372479200363\n",
            "step: 7020, loss: 0.14744356274604797\n",
            "step: 7030, loss: 0.055408187210559845\n",
            "step: 7040, loss: 0.21417981386184692\n",
            "step: 7050, loss: 0.01441013440489769\n",
            "step: 7060, loss: 0.08969801664352417\n",
            "step: 7070, loss: 0.15625646710395813\n",
            "step: 7080, loss: 0.13377147912979126\n",
            "step: 7090, loss: 0.061801373958587646\n",
            "step: 7100, loss: 0.08145353943109512\n",
            "step: 7110, loss: 0.0501464307308197\n",
            "step: 7120, loss: 0.14002349972724915\n",
            "step: 7130, loss: 0.08734465390443802\n",
            "step: 7140, loss: 0.040089722722768784\n",
            "step: 7150, loss: 0.07907555252313614\n",
            "step: 7160, loss: 0.011236355639994144\n",
            "step: 7170, loss: 0.08366407454013824\n",
            "step: 7180, loss: 0.010632999241352081\n",
            "step: 7190, loss: 0.08103218674659729\n",
            "step: 7200, loss: 0.14623242616653442\n",
            "step: 7210, loss: 0.04180865362286568\n",
            "step: 7220, loss: 0.05853339284658432\n",
            "step: 7230, loss: 0.10289353132247925\n",
            "step: 7240, loss: 0.15942621231079102\n",
            "step: 7250, loss: 0.011934798210859299\n",
            "step: 7260, loss: 0.04224442318081856\n",
            "step: 7270, loss: 0.036338843405246735\n",
            "step: 7280, loss: 0.08910128474235535\n",
            "step: 7290, loss: 0.19974151253700256\n",
            "step: 7300, loss: 0.05996798723936081\n",
            "step: 7310, loss: 0.07418446242809296\n",
            "step: 7320, loss: 0.04174470528960228\n",
            "step: 7330, loss: 0.1884462833404541\n",
            "step: 7340, loss: 0.1156139150261879\n",
            "step: 7350, loss: 0.040542926639318466\n",
            "step: 7360, loss: 0.043280575424432755\n",
            "step: 7370, loss: 0.061416417360305786\n",
            "step: 7380, loss: 0.13370685279369354\n",
            "step: 7390, loss: 0.17592307925224304\n",
            "step: 7400, loss: 0.09984756261110306\n",
            "step: 7410, loss: 0.0965825542807579\n",
            "step: 7420, loss: 0.06496382504701614\n",
            "step: 7430, loss: 0.025992128998041153\n",
            "step: 7440, loss: 0.15794648230075836\n",
            "step: 7450, loss: 0.012767845764756203\n",
            "step: 7460, loss: 0.11554627120494843\n",
            "step: 7470, loss: 0.10136821866035461\n",
            "step: 7480, loss: 0.07798150926828384\n",
            "step: 7490, loss: 0.013226708397269249\n",
            "step: 7500, loss: 0.07526250928640366\n",
            "step: 7510, loss: 0.19705452024936676\n",
            "step: 7520, loss: 0.0976174995303154\n",
            "step: 7530, loss: 0.09861605614423752\n",
            "step: 7540, loss: 0.07641749083995819\n",
            "step: 7550, loss: 0.05683491751551628\n",
            "step: 7560, loss: 0.05646030232310295\n",
            "step: 7570, loss: 0.07487837970256805\n",
            "step: 7580, loss: 0.06514300405979156\n",
            "step: 7590, loss: 0.01311725564301014\n",
            "step: 7600, loss: 0.059542734175920486\n",
            "step: 7610, loss: 0.12107086926698685\n",
            "step: 7620, loss: 0.07492204755544662\n",
            "step: 7630, loss: 0.13353370130062103\n",
            "step: 7640, loss: 0.07543481141328812\n",
            "step: 7650, loss: 0.13922767341136932\n",
            "step: 7660, loss: 0.0994863286614418\n",
            "step: 7670, loss: 0.05907787010073662\n",
            "step: 7680, loss: 0.16434574127197266\n",
            "step: 7690, loss: 0.04097968339920044\n",
            "step: 7700, loss: 0.1058138757944107\n",
            "step: 7710, loss: 0.07046213746070862\n",
            "step: 7720, loss: 0.015282665379345417\n",
            "step: 7730, loss: 0.0723288357257843\n",
            "step: 7740, loss: 0.1481887847185135\n",
            "step: 7750, loss: 0.05734337121248245\n",
            "step: 7760, loss: 0.028453828766942024\n",
            "step: 7770, loss: 0.02104310318827629\n",
            "step: 7780, loss: 0.08908354490995407\n",
            "step: 7790, loss: 0.16689331829547882\n",
            "step: 7800, loss: 0.14503009617328644\n",
            "step: 7810, loss: 0.11222634464502335\n",
            "step: 7820, loss: 0.20969633758068085\n",
            "step: 7830, loss: 0.14962483942508698\n",
            "step: 7840, loss: 0.13444587588310242\n",
            "step: 7850, loss: 0.05076850950717926\n",
            "step: 7860, loss: 0.1603962630033493\n",
            "step: 7870, loss: 0.10367568582296371\n",
            "step: 7880, loss: 0.05634129047393799\n",
            "step: 7890, loss: 0.08289521187543869\n",
            "step: 7900, loss: 0.08917706459760666\n",
            "step: 7910, loss: 0.028607923537492752\n",
            "step: 7920, loss: 0.05101625993847847\n",
            "step: 7930, loss: 0.27126288414001465\n",
            "step: 7940, loss: 0.07518845051527023\n",
            "step: 7950, loss: 0.12352152913808823\n",
            "step: 7960, loss: 0.032373812049627304\n",
            "step: 7970, loss: 0.12525828182697296\n",
            "step: 7980, loss: 0.05222146958112717\n",
            "step: 7990, loss: 0.14542369544506073\n",
            "step: 8000, loss: 0.10289162397384644\n",
            "step: 8010, loss: 0.1062280610203743\n",
            "step: 8020, loss: 0.10960976034402847\n",
            "step: 8030, loss: 0.10776900500059128\n",
            "step: 8040, loss: 0.01291927881538868\n",
            "step: 8050, loss: 0.014316593296825886\n",
            "step: 8060, loss: 0.09513958543539047\n",
            "step: 8070, loss: 0.08697732537984848\n",
            "step: 8080, loss: 0.07818545401096344\n",
            "step: 8090, loss: 0.10646507889032364\n",
            "step: 8100, loss: 0.10520504415035248\n",
            "step: 8110, loss: 0.04884902015328407\n",
            "step: 8120, loss: 0.05187061429023743\n",
            "step: 8130, loss: 0.09107200801372528\n",
            "step: 8140, loss: 0.24374596774578094\n",
            "step: 8150, loss: 0.2270020842552185\n",
            "step: 8160, loss: 0.10377172380685806\n",
            "step: 8170, loss: 0.112018883228302\n",
            "step: 8180, loss: 0.06001179665327072\n",
            "step: 8190, loss: 0.16894374787807465\n",
            "step: 8200, loss: 0.07106044143438339\n",
            "step: 8210, loss: 0.15301892161369324\n",
            "step: 8220, loss: 0.07005137205123901\n",
            "step: 8230, loss: 0.06311479210853577\n",
            "step: 8240, loss: 0.12155388295650482\n",
            "step: 8250, loss: 0.10649962723255157\n",
            "step: 8260, loss: 0.07704807072877884\n",
            "step: 8270, loss: 0.16717126965522766\n",
            "step: 8280, loss: 0.17885150015354156\n",
            "step: 8290, loss: 0.09009001404047012\n",
            "step: 8300, loss: 0.1125490665435791\n",
            "step: 8310, loss: 0.01361283753067255\n",
            "step: 8320, loss: 0.024126581847667694\n",
            "step: 8330, loss: 0.10143667459487915\n",
            "step: 8340, loss: 0.06727460026741028\n",
            "step: 8350, loss: 0.07084869593381882\n",
            "step: 8360, loss: 0.06604284048080444\n",
            "step: 8370, loss: 0.031368497759103775\n",
            "step: 8380, loss: 0.07088042050600052\n",
            "step: 8390, loss: 0.030629359185695648\n",
            "step: 8400, loss: 0.09558635205030441\n",
            "step: 8410, loss: 0.13142292201519012\n",
            "step: 8420, loss: 0.048412423580884933\n",
            "step: 8430, loss: 0.17698583006858826\n",
            "step: 8440, loss: 0.03161261975765228\n",
            "step: 8450, loss: 0.03867994621396065\n",
            "step: 8460, loss: 0.1579536348581314\n",
            "step: 8470, loss: 0.02954360842704773\n",
            "step: 8480, loss: 0.07503059506416321\n",
            "step: 8490, loss: 0.09971791505813599\n",
            "step: 8500, loss: 0.06560982763767242\n",
            "step: 8510, loss: 0.14804784953594208\n",
            "step: 8520, loss: 0.14838147163391113\n",
            "step: 8530, loss: 0.07579459249973297\n",
            "step: 8540, loss: 0.14288143813610077\n",
            "step: 8550, loss: 0.05502949655056\n",
            "step: 8560, loss: 0.26025667786598206\n",
            "step: 8570, loss: 0.07630319148302078\n",
            "step: 8580, loss: 0.056023094803094864\n",
            "step: 8590, loss: 0.11647364497184753\n",
            "step: 8600, loss: 0.17364533245563507\n",
            "step: 8610, loss: 0.11268272250890732\n",
            "step: 8620, loss: 0.08548463135957718\n",
            "step: 8630, loss: 0.1466393619775772\n",
            "step: 8640, loss: 0.10468608140945435\n",
            "step: 8650, loss: 0.21253159642219543\n",
            "step: 8660, loss: 0.12038952857255936\n",
            "step: 8670, loss: 0.12215512245893478\n",
            "step: 8680, loss: 0.16016289591789246\n",
            "step: 8690, loss: 0.07574650645256042\n",
            "step: 8700, loss: 0.12585307657718658\n",
            "step: 8710, loss: 0.01284660492092371\n",
            "step: 8720, loss: 0.08741383254528046\n",
            "step: 8730, loss: 0.08718454092741013\n",
            "step: 8740, loss: 0.10817936807870865\n",
            "step: 8750, loss: 0.01579713076353073\n",
            "step: 8760, loss: 0.013909978792071342\n",
            "step: 8770, loss: 0.16454294323921204\n",
            "step: 8780, loss: 0.21844753623008728\n",
            "step: 8790, loss: 0.029484707862138748\n",
            "step: 8800, loss: 0.12113839387893677\n",
            "step: 8810, loss: 0.06929369270801544\n",
            "step: 8820, loss: 0.06803996860980988\n",
            "step: 8830, loss: 0.1383461207151413\n",
            "step: 8840, loss: 0.04945049434900284\n",
            "step: 8850, loss: 0.06929007917642593\n",
            "step: 8860, loss: 0.07062358409166336\n",
            "step: 8870, loss: 0.09313328564167023\n",
            "step: 8880, loss: 0.11380161345005035\n",
            "step: 8890, loss: 0.01267173234373331\n",
            "step: 8900, loss: 0.03428775444626808\n",
            "step: 8910, loss: 0.062003616243600845\n",
            "step: 8920, loss: 0.10163300484418869\n",
            "step: 8930, loss: 0.062423110008239746\n",
            "step: 8940, loss: 0.06889475136995316\n",
            "step: 8950, loss: 0.029058631509542465\n",
            "step: 8960, loss: 0.1254051923751831\n",
            "step: 8970, loss: 0.054966967552900314\n",
            "step: 8980, loss: 0.06595112383365631\n",
            "step: 8990, loss: 0.17680522799491882\n",
            "step: 9000, loss: 0.104703389108181\n",
            "step: 9010, loss: 0.15886162221431732\n",
            "step: 9020, loss: 0.09758716821670532\n",
            "step: 9030, loss: 0.020401187241077423\n",
            "step: 9040, loss: 0.030496755614876747\n",
            "step: 9050, loss: 0.173349529504776\n",
            "step: 9060, loss: 0.10253792256116867\n",
            "step: 9070, loss: 0.01877240091562271\n",
            "step: 9080, loss: 0.16593097150325775\n",
            "step: 9090, loss: 0.1192152127623558\n",
            "step: 9100, loss: 0.1033545657992363\n",
            "step: 9110, loss: 0.14607737958431244\n",
            "step: 9120, loss: 0.06926200538873672\n",
            "step: 9130, loss: 0.11354207247495651\n",
            "step: 9140, loss: 0.0473334901034832\n",
            "step: 9150, loss: 0.03545139357447624\n",
            "step: 9160, loss: 0.06136004626750946\n",
            "step: 9170, loss: 0.057507000863552094\n",
            "step: 9180, loss: 0.2080017477273941\n",
            "step: 9190, loss: 0.04759058356285095\n",
            "step: 9200, loss: 0.03910859674215317\n",
            "step: 9210, loss: 0.08630284667015076\n",
            "step: 9220, loss: 0.2763582766056061\n",
            "step: 9230, loss: 0.2281581312417984\n",
            "step: 9240, loss: 0.057693205773830414\n",
            "step: 9250, loss: 0.19664177298545837\n",
            "step: 9260, loss: 0.029099976643919945\n",
            "step: 9270, loss: 0.10432180017232895\n",
            "step: 9280, loss: 0.08900606632232666\n",
            "step: 9290, loss: 0.011814283207058907\n",
            "step: 9300, loss: 0.08166591078042984\n",
            "step: 9310, loss: 0.05475272983312607\n",
            "step: 9320, loss: 0.07232885807752609\n",
            "step: 9330, loss: 0.05159389227628708\n",
            "step: 9340, loss: 0.04498078674077988\n",
            "step: 9350, loss: 0.08774542808532715\n",
            "step: 9360, loss: 0.029273752123117447\n",
            "step: 9370, loss: 0.11261697113513947\n",
            "step: 9380, loss: 0.11325986683368683\n",
            "step: 9390, loss: 0.16645535826683044\n",
            "step: 9400, loss: 0.174230694770813\n",
            "step: 9410, loss: 0.014888117089867592\n",
            "step: 9420, loss: 0.14272169768810272\n",
            "step: 9430, loss: 0.025441214442253113\n",
            "step: 9440, loss: 0.1788197010755539\n",
            "step: 9450, loss: 0.15698963403701782\n",
            "step: 9460, loss: 0.04535117372870445\n",
            "step: 9470, loss: 0.04382472485303879\n",
            "step: 9480, loss: 0.011589412577450275\n",
            "step: 9490, loss: 0.06876523792743683\n",
            "step: 9500, loss: 0.0717460960149765\n",
            "step: 9510, loss: 0.04390661418437958\n",
            "step: 9520, loss: 0.1225050836801529\n",
            "step: 9530, loss: 0.0422251932322979\n",
            "step: 9540, loss: 0.03826574981212616\n",
            "step: 9550, loss: 0.08914943784475327\n",
            "step: 9560, loss: 0.0324663482606411\n",
            "step: 9570, loss: 0.07894780486822128\n",
            "step: 9580, loss: 0.07191547751426697\n",
            "step: 9590, loss: 0.2556372582912445\n",
            "step: 9600, loss: 0.05264975503087044\n",
            "step: 9610, loss: 0.10642723739147186\n",
            "step: 9620, loss: 0.07547419518232346\n",
            "step: 9630, loss: 0.07849486172199249\n",
            "step: 9640, loss: 0.193936288356781\n",
            "step: 9650, loss: 0.013200421817600727\n",
            "step: 9660, loss: 0.11342274397611618\n",
            "step: 9670, loss: 0.059791211038827896\n",
            "step: 9680, loss: 0.04605898633599281\n",
            "step: 9690, loss: 0.1380014568567276\n",
            "step: 9700, loss: 0.07951661944389343\n",
            "step: 9710, loss: 0.05051400512456894\n",
            "step: 9720, loss: 0.0787864476442337\n",
            "step: 9730, loss: 0.01438025664538145\n",
            "step: 9740, loss: 0.1865825355052948\n",
            "step: 9750, loss: 0.10725318640470505\n",
            "step: 9760, loss: 0.24076932668685913\n",
            "step: 9770, loss: 0.04596151411533356\n",
            "step: 9780, loss: 0.058753494173288345\n",
            "step: 9790, loss: 0.09647800028324127\n",
            "step: 9800, loss: 0.04009290784597397\n",
            "step: 9810, loss: 0.06900745630264282\n",
            "step: 9820, loss: 0.07201417535543442\n",
            "step: 9830, loss: 0.1777477264404297\n",
            "step: 9840, loss: 0.1609429568052292\n",
            "step: 9850, loss: 0.015114940702915192\n",
            "step: 9860, loss: 0.23861218988895416\n",
            "step: 9870, loss: 0.040086664259433746\n",
            "step: 9880, loss: 0.11691345274448395\n",
            "step: 9890, loss: 0.05197941139340401\n",
            "step: 9900, loss: 0.04822345823049545\n",
            "step: 9910, loss: 0.05220865085721016\n",
            "step: 9920, loss: 0.08421611785888672\n",
            "step: 9930, loss: 0.0705220028758049\n",
            "step: 9940, loss: 0.08188619464635849\n",
            "step: 9950, loss: 0.11958203464746475\n",
            "step: 9960, loss: 0.07228653877973557\n",
            "step: 9970, loss: 0.029699932783842087\n",
            "step: 9980, loss: 0.15568041801452637\n",
            "step: 9990, loss: 0.011894110590219498\n",
            "step: 10000, loss: 0.039076149463653564\n",
            "step: 10010, loss: 0.02752789296209812\n",
            "step: 10020, loss: 0.0977163165807724\n",
            "step: 10030, loss: 0.06843415647745132\n",
            "step: 10040, loss: 0.09828976541757584\n",
            "step: 10050, loss: 0.07614036649465561\n",
            "step: 10060, loss: 0.04411756992340088\n",
            "step: 10070, loss: 0.131154403090477\n",
            "step: 10080, loss: 0.29331350326538086\n",
            "step: 10090, loss: 0.013089683838188648\n",
            "step: 10100, loss: 0.1441240906715393\n",
            "step: 10110, loss: 0.023525824770331383\n",
            "step: 10120, loss: 0.09472716599702835\n",
            "step: 10130, loss: 0.13066866993904114\n",
            "step: 10140, loss: 0.042639851570129395\n",
            "step: 10150, loss: 0.19685567915439606\n",
            "step: 10160, loss: 0.0977938324213028\n",
            "step: 10170, loss: 0.01476721465587616\n",
            "step: 10180, loss: 0.0746055543422699\n",
            "step: 10190, loss: 0.148602694272995\n",
            "step: 10200, loss: 0.12087119370698929\n",
            "step: 10210, loss: 0.10904654860496521\n",
            "step: 10220, loss: 0.08434924483299255\n",
            "step: 10230, loss: 0.13527421653270721\n",
            "step: 10240, loss: 0.16492271423339844\n",
            "step: 10250, loss: 0.1029713824391365\n",
            "step: 10260, loss: 0.11335143446922302\n",
            "step: 10270, loss: 0.06861817091703415\n",
            "step: 10280, loss: 0.22582919895648956\n",
            "step: 10290, loss: 0.10198245197534561\n",
            "step: 10300, loss: 0.10757051408290863\n",
            "step: 10310, loss: 0.08075977116823196\n",
            "step: 10320, loss: 0.021370194852352142\n",
            "step: 10330, loss: 0.06872984766960144\n",
            "step: 10340, loss: 0.09407144784927368\n",
            "step: 10350, loss: 0.01451034378260374\n",
            "step: 10360, loss: 0.08888169378042221\n",
            "step: 10370, loss: 0.14984041452407837\n",
            "step: 10380, loss: 0.112693652510643\n",
            "step: 10390, loss: 0.07260595262050629\n",
            "step: 10400, loss: 0.23188574612140656\n",
            "step: 10410, loss: 0.09198176860809326\n",
            "step: 10420, loss: 0.07910268753767014\n",
            "step: 10430, loss: 0.10511672496795654\n",
            "step: 10440, loss: 0.07906370609998703\n",
            "step: 10450, loss: 0.05819259583950043\n",
            "step: 10460, loss: 0.10741769522428513\n",
            "step: 10470, loss: 0.12357652187347412\n",
            "step: 10480, loss: 0.09403489530086517\n",
            "step: 10490, loss: 0.06760412454605103\n",
            "step: 10500, loss: 0.22988051176071167\n",
            "step: 10510, loss: 0.12534798681735992\n",
            "step: 10520, loss: 0.057012513279914856\n",
            "step: 10530, loss: 0.1080605760216713\n",
            "step: 10540, loss: 0.12723025679588318\n",
            "step: 10550, loss: 0.137510746717453\n",
            "step: 10560, loss: 0.0755702555179596\n",
            "step: 10570, loss: 0.10678701102733612\n",
            "step: 10580, loss: 0.07061013579368591\n",
            "step: 10590, loss: 0.05877883359789848\n",
            "step: 10600, loss: 0.009808596223592758\n",
            "step: 10610, loss: 0.2407703995704651\n",
            "step: 10620, loss: 0.03591954708099365\n",
            "step: 10630, loss: 0.09508860111236572\n",
            "step: 10640, loss: 0.12461253255605698\n",
            "step: 10650, loss: 0.012611212208867073\n",
            "step: 10660, loss: 0.1855379045009613\n",
            "step: 10670, loss: 0.04135863855481148\n",
            "step: 10680, loss: 0.09414608776569366\n",
            "step: 10690, loss: 0.0247527826577425\n",
            "step: 10700, loss: 0.10396292060613632\n",
            "step: 10710, loss: 0.049023646861314774\n",
            "step: 10720, loss: 0.11583851277828217\n",
            "step: 10730, loss: 0.07354479283094406\n",
            "step: 10740, loss: 0.033061619848012924\n",
            "step: 10750, loss: 0.05438319593667984\n",
            "step: 10760, loss: 0.07706877589225769\n",
            "step: 10770, loss: 0.030534354969859123\n",
            "step: 10780, loss: 0.059915170073509216\n",
            "step: 10790, loss: 0.10541845858097076\n",
            "step: 10800, loss: 0.042796846479177475\n",
            "step: 10810, loss: 0.17209433019161224\n",
            "step: 10820, loss: 0.17445021867752075\n",
            "step: 10830, loss: 0.09992143511772156\n",
            "step: 10840, loss: 0.14019793272018433\n",
            "step: 10850, loss: 0.13015468418598175\n",
            "step: 10860, loss: 0.08002298325300217\n",
            "step: 10870, loss: 0.06736129522323608\n",
            "step: 10880, loss: 0.09853775799274445\n",
            "step: 10890, loss: 0.06611865013837814\n",
            "step: 10900, loss: 0.13613909482955933\n",
            "step: 10910, loss: 0.04799564555287361\n",
            "step: 10920, loss: 0.09871756285429001\n",
            "step: 10930, loss: 0.039050981402397156\n",
            "step: 10940, loss: 0.052775122225284576\n",
            "step: 10950, loss: 0.21487128734588623\n",
            "step: 10960, loss: 0.11908819526433945\n",
            "step: 10970, loss: 0.183868408203125\n",
            "step: 10980, loss: 0.0626169815659523\n",
            "step: 10990, loss: 0.0808795914053917\n",
            "step: 11000, loss: 0.1404963582754135\n",
            "step: 11010, loss: 0.11812160909175873\n",
            "step: 11020, loss: 0.02286508120596409\n",
            "step: 11030, loss: 0.09724495559930801\n",
            "step: 11040, loss: 0.061715301126241684\n",
            "step: 11050, loss: 0.1265074610710144\n",
            "step: 11060, loss: 0.01167791523039341\n",
            "step: 11070, loss: 0.11247063428163528\n",
            "step: 11080, loss: 0.07916675508022308\n",
            "step: 11090, loss: 0.10723388195037842\n",
            "step: 11100, loss: 0.010410906746983528\n",
            "step: 11110, loss: 0.05345914885401726\n",
            "step: 11120, loss: 0.07312484085559845\n",
            "step: 11130, loss: 0.456976056098938\n",
            "step: 11140, loss: 0.16010336577892303\n",
            "step: 11150, loss: 0.08251817524433136\n",
            "step: 11160, loss: 0.09756920486688614\n",
            "step: 11170, loss: 0.014658360742032528\n",
            "step: 11180, loss: 0.03145597502589226\n",
            "step: 11190, loss: 0.06414677947759628\n",
            "step: 11200, loss: 0.03338546305894852\n",
            "step: 11210, loss: 0.21428513526916504\n",
            "step: 11220, loss: 0.09873637557029724\n",
            "step: 11230, loss: 0.07478578388690948\n",
            "step: 11240, loss: 0.04695471376180649\n",
            "step: 11250, loss: 0.061655037105083466\n",
            "step: 11260, loss: 0.014522787183523178\n",
            "step: 11270, loss: 0.022420642897486687\n",
            "step: 11280, loss: 0.011894595809280872\n",
            "step: 11290, loss: 0.08854157477617264\n",
            "step: 11300, loss: 0.170431986451149\n",
            "step: 11310, loss: 0.030755365267395973\n",
            "step: 11320, loss: 0.02078953944146633\n",
            "step: 11330, loss: 0.07735159993171692\n",
            "step: 11340, loss: 0.12255974113941193\n",
            "step: 11350, loss: 0.13976284861564636\n",
            "step: 11360, loss: 0.11277951300144196\n",
            "step: 11370, loss: 0.01535320095717907\n",
            "step: 11380, loss: 0.25166431069374084\n",
            "step: 11390, loss: 0.015772216022014618\n",
            "step: 11400, loss: 0.04472871124744415\n",
            "step: 11410, loss: 0.04830619692802429\n",
            "step: 11420, loss: 0.08908293396234512\n",
            "step: 11430, loss: 0.1408894658088684\n",
            "step: 11440, loss: 0.1469990611076355\n",
            "step: 11450, loss: 0.22559282183647156\n",
            "step: 11460, loss: 0.026928726583719254\n",
            "step: 11470, loss: 0.07118174433708191\n",
            "step: 11480, loss: 0.10925424844026566\n",
            "step: 11490, loss: 0.13434931635856628\n",
            "step: 11500, loss: 0.16793783009052277\n",
            "step: 11510, loss: 0.1384645700454712\n",
            "step: 11520, loss: 0.0935455858707428\n",
            "step: 11530, loss: 0.15682706236839294\n",
            "step: 11540, loss: 0.15923938155174255\n",
            "step: 11550, loss: 0.17746485769748688\n",
            "step: 11560, loss: 0.06187907233834267\n",
            "step: 11570, loss: 0.033351100981235504\n",
            "step: 11580, loss: 0.0900288000702858\n",
            "step: 11590, loss: 0.013491997495293617\n",
            "step: 11600, loss: 0.06552944332361221\n",
            "step: 11610, loss: 0.13161902129650116\n",
            "step: 11620, loss: 0.12574943900108337\n",
            "step: 11630, loss: 0.0476149283349514\n",
            "step: 11640, loss: 0.06086145341396332\n",
            "step: 11650, loss: 0.15733155608177185\n",
            "step: 11660, loss: 0.10184270143508911\n",
            "step: 11670, loss: 0.0753084197640419\n",
            "step: 11680, loss: 0.1360255777835846\n",
            "step: 11690, loss: 0.11951710283756256\n",
            "step: 11700, loss: 0.1494251936674118\n",
            "step: 11710, loss: 0.050212495028972626\n",
            "step: 11720, loss: 0.1489657610654831\n",
            "step: 11730, loss: 0.05328081175684929\n",
            "step: 11740, loss: 0.17457231879234314\n",
            "step: 11750, loss: 0.012404102832078934\n",
            "step: 11760, loss: 0.10790824145078659\n",
            "step: 11770, loss: 0.15559478104114532\n",
            "step: 11780, loss: 0.14894801378250122\n",
            "step: 11790, loss: 0.10257276147603989\n",
            "step: 11800, loss: 0.33142539858818054\n",
            "step: 11810, loss: 0.14551277458667755\n",
            "step: 11820, loss: 0.03340709209442139\n",
            "step: 11830, loss: 0.01620413362979889\n",
            "step: 11840, loss: 0.15185821056365967\n",
            "step: 11850, loss: 0.013781456276774406\n",
            "step: 11860, loss: 0.05843105912208557\n",
            "step: 11870, loss: 0.19459426403045654\n",
            "step: 11880, loss: 0.04539811611175537\n",
            "step: 11890, loss: 0.13203220069408417\n",
            "step: 11900, loss: 0.27568763494491577\n",
            "step: 11910, loss: 0.13326731324195862\n",
            "step: 11920, loss: 0.03125451132655144\n",
            "step: 11930, loss: 0.02357507310807705\n",
            "step: 11940, loss: 0.050839297473430634\n",
            "step: 11950, loss: 0.09637334942817688\n",
            "step: 11960, loss: 0.16614295542240143\n",
            "step: 11970, loss: 0.11477814614772797\n",
            "step: 11980, loss: 0.0772235244512558\n",
            "step: 11990, loss: 0.045816414058208466\n",
            "step: 12000, loss: 0.14412277936935425\n",
            "step: 12010, loss: 0.12669573724269867\n",
            "step: 12020, loss: 0.21189971268177032\n",
            "step: 12030, loss: 0.06552012264728546\n",
            "step: 12040, loss: 0.10871201008558273\n",
            "step: 12050, loss: 0.07235054671764374\n",
            "step: 12060, loss: 0.12232621759176254\n",
            "step: 12070, loss: 0.06961052864789963\n",
            "step: 12080, loss: 0.04019101336598396\n",
            "step: 12090, loss: 0.1233125627040863\n",
            "step: 12100, loss: 0.13777676224708557\n",
            "step: 12110, loss: 0.12882164120674133\n",
            "step: 12120, loss: 0.09577066451311111\n",
            "step: 12130, loss: 0.19190074503421783\n",
            "step: 12140, loss: 0.07372653484344482\n",
            "step: 12150, loss: 0.12080089747905731\n",
            "step: 12160, loss: 0.194298654794693\n",
            "step: 12170, loss: 0.10185940563678741\n",
            "step: 12180, loss: 0.07508890330791473\n",
            "step: 12190, loss: 0.06655561923980713\n",
            "step: 12200, loss: 0.15629315376281738\n",
            "step: 12210, loss: 0.10603785514831543\n",
            "step: 12220, loss: 0.11237666010856628\n",
            "step: 12230, loss: 0.1443236768245697\n",
            "step: 12240, loss: 0.12090971320867538\n",
            "step: 12250, loss: 0.07190226763486862\n",
            "step: 12260, loss: 0.012245984748005867\n",
            "step: 12270, loss: 0.1573370099067688\n",
            "step: 12280, loss: 0.20660826563835144\n",
            "step: 12290, loss: 0.11080095171928406\n",
            "step: 12300, loss: 0.14344507455825806\n",
            "step: 12310, loss: 0.2288641333580017\n",
            "step: 12320, loss: 0.19469909369945526\n",
            "step: 12330, loss: 0.05592529848217964\n",
            "step: 12340, loss: 0.08797242492437363\n",
            "step: 12350, loss: 0.17114196717739105\n",
            "step: 12360, loss: 0.10330849140882492\n",
            "step: 12370, loss: 0.09028449654579163\n",
            "step: 12380, loss: 0.1264340728521347\n",
            "step: 12390, loss: 0.11937487870454788\n",
            "step: 12400, loss: 0.09471507370471954\n",
            "step: 12410, loss: 0.09640103578567505\n",
            "step: 12420, loss: 0.11395995318889618\n",
            "step: 12430, loss: 0.0377250537276268\n",
            "step: 12440, loss: 0.11885815858840942\n",
            "step: 12450, loss: 0.11422395706176758\n",
            "step: 12460, loss: 0.03758041560649872\n",
            "step: 12470, loss: 0.08119584619998932\n",
            "step: 12480, loss: 0.15596908330917358\n",
            "step: 12490, loss: 0.051485780626535416\n",
            "step: 12500, loss: 0.060927148908376694\n",
            "step: 12510, loss: 0.07011699676513672\n",
            "step: 12520, loss: 0.15016555786132812\n",
            "step: 12530, loss: 0.033396411687135696\n",
            "step: 12540, loss: 0.07432370632886887\n",
            "step: 12550, loss: 0.03364421799778938\n",
            "step: 12560, loss: 0.05017003417015076\n",
            "step: 12570, loss: 0.04177388921380043\n",
            "step: 12580, loss: 0.09524570405483246\n",
            "step: 12590, loss: 0.09736065566539764\n",
            "step: 12600, loss: 0.18581441044807434\n",
            "step: 12610, loss: 0.051567111164331436\n",
            "step: 12620, loss: 0.06987684220075607\n",
            "step: 12630, loss: 0.19044813513755798\n",
            "step: 12640, loss: 0.062041275203228\n",
            "step: 12650, loss: 0.07654913514852524\n",
            "step: 12660, loss: 0.03682432323694229\n",
            "step: 12670, loss: 0.14075413346290588\n",
            "step: 12680, loss: 0.09070993959903717\n",
            "step: 12690, loss: 0.0722830519080162\n",
            "step: 12700, loss: 0.07777004688978195\n",
            "step: 12710, loss: 0.10815272480249405\n",
            "step: 12720, loss: 0.11657334864139557\n",
            "step: 12730, loss: 0.1093263030052185\n",
            "step: 12740, loss: 0.07639370858669281\n",
            "step: 12750, loss: 0.09961937367916107\n",
            "step: 12760, loss: 0.13208085298538208\n",
            "step: 12770, loss: 0.1936863660812378\n",
            "step: 12780, loss: 0.05538945645093918\n",
            "step: 12790, loss: 0.04791313409805298\n",
            "step: 12800, loss: 0.11337794363498688\n",
            "step: 12810, loss: 0.10043556243181229\n",
            "step: 12820, loss: 0.040697403252124786\n",
            "step: 12830, loss: 0.1542091965675354\n",
            "step: 12840, loss: 0.06364008784294128\n",
            "step: 12850, loss: 0.04274054616689682\n",
            "step: 12860, loss: 0.06224745884537697\n",
            "step: 12870, loss: 0.0652848482131958\n",
            "step: 12880, loss: 0.13299940526485443\n",
            "step: 12890, loss: 0.01129916775971651\n",
            "step: 12900, loss: 0.07254301756620407\n",
            "step: 12910, loss: 0.07194723933935165\n",
            "step: 12920, loss: 0.1490514725446701\n",
            "step: 12930, loss: 0.014137940481305122\n",
            "step: 12940, loss: 0.09808073937892914\n",
            "step: 12950, loss: 0.05803195387125015\n",
            "step: 12960, loss: 0.010113119147717953\n",
            "step: 12970, loss: 0.07119622826576233\n",
            "step: 12980, loss: 0.06543899327516556\n",
            "step: 12990, loss: 0.0732196792960167\n",
            "step: 13000, loss: 0.03631389141082764\n",
            "step: 13010, loss: 0.0634128674864769\n",
            "step: 13020, loss: 0.11618326604366302\n",
            "step: 13030, loss: 0.07797664403915405\n",
            "step: 13040, loss: 0.07594995945692062\n",
            "step: 13050, loss: 0.12535372376441956\n",
            "step: 13060, loss: 0.06315694749355316\n",
            "step: 13070, loss: 0.11248155683279037\n",
            "step: 13080, loss: 0.048017144203186035\n",
            "step: 13090, loss: 0.04453803226351738\n",
            "step: 13100, loss: 0.0815216600894928\n",
            "step: 13110, loss: 0.14567887783050537\n",
            "step: 13120, loss: 0.05739312246441841\n",
            "step: 13130, loss: 0.15190023183822632\n",
            "step: 13140, loss: 0.09194599092006683\n",
            "step: 13150, loss: 0.06519763916730881\n",
            "step: 13160, loss: 0.09411931037902832\n",
            "step: 13170, loss: 0.10114195197820663\n",
            "step: 13180, loss: 0.06501609086990356\n",
            "step: 13190, loss: 0.1750994771718979\n",
            "step: 13200, loss: 0.06674475967884064\n",
            "step: 13210, loss: 0.1248532086610794\n",
            "step: 13220, loss: 0.17604945600032806\n",
            "step: 13230, loss: 0.05223429203033447\n",
            "step: 13240, loss: 0.10743922740221024\n",
            "step: 13250, loss: 0.1292797178030014\n",
            "step: 13260, loss: 0.16434204578399658\n",
            "step: 13270, loss: 0.1798103004693985\n",
            "step: 13280, loss: 0.2612646222114563\n",
            "step: 13290, loss: 0.193050816655159\n",
            "step: 13300, loss: 0.08832203596830368\n",
            "step: 13310, loss: 0.025077514350414276\n",
            "step: 13320, loss: 0.030073808506131172\n",
            "step: 13330, loss: 0.1405206173658371\n",
            "step: 13340, loss: 0.054040346294641495\n",
            "step: 13350, loss: 0.06778956204652786\n",
            "step: 13360, loss: 0.086422398686409\n",
            "step: 13370, loss: 0.24081729352474213\n",
            "step: 13380, loss: 0.13523663580417633\n",
            "step: 13390, loss: 0.04969071224331856\n",
            "step: 13400, loss: 0.10569895058870316\n",
            "step: 13410, loss: 0.15271447598934174\n",
            "step: 13420, loss: 0.11444889008998871\n",
            "step: 13430, loss: 0.10896480083465576\n",
            "step: 13440, loss: 0.03127710148692131\n",
            "step: 13450, loss: 0.16537734866142273\n",
            "step: 13460, loss: 0.28189823031425476\n",
            "step: 13470, loss: 0.11659400910139084\n",
            "step: 13480, loss: 0.11802151054143906\n",
            "step: 13490, loss: 0.2957465946674347\n",
            "step: 13500, loss: 0.11403973400592804\n",
            "step: 13510, loss: 0.1303035318851471\n",
            "step: 13520, loss: 0.046264320611953735\n",
            "step: 13530, loss: 0.11028669029474258\n",
            "step: 13540, loss: 0.1616583615541458\n",
            "step: 13550, loss: 0.028967080637812614\n",
            "step: 13560, loss: 0.076454296708107\n",
            "step: 13570, loss: 0.29738086462020874\n",
            "step: 13580, loss: 0.09095178544521332\n",
            "step: 13590, loss: 0.06807413697242737\n",
            "step: 13600, loss: 0.19580931961536407\n",
            "step: 13610, loss: 0.06794271618127823\n",
            "step: 13620, loss: 0.023821726441383362\n",
            "step: 13630, loss: 0.08258414268493652\n",
            "step: 13640, loss: 0.13520722091197968\n",
            "step: 13650, loss: 0.06491505354642868\n",
            "step: 13660, loss: 0.30080780386924744\n",
            "step: 13670, loss: 0.05957765504717827\n",
            "step: 13680, loss: 0.03484005108475685\n",
            "step: 13690, loss: 0.04608263820409775\n",
            "step: 13700, loss: 0.08404850214719772\n",
            "step: 13710, loss: 0.15232351422309875\n",
            "step: 13720, loss: 0.14770014584064484\n",
            "step: 13730, loss: 0.10597546398639679\n",
            "step: 13740, loss: 0.04116256907582283\n",
            "step: 13750, loss: 0.1477629542350769\n",
            "step: 13760, loss: 0.07657624036073685\n",
            "step: 13770, loss: 0.10789584368467331\n",
            "step: 13780, loss: 0.18968641757965088\n",
            "step: 13790, loss: 0.014490602537989616\n",
            "step: 13800, loss: 0.029719514772295952\n",
            "step: 13810, loss: 0.05292694270610809\n",
            "step: 13820, loss: 0.052118584513664246\n",
            "step: 13830, loss: 0.07580233365297318\n",
            "step: 13840, loss: 0.044851113110780716\n",
            "step: 13850, loss: 0.1665012389421463\n",
            "step: 13860, loss: 0.1833936870098114\n",
            "step: 13870, loss: 0.05395554378628731\n",
            "step: 13880, loss: 0.016152892261743546\n",
            "step: 13890, loss: 0.12992466986179352\n",
            "step: 13900, loss: 0.1557672917842865\n",
            "step: 13910, loss: 0.07812818139791489\n",
            "step: 13920, loss: 0.06797704845666885\n",
            "step: 13930, loss: 0.14765673875808716\n",
            "step: 13940, loss: 0.1089366152882576\n",
            "step: 13950, loss: 0.14150767028331757\n",
            "step: 13960, loss: 0.014945697039365768\n",
            "step: 13970, loss: 0.17384125292301178\n",
            "step: 13980, loss: 0.05568153038620949\n",
            "step: 13990, loss: 0.1662432700395584\n",
            "step: 14000, loss: 0.06388700008392334\n",
            "step: 14010, loss: 0.09134960174560547\n",
            "step: 14020, loss: 0.11618566513061523\n",
            "step: 14030, loss: 0.16426202654838562\n",
            "step: 14040, loss: 0.07242437452077866\n",
            "step: 14050, loss: 0.149285688996315\n",
            "step: 14060, loss: 0.08616621792316437\n",
            "step: 14070, loss: 0.08155394345521927\n",
            "step: 14080, loss: 0.09441376477479935\n",
            "step: 14090, loss: 0.10592135041952133\n",
            "step: 14100, loss: 0.07592236995697021\n",
            "step: 14110, loss: 0.06646495312452316\n",
            "step: 14120, loss: 0.09849286824464798\n",
            "step: 14130, loss: 0.1372668296098709\n",
            "step: 14140, loss: 0.08025818318128586\n",
            "step: 14150, loss: 0.05489238724112511\n",
            "step: 14160, loss: 0.10203611850738525\n",
            "step: 14170, loss: 0.08744252473115921\n",
            "step: 14180, loss: 0.09830673784017563\n",
            "step: 14190, loss: 0.14793582260608673\n",
            "step: 14200, loss: 0.1606164276599884\n",
            "step: 14210, loss: 0.056238509714603424\n",
            "step: 14220, loss: 0.20459721982479095\n",
            "step: 14230, loss: 0.07326598465442657\n",
            "step: 14240, loss: 0.17964063584804535\n",
            "step: 14250, loss: 0.07965958118438721\n",
            "step: 14260, loss: 0.013782032765448093\n",
            "step: 14270, loss: 0.06657619774341583\n",
            "step: 14280, loss: 0.23007267713546753\n",
            "step: 14290, loss: 0.11387275904417038\n",
            "step: 14300, loss: 0.07386985421180725\n",
            "step: 14310, loss: 0.013757523149251938\n",
            "step: 14320, loss: 0.04520108178257942\n",
            "step: 14330, loss: 0.10633993148803711\n",
            "step: 14340, loss: 0.12898941338062286\n",
            "step: 14350, loss: 0.04508122056722641\n",
            "step: 14360, loss: 0.08844342827796936\n",
            "step: 14370, loss: 0.029424482956528664\n",
            "step: 14380, loss: 0.09393294900655746\n",
            "step: 14390, loss: 0.06942594796419144\n",
            "step: 14400, loss: 0.15547114610671997\n",
            "step: 14410, loss: 0.14478883147239685\n",
            "step: 14420, loss: 0.04994050785899162\n",
            "step: 14430, loss: 0.022116053849458694\n",
            "step: 14440, loss: 0.03110877051949501\n",
            "step: 14450, loss: 0.20341874659061432\n",
            "step: 14460, loss: 0.0843164399266243\n",
            "step: 14470, loss: 0.10640423744916916\n",
            "step: 14480, loss: 0.17165936529636383\n",
            "step: 14490, loss: 0.2334781438112259\n",
            "step: 14500, loss: 0.04507223144173622\n",
            "step: 14510, loss: 0.039480410516262054\n",
            "step: 14520, loss: 0.06553266942501068\n",
            "step: 14530, loss: 0.04462237283587456\n",
            "step: 14540, loss: 0.04964243620634079\n",
            "step: 14550, loss: 0.07471521943807602\n",
            "step: 14560, loss: 0.05948147922754288\n",
            "step: 14570, loss: 0.14019732177257538\n",
            "step: 14580, loss: 0.04231966659426689\n",
            "step: 14590, loss: 0.11688213050365448\n",
            "step: 14600, loss: 0.07823357731103897\n",
            "step: 14610, loss: 0.05824238061904907\n",
            "step: 14620, loss: 0.1761416494846344\n",
            "step: 14630, loss: 0.013667313382029533\n",
            "step: 14640, loss: 0.0688793733716011\n",
            "step: 14650, loss: 0.026886627078056335\n",
            "step: 14660, loss: 0.029809216037392616\n",
            "step: 14670, loss: 0.20582468807697296\n",
            "step: 14680, loss: 0.13920800387859344\n",
            "step: 14690, loss: 0.09904560446739197\n",
            "step: 14700, loss: 0.08835270255804062\n",
            "step: 14710, loss: 0.09953880310058594\n",
            "step: 14720, loss: 0.17188367247581482\n",
            "step: 14730, loss: 0.03023768588900566\n",
            "step: 14740, loss: 0.08893565088510513\n",
            "step: 14750, loss: 0.16553720831871033\n",
            "step: 14760, loss: 0.09041484445333481\n",
            "step: 14770, loss: 0.10857990384101868\n",
            "step: 14780, loss: 0.10346364974975586\n",
            "step: 14790, loss: 0.1713762879371643\n",
            "step: 14800, loss: 0.10085126012563705\n",
            "step: 14810, loss: 0.16519439220428467\n",
            "step: 14820, loss: 0.10882391780614853\n",
            "step: 14830, loss: 0.11989640444517136\n",
            "step: 14840, loss: 0.07842114567756653\n",
            "step: 14850, loss: 0.059437405318021774\n",
            "step: 14860, loss: 0.06073281168937683\n",
            "step: 14870, loss: 0.1026257574558258\n",
            "step: 14880, loss: 0.19316254556179047\n",
            "step: 14890, loss: 0.0787384882569313\n",
            "step: 14900, loss: 0.15270781517028809\n",
            "step: 14910, loss: 0.07059605419635773\n",
            "step: 14920, loss: 0.029375318437814713\n",
            "step: 14930, loss: 0.20008713006973267\n",
            "step: 14940, loss: 0.1027599349617958\n",
            "step: 14950, loss: 0.0990498811006546\n",
            "step: 14960, loss: 0.25502368807792664\n",
            "step: 14970, loss: 0.15932998061180115\n",
            "step: 14980, loss: 0.1722930371761322\n",
            "step: 14990, loss: 0.13905364274978638\n",
            "step: 15000, loss: 0.10449784994125366\n",
            "step: 15010, loss: 0.20792405307292938\n",
            "step: 15020, loss: 0.04500627890229225\n",
            "step: 15030, loss: 0.012982282787561417\n",
            "step: 15040, loss: 0.09900787472724915\n",
            "step: 15050, loss: 0.09329354763031006\n",
            "step: 15060, loss: 0.10548653453588486\n",
            "step: 15070, loss: 0.2341672033071518\n",
            "step: 15080, loss: 0.15142641961574554\n",
            "step: 15090, loss: 0.24453026056289673\n",
            "step: 15100, loss: 0.09633925557136536\n",
            "step: 15110, loss: 0.0903293639421463\n",
            "step: 15120, loss: 0.02614039182662964\n",
            "step: 15130, loss: 0.06951028853654861\n",
            "step: 15140, loss: 0.05887552350759506\n",
            "step: 15150, loss: 0.0838242918252945\n",
            "step: 15160, loss: 0.042209193110466\n",
            "step: 15170, loss: 0.17216981947422028\n",
            "step: 15180, loss: 0.0732443630695343\n",
            "step: 15190, loss: 0.21224813163280487\n",
            "step: 15200, loss: 0.286647230386734\n",
            "step: 15210, loss: 0.09324737638235092\n",
            "step: 15220, loss: 0.055005352944135666\n",
            "step: 15230, loss: 0.14042609930038452\n",
            "step: 15240, loss: 0.05306355282664299\n",
            "step: 15250, loss: 0.03344979137182236\n",
            "step: 15260, loss: 0.08996699005365372\n",
            "step: 15270, loss: 0.04907338321208954\n",
            "step: 15280, loss: 0.2153160125017166\n",
            "step: 15290, loss: 0.024771807715296745\n",
            "step: 15300, loss: 0.11626380681991577\n",
            "step: 15310, loss: 0.0695989802479744\n",
            "step: 15320, loss: 0.10084053128957748\n",
            "step: 15330, loss: 0.09124752879142761\n",
            "step: 15340, loss: 0.02134196273982525\n",
            "step: 15350, loss: 0.03355413302779198\n",
            "step: 15360, loss: 0.032119061797857285\n",
            "step: 15370, loss: 0.19535598158836365\n",
            "step: 15380, loss: 0.30776306986808777\n",
            "step: 15390, loss: 0.0494309663772583\n",
            "step: 15400, loss: 0.10433847457170486\n",
            "step: 15410, loss: 0.08662241697311401\n",
            "step: 15420, loss: 0.06609649211168289\n",
            "step: 15430, loss: 0.11718269437551498\n",
            "step: 15440, loss: 0.21554939448833466\n",
            "step: 15450, loss: 0.11163447797298431\n",
            "step: 15460, loss: 0.11000802367925644\n",
            "step: 15470, loss: 0.03464965894818306\n",
            "step: 15480, loss: 0.08210702240467072\n",
            "step: 15490, loss: 0.15195663273334503\n",
            "step: 15500, loss: 0.10073621571063995\n",
            "step: 15510, loss: 0.16623149812221527\n",
            "step: 15520, loss: 0.3110150694847107\n",
            "step: 15530, loss: 0.07553105056285858\n",
            "step: 15540, loss: 0.09638140350580215\n",
            "step: 15550, loss: 0.11160021275281906\n",
            "step: 15560, loss: 0.11165464669466019\n",
            "step: 15570, loss: 0.10719720274209976\n",
            "step: 15580, loss: 0.10462655127048492\n",
            "step: 15590, loss: 0.107526034116745\n",
            "step: 15600, loss: 0.07771029323339462\n",
            "step: 15610, loss: 0.12847310304641724\n",
            "step: 15620, loss: 0.06241428852081299\n",
            "step: 15630, loss: 0.15633660554885864\n",
            "step: 15640, loss: 0.15133468806743622\n",
            "step: 15650, loss: 0.11644601821899414\n",
            "step: 15660, loss: 0.09409918636083603\n",
            "step: 15670, loss: 0.04325943812727928\n",
            "step: 15680, loss: 0.1162700206041336\n",
            "step: 15690, loss: 0.18611006438732147\n",
            "step: 15700, loss: 0.10954258590936661\n",
            "step: 15710, loss: 0.18834446370601654\n",
            "step: 15720, loss: 0.10152236372232437\n",
            "step: 15730, loss: 0.06522631645202637\n",
            "step: 15740, loss: 0.17034363746643066\n",
            "step: 15750, loss: 0.09435537457466125\n",
            "step: 15760, loss: 0.04682768136262894\n",
            "step: 15770, loss: 0.06342141330242157\n",
            "step: 15780, loss: 0.0896911546587944\n",
            "step: 15790, loss: 0.08215168863534927\n",
            "step: 15800, loss: 0.1975497305393219\n",
            "step: 15810, loss: 0.026648031547665596\n",
            "step: 15820, loss: 0.13069991767406464\n",
            "step: 15830, loss: 0.045927390456199646\n",
            "step: 15840, loss: 0.06652916967868805\n",
            "step: 15850, loss: 0.09821662306785583\n",
            "step: 15860, loss: 0.011944884434342384\n",
            "step: 15870, loss: 0.17791436612606049\n",
            "step: 15880, loss: 0.09390832483768463\n",
            "step: 15890, loss: 0.3310830593109131\n",
            "step: 15900, loss: 0.03099249117076397\n",
            "step: 15910, loss: 0.15326818823814392\n",
            "step: 15920, loss: 0.17825543880462646\n",
            "step: 15930, loss: 0.07154028117656708\n",
            "step: 15940, loss: 0.1620345115661621\n",
            "step: 15950, loss: 0.014911573380231857\n",
            "step: 15960, loss: 0.08644314855337143\n",
            "step: 15970, loss: 0.12611620128154755\n",
            "step: 15980, loss: 0.09233862906694412\n",
            "step: 15990, loss: 0.07860367000102997\n",
            "step: 16000, loss: 0.01145047415047884\n",
            "step: 16010, loss: 0.059527598321437836\n",
            "step: 16020, loss: 0.030883291736245155\n",
            "step: 16030, loss: 0.039766378700733185\n",
            "step: 16040, loss: 0.13971704244613647\n",
            "step: 16050, loss: 0.13792851567268372\n",
            "step: 16060, loss: 0.14223046600818634\n",
            "step: 16070, loss: 0.17593534290790558\n",
            "step: 16080, loss: 0.05866829678416252\n",
            "step: 16090, loss: 0.013379263691604137\n",
            "step: 16100, loss: 0.09632251411676407\n",
            "step: 16110, loss: 0.04794323071837425\n",
            "step: 16120, loss: 0.13254037499427795\n",
            "step: 16130, loss: 0.03037470020353794\n",
            "step: 16140, loss: 0.06874312460422516\n",
            "step: 16150, loss: 0.15819396078586578\n",
            "step: 16160, loss: 0.08587890863418579\n",
            "step: 16170, loss: 0.03709171339869499\n",
            "step: 16180, loss: 0.1443459391593933\n",
            "step: 16190, loss: 0.05381190404295921\n",
            "step: 16200, loss: 0.06062130630016327\n",
            "step: 16210, loss: 0.17889493703842163\n",
            "step: 16220, loss: 0.19268664717674255\n",
            "step: 16230, loss: 0.04340521991252899\n",
            "step: 16240, loss: 0.058014996349811554\n",
            "step: 16250, loss: 0.013657267205417156\n",
            "step: 16260, loss: 0.1896342933177948\n",
            "step: 16270, loss: 0.048669759184122086\n",
            "step: 16280, loss: 0.09946011006832123\n",
            "step: 16290, loss: 0.014453374780714512\n",
            "step: 16300, loss: 0.128261536359787\n",
            "step: 16310, loss: 0.07241881638765335\n",
            "step: 16320, loss: 0.1180264875292778\n",
            "step: 16330, loss: 0.0591859370470047\n",
            "step: 16340, loss: 0.18141864240169525\n",
            "step: 16350, loss: 0.10914762318134308\n",
            "step: 16360, loss: 0.04724719002842903\n",
            "step: 16370, loss: 0.09291595965623856\n",
            "step: 16380, loss: 0.043379902839660645\n",
            "step: 16390, loss: 0.0312572605907917\n",
            "step: 16400, loss: 0.04123372584581375\n",
            "step: 16410, loss: 0.05188559740781784\n",
            "step: 16420, loss: 0.14487700164318085\n",
            "step: 16430, loss: 0.24696481227874756\n",
            "step: 16440, loss: 0.09408566355705261\n",
            "step: 16450, loss: 0.041834115982055664\n",
            "step: 16460, loss: 0.05223967134952545\n",
            "step: 16470, loss: 0.04297936335206032\n",
            "step: 16480, loss: 0.055840812623500824\n",
            "step: 16490, loss: 0.05399974435567856\n",
            "step: 16500, loss: 0.02913077361881733\n",
            "step: 16510, loss: 0.09801967442035675\n",
            "step: 16520, loss: 0.09602759778499603\n",
            "step: 16530, loss: 0.10940342396497726\n",
            "step: 16540, loss: 0.09550685435533524\n",
            "step: 16550, loss: 0.029491128399968147\n",
            "step: 16560, loss: 0.0668206587433815\n",
            "step: 16570, loss: 0.052722178399562836\n",
            "step: 16580, loss: 0.012594042345881462\n",
            "step: 16590, loss: 0.028438610956072807\n",
            "step: 16600, loss: 0.1229630783200264\n",
            "step: 16610, loss: 0.05191696435213089\n",
            "step: 16620, loss: 0.13514137268066406\n",
            "step: 16630, loss: 0.08526661992073059\n",
            "step: 16640, loss: 0.10430369526147842\n",
            "step: 16650, loss: 0.10125292092561722\n",
            "step: 16660, loss: 0.08760002255439758\n",
            "step: 16670, loss: 0.03970494866371155\n",
            "step: 16680, loss: 0.12132364511489868\n",
            "step: 16690, loss: 0.02534833922982216\n",
            "step: 16700, loss: 0.16780664026737213\n",
            "step: 16710, loss: 0.056164566427469254\n",
            "step: 16720, loss: 0.11251893639564514\n",
            "step: 16730, loss: 0.19463486969470978\n",
            "step: 16740, loss: 0.09784861654043198\n",
            "step: 16750, loss: 0.14605394005775452\n",
            "step: 16760, loss: 0.1220933273434639\n",
            "step: 16770, loss: 0.050575047731399536\n",
            "step: 16780, loss: 0.08104962855577469\n",
            "step: 16790, loss: 0.03449641913175583\n",
            "step: 16800, loss: 0.08330173790454865\n",
            "step: 16810, loss: 0.0698048323392868\n",
            "step: 16820, loss: 0.07766237109899521\n",
            "step: 16830, loss: 0.011756612919270992\n",
            "step: 16840, loss: 0.12957578897476196\n",
            "step: 16850, loss: 0.07775108516216278\n",
            "step: 16860, loss: 0.12443886697292328\n",
            "step: 16870, loss: 0.15395937860012054\n",
            "step: 16880, loss: 0.03263379633426666\n",
            "step: 16890, loss: 0.13114003837108612\n",
            "step: 16900, loss: 0.1115080937743187\n",
            "step: 16910, loss: 0.030208801850676537\n",
            "step: 16920, loss: 0.09218045324087143\n",
            "step: 16930, loss: 0.09318261593580246\n",
            "step: 16940, loss: 0.15476302802562714\n",
            "step: 16950, loss: 0.05248385667800903\n",
            "step: 16960, loss: 0.05202494189143181\n",
            "step: 16970, loss: 0.3503393828868866\n",
            "step: 16980, loss: 0.07089962810277939\n",
            "step: 16990, loss: 0.18563398718833923\n",
            "step: 17000, loss: 0.04432200267910957\n",
            "step: 17010, loss: 0.10884073376655579\n",
            "step: 17020, loss: 0.05046556144952774\n",
            "step: 17030, loss: 0.1102999895811081\n",
            "step: 17040, loss: 0.27875059843063354\n",
            "step: 17050, loss: 0.06820502877235413\n",
            "step: 17060, loss: 0.14489825069904327\n",
            "step: 17070, loss: 0.04560527577996254\n",
            "step: 17080, loss: 0.14795900881290436\n",
            "step: 17090, loss: 0.012735503725707531\n",
            "step: 17100, loss: 0.012791444547474384\n",
            "step: 17110, loss: 0.16915607452392578\n",
            "step: 17120, loss: 0.0591098815202713\n",
            "step: 17130, loss: 0.10594329982995987\n",
            "step: 17140, loss: 0.08126556873321533\n",
            "step: 17150, loss: 0.0890546664595604\n",
            "step: 17160, loss: 0.11857864260673523\n",
            "step: 17170, loss: 0.05871513485908508\n",
            "step: 17180, loss: 0.03890054672956467\n",
            "step: 17190, loss: 0.07095351815223694\n",
            "step: 17200, loss: 0.2053370177745819\n",
            "step: 17210, loss: 0.047995731234550476\n",
            "step: 17220, loss: 0.14125534892082214\n",
            "step: 17230, loss: 0.10776892304420471\n",
            "step: 17240, loss: 0.10885734856128693\n",
            "step: 17250, loss: 0.10583791136741638\n",
            "step: 17260, loss: 0.08042258769273758\n",
            "step: 17270, loss: 0.1163545697927475\n",
            "step: 17280, loss: 0.12851107120513916\n",
            "step: 17290, loss: 0.07188854366540909\n",
            "step: 17300, loss: 0.03808921203017235\n",
            "step: 17310, loss: 0.0613027885556221\n",
            "step: 17320, loss: 0.05158321559429169\n",
            "step: 17330, loss: 0.07619653642177582\n",
            "step: 17340, loss: 0.23487453162670135\n",
            "step: 17350, loss: 0.05134928971529007\n",
            "step: 17360, loss: 0.13565993309020996\n",
            "step: 17370, loss: 0.15017087757587433\n",
            "step: 17380, loss: 0.22292284667491913\n",
            "step: 17390, loss: 0.09535270184278488\n",
            "step: 17400, loss: 0.09602588415145874\n",
            "step: 17410, loss: 0.14757394790649414\n",
            "step: 17420, loss: 0.09075927734375\n",
            "step: 17430, loss: 0.12010705471038818\n",
            "step: 17440, loss: 0.06645689904689789\n",
            "step: 17450, loss: 0.12368374317884445\n",
            "step: 17460, loss: 0.07251855731010437\n",
            "step: 17470, loss: 0.08882858604192734\n",
            "step: 17480, loss: 0.10901251435279846\n",
            "step: 17490, loss: 0.18765169382095337\n",
            "step: 17500, loss: 0.0953487679362297\n",
            "step: 17510, loss: 0.047856416553258896\n",
            "step: 17520, loss: 0.06958448886871338\n",
            "step: 17530, loss: 0.03297651559114456\n",
            "step: 17540, loss: 0.03145859017968178\n",
            "step: 17550, loss: 0.13419701159000397\n",
            "step: 17560, loss: 0.105474554002285\n",
            "step: 17570, loss: 0.04124751687049866\n",
            "step: 17580, loss: 0.06709559261798859\n",
            "step: 17590, loss: 0.11089549213647842\n",
            "step: 17600, loss: 0.07509991526603699\n",
            "step: 17610, loss: 0.05924030765891075\n",
            "step: 17620, loss: 0.02480481192469597\n",
            "step: 17630, loss: 0.07521656900644302\n",
            "step: 17640, loss: 0.13014158606529236\n",
            "step: 17650, loss: 0.11521045118570328\n",
            "step: 17660, loss: 0.05032813176512718\n",
            "step: 17670, loss: 0.015756158158183098\n",
            "step: 17680, loss: 0.04822465032339096\n",
            "step: 17690, loss: 0.16676758229732513\n",
            "step: 17700, loss: 0.20872889459133148\n",
            "step: 17710, loss: 0.114143967628479\n",
            "step: 17720, loss: 0.08262395858764648\n",
            "step: 17730, loss: 0.23183442652225494\n",
            "step: 17740, loss: 0.21836401522159576\n",
            "step: 17750, loss: 0.23546102643013\n",
            "step: 17760, loss: 0.07867085188627243\n",
            "step: 17770, loss: 0.014875187538564205\n",
            "step: 17780, loss: 0.14983482658863068\n",
            "step: 17790, loss: 0.08231721073389053\n",
            "step: 17800, loss: 0.03291460871696472\n",
            "step: 17810, loss: 0.11164229363203049\n",
            "step: 17820, loss: 0.08760477602481842\n",
            "step: 17830, loss: 0.02744520641863346\n",
            "step: 17840, loss: 0.08670079708099365\n",
            "step: 17850, loss: 0.058750901371240616\n",
            "step: 17860, loss: 0.06941937655210495\n",
            "step: 17870, loss: 0.034620411694049835\n",
            "step: 17880, loss: 0.08028652518987656\n",
            "step: 17890, loss: 0.1354537457227707\n",
            "step: 17900, loss: 0.07097084075212479\n",
            "step: 17910, loss: 0.15386240184307098\n",
            "step: 17920, loss: 0.053780701011419296\n",
            "step: 17930, loss: 0.08030778169631958\n",
            "step: 17940, loss: 0.05766436830163002\n",
            "step: 17950, loss: 0.05423278734087944\n",
            "step: 17960, loss: 0.09571494162082672\n",
            "step: 17970, loss: 0.03639785200357437\n",
            "step: 17980, loss: 0.1655506193637848\n",
            "step: 17990, loss: 0.26164349913597107\n",
            "step: 18000, loss: 0.25964945554733276\n",
            "step: 18010, loss: 0.1133618876338005\n",
            "step: 18020, loss: 0.07813513278961182\n",
            "step: 18030, loss: 0.04231451824307442\n",
            "step: 18040, loss: 0.03716306760907173\n",
            "step: 18050, loss: 0.0987105444073677\n",
            "step: 18060, loss: 0.07866255939006805\n",
            "step: 18070, loss: 0.07198720425367355\n",
            "step: 18080, loss: 0.1941506415605545\n",
            "step: 18090, loss: 0.03196299448609352\n",
            "step: 18100, loss: 0.08148296177387238\n",
            "step: 18110, loss: 0.030801743268966675\n",
            "step: 18120, loss: 0.16885906457901\n",
            "step: 18130, loss: 0.17997443675994873\n",
            "step: 18140, loss: 0.06391443312168121\n",
            "step: 18150, loss: 0.06898326426744461\n",
            "step: 18160, loss: 0.0289913322776556\n",
            "step: 18170, loss: 0.06985164433717728\n",
            "step: 18180, loss: 0.03620994836091995\n",
            "step: 18190, loss: 0.08438751101493835\n",
            "step: 18200, loss: 0.07923789322376251\n",
            "step: 18210, loss: 0.1557910144329071\n",
            "step: 18220, loss: 0.0984792560338974\n",
            "step: 18230, loss: 0.1264239251613617\n",
            "step: 18240, loss: 0.1995552033185959\n",
            "step: 18250, loss: 0.19478052854537964\n",
            "step: 18260, loss: 0.15932732820510864\n",
            "step: 18270, loss: 0.16615089774131775\n",
            "step: 18280, loss: 0.05385541543364525\n",
            "step: 18290, loss: 0.10570541769266129\n",
            "step: 18300, loss: 0.09915615618228912\n",
            "step: 18310, loss: 0.05250702425837517\n",
            "step: 18320, loss: 0.11206752061843872\n",
            "step: 18330, loss: 0.08016099780797958\n",
            "step: 18340, loss: 0.014215243048965931\n",
            "step: 18350, loss: 0.05163579434156418\n",
            "step: 18360, loss: 0.021522751078009605\n",
            "step: 18370, loss: 0.10058930516242981\n",
            "step: 18380, loss: 0.06740805506706238\n",
            "step: 18390, loss: 0.05342788249254227\n",
            "step: 18400, loss: 0.12422895431518555\n",
            "step: 18410, loss: 0.07173050194978714\n",
            "step: 18420, loss: 0.16287347674369812\n",
            "step: 18430, loss: 0.14782734215259552\n",
            "step: 18440, loss: 0.037388868629932404\n",
            "step: 18450, loss: 0.08175249397754669\n",
            "step: 18460, loss: 0.06953416019678116\n",
            "step: 18470, loss: 0.1695532351732254\n",
            "step: 18480, loss: 0.16566941142082214\n",
            "step: 18490, loss: 0.08502558618783951\n",
            "step: 18500, loss: 0.1726192981004715\n",
            "step: 18510, loss: 0.07349175214767456\n",
            "step: 18520, loss: 0.0321354903280735\n",
            "step: 18530, loss: 0.03437919542193413\n",
            "step: 18540, loss: 0.2572935223579407\n",
            "step: 18550, loss: 0.10954324156045914\n",
            "step: 18560, loss: 0.024105269461870193\n",
            "step: 18570, loss: 0.03796956688165665\n",
            "step: 18580, loss: 0.17415282130241394\n",
            "step: 18590, loss: 0.09163887053728104\n",
            "step: 18600, loss: 0.088514044880867\n",
            "step: 18610, loss: 0.05712181329727173\n",
            "step: 18620, loss: 0.046947676688432693\n",
            "step: 18630, loss: 0.10319919139146805\n",
            "step: 18640, loss: 0.11330775171518326\n",
            "step: 18650, loss: 0.11304629594087601\n",
            "step: 18660, loss: 0.014104771427810192\n",
            "step: 18670, loss: 0.044984493404626846\n",
            "step: 18680, loss: 0.12375260889530182\n",
            "step: 18690, loss: 0.050800830125808716\n",
            "step: 18700, loss: 0.3132558763027191\n",
            "step: 18710, loss: 0.16255024075508118\n",
            "step: 18720, loss: 0.14175906777381897\n",
            "step: 18730, loss: 0.07454947382211685\n",
            "step: 18740, loss: 0.09760002046823502\n",
            "step: 18750, loss: 0.07496029138565063\n",
            "step: 18760, loss: 0.0614844486117363\n",
            "step: 18770, loss: 0.06179654598236084\n",
            "step: 18780, loss: 0.08666353672742844\n",
            "step: 18790, loss: 0.03589252382516861\n",
            "step: 18800, loss: 0.09051195532083511\n",
            "step: 18810, loss: 0.03618578985333443\n",
            "step: 18820, loss: 0.12303709983825684\n",
            "step: 18830, loss: 0.024096224457025528\n",
            "step: 18840, loss: 0.10785254836082458\n",
            "step: 18850, loss: 0.26263248920440674\n",
            "step: 18860, loss: 0.014049669727683067\n",
            "step: 18870, loss: 0.07804708182811737\n",
            "step: 18880, loss: 0.08376983553171158\n",
            "step: 18890, loss: 0.05154871568083763\n",
            "step: 18900, loss: 0.044889144599437714\n",
            "step: 18910, loss: 0.0818377360701561\n",
            "step: 18920, loss: 0.09798960387706757\n",
            "step: 18930, loss: 0.09479764103889465\n",
            "step: 18940, loss: 0.10807228088378906\n",
            "step: 18950, loss: 0.0518496073782444\n",
            "step: 18960, loss: 0.09050223976373672\n",
            "step: 18970, loss: 0.032589372247457504\n",
            "step: 18980, loss: 0.03035750798881054\n",
            "step: 18990, loss: 0.03734523057937622\n",
            "step: 19000, loss: 0.126520574092865\n",
            "step: 19010, loss: 0.08136188983917236\n",
            "step: 19020, loss: 0.2624185383319855\n",
            "step: 19030, loss: 0.1856345534324646\n",
            "step: 19040, loss: 0.06287672370672226\n",
            "step: 19050, loss: 0.014034618623554707\n",
            "step: 19060, loss: 0.063912034034729\n",
            "step: 19070, loss: 0.06754785776138306\n",
            "step: 19080, loss: 0.04246581718325615\n",
            "step: 19090, loss: 0.1374007910490036\n",
            "step: 19100, loss: 0.16844940185546875\n",
            "step: 19110, loss: 0.1793757975101471\n",
            "step: 19120, loss: 0.11499424278736115\n",
            "step: 19130, loss: 0.08310100436210632\n",
            "step: 19140, loss: 0.0704074576497078\n",
            "step: 19150, loss: 0.08864292502403259\n",
            "step: 19160, loss: 0.22116465866565704\n",
            "step: 19170, loss: 0.10805583745241165\n",
            "step: 19180, loss: 0.042910926043987274\n",
            "step: 19190, loss: 0.1538044512271881\n",
            "step: 19200, loss: 0.14562368392944336\n",
            "step: 19210, loss: 0.1075967326760292\n",
            "step: 19220, loss: 0.06571906805038452\n",
            "step: 19230, loss: 0.17179155349731445\n",
            "step: 19240, loss: 0.11185861378908157\n",
            "step: 19250, loss: 0.22187486290931702\n",
            "step: 19260, loss: 0.06558044999837875\n",
            "step: 19270, loss: 0.15133462846279144\n",
            "step: 19280, loss: 0.012549497187137604\n",
            "step: 19290, loss: 0.06417965143918991\n",
            "step: 19300, loss: 0.10085421800613403\n",
            "step: 19310, loss: 0.13888394832611084\n",
            "step: 19320, loss: 0.12961901724338531\n",
            "step: 19330, loss: 0.08813349157571793\n",
            "step: 19340, loss: 0.033293064683675766\n",
            "step: 19350, loss: 0.050732750445604324\n",
            "step: 19360, loss: 0.19886258244514465\n",
            "step: 19370, loss: 0.0896991640329361\n",
            "step: 19380, loss: 0.14829479157924652\n",
            "step: 19390, loss: 0.1468074768781662\n",
            "step: 19400, loss: 0.06608704477548599\n",
            "step: 19410, loss: 0.060497816652059555\n",
            "step: 19420, loss: 0.092316173017025\n",
            "step: 19430, loss: 0.06594690680503845\n",
            "step: 19440, loss: 0.07510911673307419\n",
            "step: 19450, loss: 0.15937504172325134\n",
            "step: 19460, loss: 0.06575081497430801\n",
            "step: 19470, loss: 0.12424268573522568\n",
            "step: 19480, loss: 0.121257483959198\n",
            "step: 19490, loss: 0.20726965367794037\n",
            "step: 19500, loss: 0.08704966306686401\n",
            "step: 19510, loss: 0.023681387305259705\n",
            "step: 19520, loss: 0.14149531722068787\n",
            "step: 19530, loss: 0.08583742380142212\n",
            "step: 19540, loss: 0.1820390224456787\n",
            "step: 19550, loss: 0.05310603231191635\n",
            "step: 19560, loss: 0.1811779886484146\n",
            "step: 19570, loss: 0.04172460734844208\n",
            "step: 19580, loss: 0.07624373584985733\n",
            "step: 19590, loss: 0.10045021772384644\n",
            "step: 19600, loss: 0.10830538719892502\n",
            "step: 19610, loss: 0.11037398129701614\n",
            "step: 19620, loss: 0.10763266682624817\n",
            "step: 19630, loss: 0.03907480090856552\n",
            "step: 19640, loss: 0.2330799400806427\n",
            "step: 19650, loss: 0.08392352610826492\n",
            "step: 19660, loss: 0.15380750596523285\n",
            "step: 19670, loss: 0.04393831267952919\n",
            "step: 19680, loss: 0.08231550455093384\n",
            "step: 19690, loss: 0.20440515875816345\n",
            "step: 19700, loss: 0.05342957749962807\n",
            "step: 19710, loss: 0.09639452397823334\n",
            "step: 19720, loss: 0.09009317308664322\n",
            "step: 19730, loss: 0.014136075042188168\n",
            "step: 19740, loss: 0.056900154799222946\n",
            "step: 19750, loss: 0.11594255268573761\n",
            "step: 19760, loss: 0.15401864051818848\n",
            "step: 19770, loss: 0.1137106642127037\n",
            "step: 19780, loss: 0.05008105933666229\n",
            "step: 19790, loss: 0.11530699580907822\n",
            "step: 19800, loss: 0.08232679218053818\n",
            "step: 19810, loss: 0.0814020186662674\n",
            "step: 19820, loss: 0.050462715327739716\n",
            "step: 19830, loss: 0.08493973314762115\n",
            "step: 19840, loss: 0.07139184325933456\n",
            "step: 19850, loss: 0.07002108544111252\n",
            "step: 19860, loss: 0.045348092913627625\n",
            "step: 19870, loss: 0.01255701668560505\n",
            "step: 19880, loss: 0.10705017298460007\n",
            "step: 19890, loss: 0.08286751806735992\n",
            "step: 19900, loss: 0.07766223698854446\n",
            "step: 19910, loss: 0.08548864722251892\n",
            "step: 19920, loss: 0.01212879829108715\n",
            "step: 19930, loss: 0.03865400701761246\n",
            "step: 19940, loss: 0.09910362958908081\n",
            "step: 19950, loss: 0.1628854125738144\n",
            "step: 19960, loss: 0.08910772949457169\n",
            "step: 19970, loss: 0.05273475497961044\n",
            "step: 19980, loss: 0.20487023890018463\n",
            "step: 19990, loss: 0.1149488240480423\n",
            "step: 20000, loss: 0.06935190409421921\n",
            "step: 20010, loss: 0.09386362135410309\n",
            "step: 20020, loss: 0.11271826177835464\n",
            "step: 20030, loss: 0.13452622294425964\n",
            "step: 20040, loss: 0.11350643634796143\n",
            "step: 20050, loss: 0.09797365218400955\n",
            "step: 20060, loss: 0.09603728353977203\n",
            "step: 20070, loss: 0.11969269067049026\n",
            "step: 20080, loss: 0.08660371601581573\n",
            "step: 20090, loss: 0.06258577108383179\n",
            "step: 20100, loss: 0.14153242111206055\n",
            "step: 20110, loss: 0.1237526684999466\n",
            "step: 20120, loss: 0.0886559784412384\n",
            "step: 20130, loss: 0.13312047719955444\n",
            "step: 20140, loss: 0.15895125269889832\n",
            "step: 20150, loss: 0.36336323618888855\n",
            "step: 20160, loss: 0.10313969105482101\n",
            "step: 20170, loss: 0.05901617556810379\n",
            "step: 20180, loss: 0.079945869743824\n",
            "step: 20190, loss: 0.13249225914478302\n",
            "step: 20200, loss: 0.10523476451635361\n",
            "step: 20210, loss: 0.05221288278698921\n",
            "step: 20220, loss: 0.015009301714599133\n",
            "step: 20230, loss: 0.03415314480662346\n",
            "step: 20240, loss: 0.19901511073112488\n",
            "step: 20250, loss: 0.28346604108810425\n",
            "step: 20260, loss: 0.191183939576149\n",
            "step: 20270, loss: 0.11454393714666367\n",
            "step: 20280, loss: 0.14160273969173431\n",
            "step: 20290, loss: 0.1433255523443222\n",
            "step: 20300, loss: 0.10688985139131546\n",
            "step: 20310, loss: 0.049611177295446396\n",
            "step: 20320, loss: 0.16442148387432098\n",
            "step: 20330, loss: 0.06639178842306137\n",
            "step: 20340, loss: 0.012117335572838783\n",
            "step: 20350, loss: 0.1382124125957489\n",
            "step: 20360, loss: 0.053488798439502716\n",
            "step: 20370, loss: 0.12259817123413086\n",
            "step: 20380, loss: 0.12597915530204773\n",
            "step: 20390, loss: 0.0950879231095314\n",
            "step: 20400, loss: 0.08833133429288864\n",
            "step: 20410, loss: 0.15468905866146088\n",
            "step: 20420, loss: 0.12281016260385513\n",
            "step: 20430, loss: 0.05422954261302948\n",
            "step: 20440, loss: 0.09196683019399643\n",
            "step: 20450, loss: 0.14272935688495636\n",
            "step: 20460, loss: 0.08650583773851395\n",
            "step: 20470, loss: 0.03448677808046341\n",
            "step: 20480, loss: 0.030626961961388588\n",
            "step: 20490, loss: 0.15745802223682404\n",
            "step: 20500, loss: 0.21750298142433167\n",
            "step: 20510, loss: 0.08581811189651489\n",
            "step: 20520, loss: 0.10941123217344284\n",
            "step: 20530, loss: 0.08639390766620636\n",
            "step: 20540, loss: 0.1452469676733017\n",
            "step: 20550, loss: 0.07293744385242462\n",
            "step: 20560, loss: 0.07035122066736221\n",
            "step: 20570, loss: 0.09938839823007584\n",
            "step: 20580, loss: 0.028425056487321854\n",
            "step: 20590, loss: 0.21865320205688477\n",
            "step: 20600, loss: 0.1751003861427307\n",
            "step: 20610, loss: 0.07206506282091141\n",
            "step: 20620, loss: 0.05467548593878746\n",
            "step: 20630, loss: 0.22229380905628204\n",
            "step: 20640, loss: 0.25872042775154114\n",
            "step: 20650, loss: 0.05936618149280548\n",
            "step: 20660, loss: 0.12446754425764084\n",
            "step: 20670, loss: 0.0888618752360344\n",
            "step: 20680, loss: 0.1417148858308792\n",
            "step: 20690, loss: 0.06829748302698135\n",
            "step: 20700, loss: 0.037812937051057816\n",
            "step: 20710, loss: 0.1417330801486969\n",
            "step: 20720, loss: 0.012222240678966045\n",
            "step: 20730, loss: 0.05076579004526138\n",
            "step: 20740, loss: 0.14463861286640167\n",
            "step: 20750, loss: 0.07702719420194626\n",
            "step: 20760, loss: 0.0552489347755909\n",
            "step: 20770, loss: 0.013758378103375435\n",
            "step: 20780, loss: 0.04941572993993759\n",
            "step: 20790, loss: 0.09852399677038193\n",
            "step: 20800, loss: 0.06510558724403381\n",
            "step: 20810, loss: 0.1296662539243698\n",
            "step: 20820, loss: 0.09364087879657745\n",
            "step: 20830, loss: 0.012643063440918922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H8ht2Yq0HbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "fname = '/content/drive/My Drive/imrsv/Colab Notebooks/weights_overall.pt'\n",
        "# torch.save(model.state_dict(), f\"{fname}\")\n",
        "print(f\"weights were saved to {fname}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msCdkMVaxLc-",
        "colab_type": "code",
        "outputId": "46484464-80f5-465c-9682-47a9e94c06ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(torch.cuda.max_memory_allocated(device=None))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3655972864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHhXZbBewTKx",
        "colab_type": "text"
      },
      "source": [
        "## load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq_giS9OeFBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "fname = '/content/drive/My Drive/imrsv/Colab Notebooks/weights_overall.pt'\n",
        "finetuning = True\n",
        "top_rnns = False\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = Net(top_rnns=top_rnns, vocab_size=len(VOCAB), device=device, finetuning=finetuning).cuda()\n",
        "model = nn.DataParallel(model)\n",
        "# model.load_state_dict(torch.load(fname))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dss_jpf2wWvI",
        "colab_type": "text"
      },
      "source": [
        "## evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1OmwJMoydC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "eval_iter = data.DataLoader(dataset=eval_dataset,\n",
        "                              batch_size=32,\n",
        "                              shuffle=True,\n",
        "                              num_workers=1,\n",
        "                              collate_fn=pad)\n",
        "eval_dataset = NerDataset(eval_files[0])\n",
        "_ = [eval_dataset.append(NerDataset(eval_file)) for eval_file in eval_files[1:200]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpqJpABHwSFO",
        "colab_type": "code",
        "outputId": "c6a8f9ca-bd54-42a4-9faf-42434ae7ff67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "#experiment_code\n",
        "precision, recall, f1 = eval(model, eval_iter, './')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_proposed:260974\n",
            "num_correct:307\n",
            "num_gold:4625\n",
            "precision=0.00\n",
            "recall=0.07\n",
            "f1=0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKYtfMz9xC7J",
        "colab_type": "code",
        "outputId": "ba1d626c-f2de-455c-b763-71c3cf5d9743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#experiment_code\n",
        "precision, recall, f1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0008326713770085149, 0.05167567567567567, 0.0016389339386188382)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    }
  ]
}