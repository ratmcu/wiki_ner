{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "info_box.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratmcu/wiki_ner/blob/master/info_box.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C-w8h_nQnIm",
        "colab_type": "code",
        "outputId": "888ade2a-ea14-445f-82c9-422dde85eadf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install pyahocorasick\n",
        "!pip install fuzzyset\n",
        "from ahocorasick import Automaton\n",
        "import fuzzyset\n",
        "import urllib\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "!pip install wget\n",
        "!pip install spacy\n",
        "!pip install regex\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "import regex\n",
        "\n",
        "import os\n",
        "import wget\n",
        "colabimporturl = 'https://github.com/ratmcu/colaboratory_import/raw/master/colabimport.py'\n",
        "filename = colabimporturl.split(\"/\")[-1].split(\"?\")[0]\n",
        "if os.path.isfile(filename):\n",
        "    os.remove(filename)\n",
        "wget.download(colabimporturl)\n",
        "\n",
        "import colabimport\n",
        "colabimport.get_notebook('https://github.com/ratmcu/wiki_ner/blob/master/reusable_annotator.ipynb?raw=true')\n",
        "colabimport.get_notebook('https://github.com/ratmcu/wiki_ner/blob/master/info_box.ipynb?raw=true')\n",
        "# import io, os, sys, types\n",
        "from reusable_annotator import PageContents\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\r\u001b[K     |█                               | 10kB 12.2MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 3.3MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 4.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 3.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 163kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 225kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 245kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 256kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 266kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 276kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 296kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 4.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81700 sha256=72832d5a1502b0232f51b98d30907868477facb6720cc018a3f2c786f9aa19e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick\n",
            "Successfully installed pyahocorasick-1.4.0\n",
            "Collecting fuzzyset\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/78/7509f3efbb6acbcf842d7bdbd9a919ca8c0ed248123bdd8c57f08497e0dd/fuzzyset-0.0.19.tar.gz (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 4.8MB/s \n",
            "\u001b[?25hCollecting python-levenshtein (from fuzzyset)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 18.3MB/s \n",
            "\u001b[?25hCollecting texttable (from fuzzyset)\n",
            "  Downloading https://files.pythonhosted.org/packages/82/a8/60df592e3a100a1f83928795aca210414d72cebdc6e4e0c95a6d8ac632fe/texttable-1.6.2.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein->fuzzyset) (41.2.0)\n",
            "Building wheels for collected packages: fuzzyset, python-levenshtein, texttable\n",
            "  Building wheel for fuzzyset (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fuzzyset: filename=fuzzyset-0.0.19-cp36-cp36m-linux_x86_64.whl size=167543 sha256=e04535154f3aa50e0eeff45d611a0971344fc2d5a45b8dc10c3fa3699d1dca3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d8/36/9a/8f1cac047c7c3b03dce3d5434ed0088bfd8da8aeca615dfb4c\n",
            "  Building wheel for python-levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144668 sha256=6735481fd19951fd9457563a9688932ea9f42c78a836f7edac10b1c4a2c8393f\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "  Building wheel for texttable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for texttable: filename=texttable-1.6.2-cp36-none-any.whl size=10654 sha256=b9ff0a72e42ff106ba81116d303e8a8f76948f296f304ce6ebf76025852933c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/d1/d6/dfbe4eb3c468832f7fbe4bd27f9875fa97277cabed8fb6715c\n",
            "Successfully built fuzzyset python-levenshtein texttable\n",
            "Installing collected packages: python-levenshtein, texttable, fuzzyset\n",
            "Successfully installed fuzzyset-0.0.19 python-levenshtein-0.12.0 texttable-1.6.2\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=8418a74d1bb7b90c6d9c49e377bf547e7e0801c6fd391a16cf42d7cec2958672\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.8)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.1.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 5.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.8.19-cp36-cp36m-linux_x86_64.whl size=609251 sha256=d46c1c3302beb616a2f93051f079bd19534f9fa9f1bb4fe96b49bba07e049461\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\n",
            "Successfully built regex\n",
            "Installing collected packages: regex\n",
            "Successfully installed regex-2019.8.19\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "importing Jupyter notebook from reusable_annotator.ipynb\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: fuzzyset in /usr/local/lib/python3.6/dist-packages (0.0.19)\n",
            "Requirement already satisfied: python-levenshtein in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (0.12.0)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.6/dist-packages (from fuzzyset) (1.6.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein->fuzzyset) (41.2.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfb8Zq6EP6vR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InfoCard():\n",
        "    '''accepts a beautiful soup html table from a wikipedia page and scrapes the table to a dictionary'''\n",
        "    def __init__(self, page_content):\n",
        "        self.info_table = {}\n",
        "        if not page_content.table:\n",
        "            raise Exception('page content is incomplete')\n",
        "        for table_entry in page_content.table_entry_list:\n",
        "            try:\n",
        "                left_col = table_entry.find('th', attrs={'scope': 'row'})\n",
        "                right_col = left_col.next_sibling\n",
        "                self.info_table[self._row_filter(left_col.text)] = self._get_text_parts(self._get_kids(right_col))\n",
        "            except:\n",
        "                pass\n",
        "        self.info_table_unfiltered = self.info_table\n",
        "        self.info_table = self._filter_info_scrapes(self.info_table)\n",
        "        print('info card is scraped successfully')\n",
        "        logging.debug(self.info_table)\n",
        "    \n",
        "    def _row_filter(self, text):\n",
        "        return re.sub('\\xa0', ' ', text)\n",
        "        \n",
        "        \n",
        "    def _get_kids(self, html_mother):\n",
        "        kid_list = []\n",
        "        try:\n",
        "            kids = html_mother.children\n",
        "            for kid in kids:\n",
        "                kid_list.append(self._get_kids(kid))\n",
        "        except:\n",
        "            return html_mother\n",
        "        return kid_list \n",
        "\n",
        "    def _get_text_parts(self, text_lists):\n",
        "        text_parts = []\n",
        "        if type(text_lists) == list:\n",
        "            for element in text_lists:\n",
        "                text_parts.extend(self._get_text_parts(element))\n",
        "        else:\n",
        "            text_parts.append(text_lists)\n",
        "        return(text_parts)\n",
        "\n",
        "    def _filter_info_scrapes(self, scape_dict):\n",
        "        mask_dict = {}\n",
        "        for key, val in scape_dict.items():\n",
        "            mask_dict[key] = []\n",
        "            for i, element in enumerate(scape_dict[key]):\n",
        "                name = ''\n",
        "                lst = [' '+nm[0] for nm in [part.split('(') for part in element.split(' ')] if nm[0]!='']\n",
        "                element = ''.join(lst).strip()\n",
        "                fnd = [c in element for c in u'[]\\n\\xa0']\n",
        "                if not True in fnd:\n",
        "                    mask_dict[key].append(element)  \n",
        "        return mask_dict    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THHEwExcSNW2",
        "colab_type": "code",
        "outputId": "2b064152-13d4-44d2-e803-d4b1b05ba95a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "#experiment_code\n",
        "# pg = PageContents('https://en.wikipedia.org/wiki/Barack_Obama')\n",
        "pg = PageContents('https://en.wikipedia.org/wiki/Donald_Trump')\n",
        "# pg = PageContents('http://en.wikipedia.org/wiki/Gajendra_Singh_Rajukhedi')\n",
        "\n",
        "info_card = InfoCard(pg)\n",
        "print(info_card.info_table_unfiltered)\n",
        "print(info_card.info_table)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "info card is scraped successfully\n",
            "{'Vice President': ['Mike Pence'], 'Preceded by': ['Barack Obama'], 'Born': ['Donald John Trump', ' (', '1946-06-14', ') ', 'June 14, 1946', ' (age\\xa073)', 'Queens', ', ', 'New York City'], 'Political party': ['Republican', ' (1987–1999, 2009–2011, 2012–present)'], 'Other politicalaffiliations': ['\\n', 'Democratic', ' (until 1987, 2001–2009)', '\\n', 'Reform', ' (1999–2001)', '\\n', 'Independent', ' (2011–2012)', '\\n'], 'Spouse(s)': ['\\n', 'Ivana Zelníčková', '(', 'm.', '\\xa0', '1977', '; ', 'div.', '\\xa0', '1992', ')', '\\n', 'Marla Maples', '(', 'm.', '\\xa0', '1993', '; ', 'div.', '\\xa0', '1999', ')', '\\n', 'Melania Knauss', ' (', 'm.', '\\xa0', '2005', ')', '\\n'], 'Children': ['\\n', 'Donald Jr.', '\\n', 'Ivanka', '\\n', 'Eric', '\\n', 'Tiffany', '\\n', 'Barron', '\\n'], 'Parents': ['Fred Trump', 'Mary Anne MacLeod'], 'Relatives': ['Trump family'], 'Residence': ['\\n', 'White House', ' (official)', '\\n', 'Trump Tower', ' (personal)', '\\n', 'Full list', '\\n'], 'Education': ['The Wharton School', ' (', 'BS', ' in ', 'Econ.', ')'], 'Occupation': ['Politician', 'businessman', 'real-estate developer', 'television personality'], 'Net worth': ['US$3.1 billion (March 2019)', '[a]'], 'Awards': ['List of honors and awards'], 'Signature': [], 'Website': ['\\n', 'Official website', '\\n', 'White House website', '\\n', 'Presidential Twitter', '\\n', 'Personal Twitter', '\\n']}\n",
            "{'Vice President': ['Mike Pence'], 'Preceded by': ['Barack Obama'], 'Born': ['Donald John Trump', '', '1946-06-14', ')', 'June 14, 1946', '', 'Queens', ',', 'New York City'], 'Political party': ['Republican', '2009–2011, 2012–present)'], 'Other politicalaffiliations': ['', 'Democratic', '1987, 2001–2009)', '', 'Reform', '', '', 'Independent', '', ''], 'Spouse(s)': ['', 'Ivana Zelníčková', '', 'm.', '', '1977', ';', 'div.', '', '1992', ')', '', 'Marla Maples', '', 'm.', '', '1993', ';', 'div.', '', '1999', ')', '', 'Melania Knauss', '', 'm.', '', '2005', ')', ''], 'Children': ['', 'Donald Jr.', '', 'Ivanka', '', 'Eric', '', 'Tiffany', '', 'Barron', ''], 'Parents': ['Fred Trump', 'Mary Anne MacLeod'], 'Relatives': ['Trump family'], 'Residence': ['', 'White House', '', '', 'Trump Tower', '', '', 'Full list', ''], 'Education': ['The Wharton School', '', 'BS', 'in', 'Econ.', ')'], 'Occupation': ['Politician', 'businessman', 'real-estate developer', 'television personality'], 'Net worth': ['US$3.1 billion 2019)'], 'Awards': ['List of honors and awards'], 'Signature': [], 'Website': ['', 'Official website', '', 'White House website', '', 'Presidential Twitter', '', 'Personal Twitter', '']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbAQlKcZl1j2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# can we use snorkle to get this entity scraping done??? \n",
        "# if so it'd be better to have the lable functions in a ready way to generally working on all the pages!!\n",
        "\n",
        "class PrivateEntities():\n",
        "    '''given a dictionary of a side bar it is processed for private entities of interest'''\n",
        "    def __init__(self, info_card):\n",
        "        self.info_dict = info_card.info_table\n",
        "        # print(info_card.info_table)\n",
        "        self._get_entity_dict()\n",
        "        self._extract_entities()\n",
        "        \n",
        "    def _extract_entities(self):\n",
        "        # TREAT everyone the same? or call a seperate function per each entity?\n",
        "        # leave the not found entities as empty lists so we can use the html scraper to fill them (usually the name of the person)\n",
        "        # lets treat everything generally, ultimate filtering happens at the comparison stage(if theres one)\n",
        "        for entity_key in self.entity_dict.keys():\n",
        "            print(entity_key, self.entity_dict[entity_key])\n",
        "            if self.entity_dict[entity_key] == []:\n",
        "                continue\n",
        "            for tiny_dict in self.entity_dict[entity_key]:\n",
        "                self._pick_entity(tiny_dict, entity_key)\n",
        "            logging.debug(self.entity_dict[entity_key])\n",
        "    \n",
        "    def _pick_entity(self, entity_list, entity_key):\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "#         print(entity_list)\n",
        "#         Find named entities, phrases and concepts\n",
        "#         entity_list.append([])\n",
        "#         print(entity_list[0][0])\n",
        "        #TODO: add a check for the NAME too\n",
        "        if entity_key == 'BIRTH_PLACE':\n",
        "            mask_str = ''\n",
        "            for scrape in self.info_dict[entity_list['dict_key']]:\n",
        "                doc = nlp(str(scrape))\n",
        "                if doc.ents and doc.ents[0].label_ == 'DATE':\n",
        "                    mask_str = mask_str + 'd'\n",
        "                elif str(scrape) == '' or str(scrape) == ',':\n",
        "                    mask_str = mask_str + 's'\n",
        "                elif self._is_name(str(scrape)):\n",
        "                    mask_str = mask_str + 't'\n",
        "                else:\n",
        "                    print(scrape)\n",
        "                    mask_str = mask_str + 'u'\n",
        "            print(mask_str)\n",
        "            indices = self._get_target_indices([r'[s][t]',r'[d][t]'], mask_str)  \n",
        "            for index in indices:\n",
        "                entity_list['entity_list'].append(self.info_dict[entity_list['dict_key']][index])\n",
        "        else:    \n",
        "            for scrape in self.info_dict[entity_list['dict_key']]:\n",
        "                if entity_list['type'] == 'PERSON': #spacy skips certain parts of a name, so we use this trick instead\n",
        "                    doc = nlp('was named '+str(scrape))\n",
        "                    logging.debug(scrape)\n",
        "                else:\n",
        "                    doc = nlp(str(scrape))\n",
        "                if doc.ents and doc.ents[0].label_ == entity_list['type']:   \n",
        "                    entity_list['entity_list'].append(doc.ents[0].text)\n",
        "                    logging.debug('appended entity: ', doc.ents[0].text, doc.ents[0].label_, len(doc.ents), doc.ents)\n",
        "                elif not self._entity_noise(entity_key, str(scrape)):\n",
        "                    entity_list['entity_list'].append(str(scrape))\n",
        "                else:\n",
        "                    pass\n",
        "                \n",
        "    def _get_target_indices(self, pattern_list, mask_str):\n",
        "        '''TODO: if the target is inside the pattern'''\n",
        "        indices = []\n",
        "        for pattern in pattern_list:\n",
        "            m_iter = re.finditer(pattern, mask_str)\n",
        "            if m_iter:\n",
        "                for m in m_iter: \n",
        "                    indices.append(m.start()+1)\n",
        "        return indices        \n",
        "        \n",
        "    def _entity_noise(self, entity_key, text):\n",
        "        try:\n",
        "            return getattr(self, '_filter_{0}'.format(entity_key))(text)\n",
        "        except:\n",
        "            logging.error('filter function for {0} not found'.format(entity_key))\n",
        "            return True\n",
        "    \n",
        "    def _get_entity_dict(self):\n",
        "        self.entity_dict = {}\n",
        "        self.entity_dict['NAME'] = []\n",
        "        self.entity_dict['BIRTH_DATE'] = []\n",
        "        self.entity_dict['BIRTH_PLACE'] = []\n",
        "        self.entity_dict['CHILDREN'] = []\n",
        "        self.entity_dict['SPOUSES'] = []\n",
        "        self.entity_dict['PARENTS'] = []\n",
        "        self.entity_dict['EDUCATION'] = []\n",
        "        if 'Born' in self.info_dict.keys():\n",
        "            self.entity_dict['NAME'] += [{'dict_key':'Born', 'type':'PERSON', 'entity_list':[]}]\n",
        "            self.entity_dict['BIRTH_DATE'] += [{'dict_key':'Born', 'type':'DATE', 'entity_list':[]}]\n",
        "            self.entity_dict['BIRTH_PLACE'] += [{'dict_key':'Born', 'type':'GPE', 'entity_list':[]}]\n",
        "        if 'Born:' in self.info_dict.keys():\n",
        "            self.entity_dict['NAME'] += [{'dict_key':'Born:', 'type':'PERSON', 'entity_list':[]}]\n",
        "            self.entity_dict['BIRTH_DATE'] += [{'dict_key':'Born:', 'type':'DATE', 'entity_list':[]}]\n",
        "            self.entity_dict['BIRTH_PLACE'] += [{'dict_key':'Born:', 'type':'GPE', 'entity_list':[]}]\n",
        "            \n",
        "        if 'Children' in self.info_dict.keys():\n",
        "            self.entity_dict['CHILDREN'] += [{'dict_key':'Children', 'type':'PERSON', 'entity_list':[]}]\n",
        "        if 'Spouse(s)' in self.info_dict.keys():\n",
        "            self.entity_dict['SPOUSES'] += [{'dict_key':'Spouse(s)', 'type':'PERSON', 'entity_list':[]}]\n",
        "        if 'Parents' in self.info_dict.keys():\n",
        "            self.entity_dict['PARENTS'] += [{'dict_key':'Parents', 'type':'PERSON', 'entity_list':[]}]\n",
        "        if 'Father' in self.info_dict.keys():\n",
        "            self.entity_dict['PARENTS'] += [{'dict_key':'Father', 'type':'PERSON', 'entity_list':[]}]\n",
        "        if 'Mother' in self.info_dict.keys():\n",
        "            self.entity_dict['PARENTS'] += [{'dict_key':'Mother', 'type':'PERSON', 'entity_list':[]}]\n",
        "        if 'Education' in self.info_dict.keys():\n",
        "            self.entity_dict['EDUCATION'] += [{'dict_key':'Education', 'type':'ORG', 'entity_list':[]}]\n",
        "        if 'Alma mater' in self.info_dict.keys():\n",
        "            self.entity_dict['EDUCATION'] += [{'dict_key':'Born', 'type':'ORG', 'entity_list':[]}]\n",
        "            \n",
        "    def _get_sub_entity_dict(self, entity, dict_key):\n",
        "        key_association_dict = {'NAME':{'Birth name', 'Born', 'Born:','Name','Name(s)','Full name'},\n",
        "                                'BIRTH_PLACE':{'Born', 'Born:' ,'Home town'},\n",
        "                                'BIRTH_DATE':{'Born', 'Born:'},\n",
        "                                'CHILDREN':{'Children'},\n",
        "                                'SPOUSES':{'Spouse','Spouse(s)','Spouses'},\n",
        "                                'PARENTS':{'Parent','Parent(s)','Parents','Father','Father’s name','Mother','Mother’s name'},\n",
        "                                'EDUCATION':{'Education','High school','High school:','Law School','School','Schools',\n",
        "                                             'College','College(s)','Colleges','Alma mater','Almat mater','Alma\\xa0mater'}}\n",
        "        \n",
        "        \n",
        "    def _filter_NAME(self, text):\n",
        "        '''cannot simply filter noise due to many possible candidates'''\n",
        "        return True\n",
        "    def _filter_BIRTH_DATE(self, text):\n",
        "        return True\n",
        "    def _filter_BIRTH_PLACE(self, text):\n",
        "        return True\n",
        "    def _filter_CHILDREN(self, text):\n",
        "#         return False # let's assume we don't find any noisy text under info boxes children till we do an analysis\n",
        "        return not self._is_name(text)\n",
        "    def _filter_SPOUSES(self, text):\n",
        "        return not self._is_name(text)\n",
        "    def _filter_PARENTS(self, text):\n",
        "        return not self._is_name(text)\n",
        "    def _filter_EDUCATION(self, text):\n",
        "        return True\n",
        "    def _is_name(self, text):\n",
        "        p = regex.compile(r\"\\p{Lu}\") # To support (currently) 1702 uppercase letters\n",
        "#         p = regex.compile(r\"[[:upper:]]\") # To support (currently) 1822 uppercase letters\n",
        "        if p.match(text):\n",
        "            return True\n",
        "        else:\n",
        "            return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SWMEhG1qpVL",
        "colab_type": "code",
        "outputId": "a3d4aa93-f62b-458f-e9ae-f7953a930a10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# pg = PageContents('https://en.wikipedia.org/wiki/James_R._Alexander')\n",
        "# pg = PageContents('https://en.wikipedia.org/wiki/Brad_Pitt')\n",
        "# pg = PageContents('https://en.wikipedia.org/wiki/Mohamed_Abdel-El')\n",
        "# pg = PageContents('https://en.wikipedia.org/wiki/Donald_Trump')\n",
        "# info_card = InfoCard(pg)\n",
        "# entities = PrivateEntities(info_card).entity_dict\n",
        "# print(info_card.info_table['Born'], entities)\n",
        "\n",
        "pg = PageContents('https://en.wikipedia.org/wiki/Brad_Pitt')\n",
        "info_card = InfoCard(pg)\n",
        "entities = PrivateEntities(info_card).entity_dict\n",
        "# print(info_card.info_table['Born'], entities)\n",
        "\n",
        "# pg = PageContents('http://en.wikipedia.org/wiki/Gajendra_Singh_Rajukhedi')\n",
        "# info_card = InfoCard(pg)\n",
        "# entities = PrivateEntities(info_card).entity_dict\n",
        "# print(info_card.info_table, entities)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "info card is scraped successfully\n",
            "NAME [{'dict_key': 'Born', 'type': 'PERSON', 'entity_list': []}]\n",
            "BIRTH_DATE [{'dict_key': 'Born', 'type': 'DATE', 'entity_list': []}]\n",
            "BIRTH_PLACE [{'dict_key': 'Born', 'type': 'GPE', 'entity_list': []}]\n",
            ")\n",
            ", U.S.\n",
            "tsdudstu\n",
            "CHILDREN [{'dict_key': 'Children', 'type': 'PERSON', 'entity_list': []}]\n",
            "SPOUSES [{'dict_key': 'Spouse(s)', 'type': 'PERSON', 'entity_list': []}]\n",
            "PARENTS []\n",
            "EDUCATION [{'dict_key': 'Education', 'type': 'ORG', 'entity_list': []}, {'dict_key': 'Born', 'type': 'ORG', 'entity_list': []}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pGXcOVoNGsc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a97aa2b6-db7c-40b7-b948-d7bd50aaf576"
      },
      "source": [
        "dates = [entity  for entity_list in entities['SPOUSES'] for entity in entity_list['entity_list']]\n",
        "dates"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Jennifer Aniston', 'Angelina Jolie']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXrBEsRHxT_X",
        "colab_type": "code",
        "outputId": "196873b6-b98e-44be-dc53-2b99c395ca32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "#experiment_code\n",
        "# pg = PageContents('https://en.wikipedia.org/wiki/Donald_Trump')\n",
        "# pg = PageContents('https://en.wikipedia.org/wiki/Barack_Obama')\n",
        "# pg = PageContents('https://en.wikipedia.org/wiki/Bella_Hadid')\n",
        "# pg = PageContents('https://en.wikipedia.org/wiki/Mahinda_Rajapaksa')\n",
        "# pg = PageContents('https://en.wikipedia.org/wiki/Joachim_Gauck')\n",
        "# pg = PageContents('https://en.wikipedia.org/wiki/Mike_Tyson')\n",
        "pg = PageContents('https://en.wikipedia.org/wiki/Brad_Pitt')\n",
        "\n",
        "\n",
        "info_card = InfoCard(pg)\n",
        "# print(type(str(info_card.info_table['Born'][0])))\n",
        "pe = PrivateEntities(info_card)\n",
        "# pe.entity_dict"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "info card is scraped successfully\n",
            "NAME [{'dict_key': 'Born', 'type': 'PERSON', 'entity_list': []}]\n",
            "BIRTH_DATE [{'dict_key': 'Born', 'type': 'DATE', 'entity_list': []}]\n",
            "BIRTH_PLACE [{'dict_key': 'Born', 'type': 'GPE', 'entity_list': []}]\n",
            ")\n",
            ", U.S.\n",
            "tsdudstu\n",
            "CHILDREN [{'dict_key': 'Children', 'type': 'PERSON', 'entity_list': []}]\n",
            "SPOUSES [{'dict_key': 'Spouse(s)', 'type': 'PERSON', 'entity_list': []}]\n",
            "PARENTS []\n",
            "EDUCATION [{'dict_key': 'Education', 'type': 'ORG', 'entity_list': []}, {'dict_key': 'Born', 'type': 'ORG', 'entity_list': []}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f84vuv5FxFH4",
        "colab_type": "code",
        "outputId": "5b50de76-f123-4236-fdbf-0d619e72ab46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        }
      },
      "source": [
        "#experiment_code\n",
        "info_card.info_table"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Alma mater': ['University of Missouri'],\n",
              " 'Born': ['William Bradley Pitt',\n",
              "  '',\n",
              "  '1963-12-18',\n",
              "  ')',\n",
              "  'December 18, 1963',\n",
              "  '',\n",
              "  'Shawnee, Oklahoma',\n",
              "  ', U.S.'],\n",
              " 'Children': ['6'],\n",
              " 'Education': ['Kickapoo High School'],\n",
              " 'Home town': ['Springfield, Missouri', ', U.S.'],\n",
              " 'Net worth': ['$240 million'],\n",
              " 'Occupation': ['Actor', 'film producer'],\n",
              " 'Relatives': ['Douglas Pitt', ''],\n",
              " 'Spouse(s)': ['Jennifer Aniston',\n",
              "  '',\n",
              "  'm.',\n",
              "  '2000;',\n",
              "  'div.',\n",
              "  '2005)',\n",
              "  'Angelina Jolie',\n",
              "  '',\n",
              "  'm.',\n",
              "  '2014;',\n",
              "  'div.',\n",
              "  '2019)'],\n",
              " 'Works': ['Filmography'],\n",
              " 'Years active': ['1987–present']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuaSK6_Xmh7x",
        "colab_type": "code",
        "outputId": "6c052e55-7ae2-4339-cf3b-fa35962bf1e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "fz = fuzzyset.FuzzySet(use_levenshtein=False)\n",
        "fz.add('Alma\\xa0mater')        \n",
        "fz.add('Alma ma')        \n",
        "result = fz.get('Alma mater')\n",
        "result\n",
        "# [(0.7378647873726218, 'Almamater')]\n",
        "re.search('\\xa0', 'Alma\\xa0mater').span(0)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU9JEUxu_MkD",
        "colab_type": "code",
        "outputId": "79a210bc-400b-47ae-f001-a1151991b012",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "ss = 'Gayatri Singh ('\n",
        "name = ''\n",
        "lst = [' '+nm[0] for nm in [part.split('(') for part in ss.split(' ')] if nm[0]!='']\n",
        "# lst = [' '+nm[0] for nm in [part in part for re.split(' ()',ss)] if nm[0]!='']\n",
        "# [part in part for re.split(' ()',ss)]\n",
        "name.join(lst).strip()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Gayatri Singh'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASNawujnDUke",
        "colab_type": "code",
        "outputId": "c8018b2f-f28f-41e6-efe7-3e0282e1d53b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "s = []+['s']\n",
        "s"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['s']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}