{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "segregation_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMK9S6TXMqxh+j06Cqu+dTX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratmcu/wiki_ner/blob/master/segregation_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efYNQ2xExNsS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ae9f3d1-206a-4e96-ddaf-a1eb425db585"
      },
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import wget\n",
        "import logging\n",
        "import torch\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "try:\n",
        "    import colabimport\n",
        "except:\n",
        "    colabimporturl = 'https://github.com/ratmcu/colaboratory_import/raw/master/colabimport.py'\n",
        "    filename = colabimporturl.split(\"/\")[-1].split(\"?\")[0]\n",
        "    if os.path.isfile(filename):\n",
        "        os.remove(filename)\n",
        "    wget.download(colabimporturl)\n",
        "    import colabimport\n",
        "    \n",
        "colabimport.get_notebook('https://github.com/ratmcu/wiki_ner/blob/master/dataset_results.ipynb?raw=true')\n",
        "# colabimport.get_notebook('https://github.com/ratmcu/wiki_ner/blob/master/remote_bert_wikiner.ipynb?raw=true')\n",
        "# from dataset_results import metric_calc, Net "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKjWXKs5v73O",
        "colab_type": "code",
        "outputId": "426c8a58-ae71-49a4-f334-37e90fc0905f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#experiment_code\n",
        "import tarfile\n",
        "!pip install wget\n",
        "import os\n",
        "import wget\n",
        "wget.download('http://archive.org/download/wikiner_dataset_csv.tar/wikiner_dataset_txt.tar.gz')\n",
        "tar = tarfile.open('wikiner_dataset_txt.tar.gz', mode='r')\n",
        "tar.extractall('./dataset_txt')\n",
        "tar.close()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duvMVkWfCO3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "wget.download('https://archive.org/download/auto_manual_annots.tar/auto_manual_annots.tar.gz')\n",
        "tar =  tarfile.open('./auto_manual_annots.tar.gz', mode='r')\n",
        "tar.extractall('./auto_manual_annots')\n",
        "tar.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acL2OzyNpArT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "paths_annot = sorted([os.path.join(f[0], name) for f in os.walk('./dataset_txt') \n",
        "                if len(f[2])!=0 for name in f[2] if os.path.splitext(name)[-1] == '.txt' and name.split('_')[0]=='annot'],\n",
        "               key=lambda path: int(path.split('_')[-1].split('.')[0]))\n",
        "training_files = [path for path in paths_annot if 'training' in path.split('/')]\n",
        "eval_files = [path for path in paths_annot if 'eval' in path.split('/')]\n",
        "test_files = [path for path in paths_annot if 'test' in path.split('/')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INJK415_emDK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "bc74863d-a190-4cc5-ed07-cc2a364fcca3"
      },
      "source": [
        "#experiment_code\n",
        "from dataset_results import NerDataset, pad, data \n",
        "eval_dataset = NerDataset(eval_files[0])\n",
        "_ = [eval_dataset.append(NerDataset(eval_file)) for eval_file in eval_files[1:]]\n",
        "\n",
        "test_dataset = NerDataset(test_files[0])\n",
        "_ = [test_dataset.append(NerDataset(test_file)) for test_file in test_files[1:]]\n",
        "\n",
        "train_dataset = NerDataset(training_files[0])\n",
        "_ = [train_dataset.append(NerDataset(train_file)) for train_file in training_files[1:]]\n",
        "\n",
        "eval_iter = data.DataLoader(dataset=eval_dataset,\n",
        "                              batch_size=1,\n",
        "                              shuffle=True,\n",
        "                              num_workers=16,\n",
        "                              collate_fn=pad)\n",
        "\n",
        "test_iter = data.DataLoader(dataset=test_dataset,\n",
        "                              batch_size=32,\n",
        "                              shuffle=True,\n",
        "                              num_workers=16,\n",
        "                              collate_fn=pad)\n",
        "\n",
        "train_iter = data.DataLoader(dataset=train_dataset,\n",
        "                              batch_size=1,\n",
        "                              shuffle=True,\n",
        "                              num_workers=16,\n",
        "                              collate_fn=pad)\n",
        "\n",
        "print(len(train_iter))\n",
        "print(len(eval_iter))\n",
        "print(len(test_iter))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from dataset_results.ipynb\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.38.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.3)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.5.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.12.46)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.4.5.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.46 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.15.46)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.46->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "man_test_iter iterator size: 320\n",
            "aut_test_iter iterator size: 426\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
            "77703\n",
            "12267\n",
            "65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILEFDXeRk18k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tags = ['BD', 'BP', 'PR', 'SP', 'CH', 'ED']\n",
        "VOCAB_list = ['<PAD>', 'O',]\n",
        "for tag in tags:\n",
        "    VOCAB_list.append('I-'+tag)\n",
        "    VOCAB_list.append('B-'+tag)\n",
        "VOCAB = tuple(VOCAB_list)\n",
        "tag2idx = {tag: idx for idx, tag in enumerate(VOCAB)}\n",
        "idx2tag = {idx: tag for idx, tag in enumerate(VOCAB)}\n",
        "\n",
        "class SegNerDataset(data.Dataset):\n",
        "    def __init__(self, entity):\n",
        "        self.entity = entity\n",
        "        self.percentage = 1\n",
        "        self.sents = []\n",
        "        self.tags_li = []\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        self.is_heads = []\n",
        "        self.seqlen = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(len(self.sents)*self.percentage)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        words, tags = self.sents[idx], self.tags_li[idx] \n",
        "        x, y = self.x[idx], self.y[idx]  \n",
        "        is_heads, seqlen = self.is_heads[idx], self.seqlen[idx] \n",
        "        return words, x, is_heads, tags, y, seqlen\n",
        "    \n",
        "    def append(self, other):\n",
        "        self.sents.extend(other.sents)\n",
        "        self.tags_li.extend(other.tags_li)\n",
        "        self.x.extend(other.x)\n",
        "        self.y.extend(other.y)\n",
        "        self.is_heads.extend(other.is_heads)\n",
        "        self.seqlen.extend(other.seqlen)\n",
        "    \n",
        "    def add(self, entry):\n",
        "        self.sents.extend(entry[0])\n",
        "        self.x.extend(entry[1])\n",
        "        self.is_heads.extend(entry[2])\n",
        "        self.tags_li.extend(entry[3])\n",
        "        self.y.extend(entry[4])\n",
        "        self.seqlen.extend(entry[5])\n",
        "    \n",
        "    def setContribution(self, percentage):\n",
        "        self.percentage = percentage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5GItSHJ6NBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "# getting the indices relavant to a class.\n",
        "# from dataset_results import train_dataset\n",
        "from collections import defaultdict\n",
        "tag_idx_dict = {'BD':[], 'BP':[], 'PR':[], 'SP':[], 'CH':[], 'ED':[]}\n",
        "segregated_ds = defaultdict()\n",
        "\n",
        "for key in tag_idx_dict.keys():\n",
        "    segregated_ds[key] = SegNerDataset(key)\n",
        "\n",
        "for i, entry in enumerate(train_iter):\n",
        "    # print(entry)\n",
        "    words, x, is_heads, tags, y, seqlens = entry\n",
        "    tags = tags[0].split()\n",
        "    for tag in tags:\n",
        "        for key in tag_idx_dict.keys():\n",
        "            if tag[2:] == key:\n",
        "                tag_idx_dict[key].append(i)\n",
        "                # segregated_ds[key].add(entry)\n",
        "                break\n",
        "        else:\n",
        "            continue\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UCU9CkIRT23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tag_idx_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC7W_MCdPs-4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "eb93dbbe-79ff-4d97-efe3-177daf88fa45"
      },
      "source": [
        "import pickle\n",
        "a_file = open(\"tag_idx_dict.pkl\", \"rb\")\n",
        "tag_idx_dict = pickle.load(a_file)\n",
        "for key, induces in tag_idx_dict.items():\n",
        "    print(key)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BD\n",
            "BP\n",
            "PR\n",
            "SP\n",
            "CH\n",
            "ED\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txq1tkHSvJ6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creation of subsets by choosing proportions of the indices\n",
        "# concatenation subsets to form the dataset \n",
        "# training the model\n",
        "# getting evaluation results\n",
        "# saving the model\n",
        "import pickle\n",
        "ratio = 1.0\n",
        "a_file = open(\"tag_idx_dict.pkl\", \"rb\")\n",
        "tag_idx_dict = pickle.load(a_file)\n",
        "ds_list = []\n",
        "for key, indices in tag_idx_dict.items():\n",
        "    if len(indces) != 0:\n",
        "        print(key)\n",
        "        ds_list.append(data.Subset(train_dataset, indices[:int(len(indices)*ratio)]))\n",
        "\n",
        "train_dataset_seg = data.ConcatDataset(ds_list)\n",
        "train_iter = data.DataLoader(dataset=train_dataset_seg,\n",
        "                              batch_size=128,\n",
        "                              shuffle=True,\n",
        "                              num_workers=16,\n",
        "                              collate_fn=pad)\n",
        "for epoch in range(1, 1+1):\n",
        "    train(model, train_iter, optimizer, criterion)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ26gkijlGCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chG1hM9SOX5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "from google.colab import drive\n",
        "import time\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y31iuYHgnln3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "precision, recall, f1 = eval(model, man_test_iter, './eval')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BiyHj0lOcxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "fname = f'/content/drive/My Drive/imrsv/experiments/data_segregation/weights/w_r_wikiner_{int(100.0*ratio)}.pt'\n",
        "torch.save(model.state_dict(), f\"{fname}\")\n",
        "# torch.save(model.state_dict(), 'weights_overall_P0.79_R0.70_F0.74.pt')\n",
        "print(f\"weights were saved to {fname}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtA0JRkkGTku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    model.train()\n",
        "    for i, batch in enumerate(iterator):\n",
        "        words, x, is_heads, tags, y, seqlens = batch\n",
        "        _y = y # for monitoring\n",
        "        optimizer.zero_grad()\n",
        "        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n",
        "\n",
        "        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n",
        "        y = y.view(-1)  # (N*T,)\n",
        "\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i==0:\n",
        "            print(\"=====sanity check======\")\n",
        "            print(\"words:\", words[0])\n",
        "            print(\"x:\", x.cpu().numpy()[0][:seqlens[0]])\n",
        "            print(\"tokens:\", tokenizer.convert_ids_to_tokens(x.cpu().numpy()[0])[:seqlens[0]])\n",
        "            print(\"is_heads:\", is_heads[0])\n",
        "            print(\"y:\", _y.cpu().numpy()[0][:seqlens[0]])\n",
        "            print(\"tags:\", tags[0])\n",
        "            print(\"seqlen:\", seqlens[0])\n",
        "            print(\"=======================\")\n",
        "\n",
        "        if i%10==0: # monitoring\n",
        "            print(f\"step: {i}, loss: {loss.item()}\")\n",
        "            # return\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOv9pePnWird",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dae45c48-48e1-4ba1-931e-bd6faf80ee02"
      },
      "source": [
        "import pickle\n",
        "a_file = open(\"tag_idx_dict.pkl\", \"wb\")\n",
        "pickle.dump(tag_idx_dict, a_file)\n",
        "a_file.close()\n",
        "\n",
        "a_file = open(\"tag_idx_dict.pkl\", \"rb\")\n",
        "output = pickle.load(a_file)\n",
        "output == tag_idx_dict"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz7P3OVdLz35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "federated_train_loader = sy.FederatedDataLoader(train_dataset.federate((bob, alice)),\n",
        "                              batch_size=128,\n",
        "                              shuffle=True)\n",
        "federated_train_iter = federated_train_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syif2BjtDak_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "from dataset_results import VOCAB, nn\n",
        "import remote_bert_wikiner\n",
        "print(torch.cuda.max_memory_allocated(device=None))\n",
        "batch_size = 32\n",
        "lr = 0.0001\n",
        "n_epochs = 30\n",
        "finetuning = True\n",
        "top_rnns = False\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = remote_bert_wikiner.Net(top_rnns=top_rnns, vocab_size=len(VOCAB), device=device, finetuning=finetuning).to(device)\n",
        "print(torch.cuda.max_memory_allocated(device=None))\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbP4XMxLRq7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import optim\n",
        "from remote_bert_wikiner import VOCAB_list\n",
        "def federated_train(model, device, iterator, criterion):\n",
        "    model.train()\n",
        "    loc = ''\n",
        "    for i, batch in enumerate(iterator):\n",
        "        # words, x, is_heads, tags, y, seqlens = batch\n",
        "        x, y = batch\n",
        "        if loc != x.location:\n",
        "            print(f'data location {x.location}')\n",
        "            if model.location != None:\n",
        "                model.get()\n",
        "            model.send(x.location) # <-- NEW: send the model to the right location\n",
        "            optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "            loc = x.location\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        # x, y = x.get(), y.get()\n",
        "        _y = y # for monitoring\n",
        "        optimizer.zero_grad()\n",
        "        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n",
        "        # print(logits.shape[-1])\n",
        "        logits = logits.view(-1, len(VOCAB_list)) # (N*T, VOCAB), vocab len = #tags*2<I,N> + 0 + <PAD> \n",
        "        y = y.view(-1)  # (N*T,)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i%10==0: # monitoring\n",
        "            print(f\"step: {i}, loss: {loss.get().item()}\")\n",
        "        # print(f\"step: {i}, loss: {loss.get().item()}\")\n",
        "    model.get() # <-- NEW: get the model back"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgJ4XQGaFYpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment_code\n",
        "# from remote_bert_wikiner import federated_train\n",
        "for epoch in range(1, 1+1):\n",
        "    federated_train(model, device, federated_train_iter, criterion)\n",
        "    print(f\"=========eval at epoch={epoch}=========\")\n",
        "print(torch.cuda.max_memory_allocated(device=None))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZb2j2itTEkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}